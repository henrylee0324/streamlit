{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194723, Requested 7889. Please try again in 783ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194723, Requested 7889. Please try again in 783ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Asian Americans see shooting as a culmination of a year of racism\nAs Helen Kim Ho learned that a White man with a self-described sex addiction was charged with killing eight people - including six Asian women - at spas in the Atlanta area on Tuesday, she imagined the stereotypes of Asian women that must have run through his head.\nWere not really Americans, were perpetually foreigners, and that idea plays out with women as being oversexualized, said Ho, a Korean American and a founder of the advocacy group Asian Americans Advancing Justice in Atlanta. All of that had to have played out in this mans own mind. In addition to the unspoken notion that Asian people are easy targets.\nAtlanta police said the suspect told them his actions were not racially motivated - even though the shooter targeted businesses known for employing Asians, and six victims were Asian women. The suspect claimed he had a sex addiction, according to police, and wanted to eliminate temptation, which sounded to many women as if their sexuality was somehow to blame.\nThe gunmans intent seemed crystal clear to Asians living in Atlanta and across the nation who have long had to confront stereotyping, hateful harassment and even violence - and who say things have gotten even worse amid the coronavirus pandemic.\nFor Asian women, the moment felt particularly threatening.\nAs soon as Crystal Jin Kim heard about the shooting, she reached out to her mother and father, who immigrated to the United States from Korea. In a text, she urged them to be safe, to be careful, and to pray.\nShe thought about rescheduling one of their upcoming doctor appointments, and she worried about her mother going to work at a small business in the Atlanta area - she asked that the type of business not be named for fear of her mother being targeted.\nSince I was a kid Ive heard racial slurs yelled at me or my parents, or witnessed my parents being treated as if they were stupid because their English isnt perfect, even though my moms English is really good, said Kim, a second-generation Korean American. Those small moments really add up. I dont think weve ever spoken up against those small moments. . . . Its easier to try not to think about it, or to try to let it go. To try to bury the hurt.\nPeople have leaned out of cars to scream Asian! at her, she said, and she has gotten comments about how she must like Jackie Chan, the Hong Kong martial artist and actor. As an Asian woman, she said men often ascribe characteristics to her that dont reflect her personality - thats shes a pushover, or soft-spoken.\nReally, Im not - Im very talkative and extroverted, Kim said. Its just people having these assumptions and treating me like a perfect Asian girl. Not even a woman, but a girl.\nDavid Palumbio-Liu, a Stanford professor and author of Asian/American: Historical Crossings of a Racial Frontier, said there is a long history, extending well before the Vietnam War, of the fetishization of and murderous intent toward Asian women. He cited the Broadway musical Miss Saigon, which critics have said romanticizes an imperialistic relationship and portrays Asian women as acquiescent and self-sacrificing.\nThe suspect said it wasnt racially motivated, but on the other hand, hes going specifically to these spas where Asian women work precisely to serve the sexual fantasies of white males, Palumbio-Liu said in an interview, so to disentangle them is really to do a disservice to the fact that these things are so linked together.\nThe shooting made visible the worstcase scenarios many Asians living in the United States had feared. Many sadly expressed a similar sentiment: We knew this was coming.\nThe coalition Stop AAPI Hate has been documenting anti-Asian attacks since the pandemic started last March and says there have been nearly 3,800 hate-fueled incidents against the Asian American and Pacific Islander community in the U.S. - a number the group says is likely a fraction of the true number. About 3 in 10 Asian adults said theyve experienced jokes or slurs about their race or ethnicity during the pandemic, according to Pew Research - the highest percent among all races.\nMore than 68% of documented reports of anti-Asian harassment and violence since the beginning of the coronavirus pandemic have been from women.\n\"I've never been this afraid to be Asian in America,\" said Dorothy Kuo, 38, who attempted to explain to her 6-year-old daughter what had happened in their community.\n\"I told her 'Mommy is having a hard time focusing today because last night there were eight people killed.' I just told her, honestly, what happened,\" said Kuo, who is Korean American, like several of the victims.\nKuo said she felt it was important to tell her daughter so she understands the world she's stepping into, one in which she might have to be more careful as an Asian woman.\n\"I've been refused seating at a restaurant,\" Kuo said, \"and even then I've never felt what I feel now.\"\nJane Kim Coloseus, 32, grew angry Wednesday when Atlanta police declared it was too soon to say whether the slayings were racially motivated.\n\"As an Asian woman, it brings out a lot of the experiences or harassment we have received throughout our lives in general, and just have that completely invalidated because of what the suspect is saying,\" said Coloseus, a Korean American who is executive director of the nonprofit Her Term, which recruits women to run for office in Georgia.\nShe has long felt like she had to be more guarded - whether it's in the workplace or walking down the street - because of anti-Asian sentiments and the sexualization of Asian women's bodies. Asians have long been part of the fabric of the United States, she said, yet have \"been kept on the sidelines as the model minority,\" their voices ignored or muted.\n\"To me the equation is"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193684, Requested 7202. Please try again in 265ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193684, Requested 7202. Please try again in 265ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Manila, April 6 The Philippine Consulate General in New York has joined the Asian community in calling an end to the increasing hate crimes against Asians across the United States, the latest of which was the brutal assault of a 65-year-old Filipina in Manhattan. \n\"We take this opportunity to thank the New York Police District and its Asian Hate Crime Task Force for their dedication to their work and their quick action in apprehending Elliot,\" Philippine Consul General Elmer Cato said in a statement read at the rally. \nCato was referring to Brandon Elliot, a 38-year-old convict out on parole for killing his mother, who was charged with assault and hate crime offenses for kicking and stomping on Filipina Vilma Kari's face while yelling anti-Asian slurs at her last March 29. \nThe support was reiterated during a rally organized by the Chinese-American Citizens Alliance of Greater New York outside the Manhattan criminal court on April 5 to demand justice for Kari. \n\"Today, Brandon Elliot will appear in this Criminal Court, and to him we say Vilma belongs here, Asians in New York belong here, everyone belongs here,\" Cato said. \n\"I can see in Vilma the face of my own mother and other elderly Asian women living in New York, who, due to the recent spate of violence against Asians, would not even want to leave the safety of their homes anymore for fear that they, too, would be violently assaulted,\" he added. \nAside from Kari, another Filipino immigrant, 61-year-old Noel Quintana got his face slashed after he was attacked with a box cutter while riding the subway on his way to work in Manhattan a few weeks ago. \n\"These cases hit close to home, since we, Asians, revere our elders. It is part of our DNA to look after our elderly. This is why Elliot's violent attack on Vilma had been particularly distressing to all of us,\" Cato said. \n\"But contrary to what Elliot said, Vilma and the rest of the Asian-American and Pacific Islander (AAPI) Community belong here. Our community contributes significantly to the greatness of this city,\" he added. \nThere are about 2.4 million AAPI residents in the New York City Metro Area alone, the largest in the US, contributing more than USD6.2 billion in state and local tax revenues, Cato said. \nHe added that Filipino nurses continue to man the frontline in the city's fight against the coronavirus disease 2019 (Covid-19). \nCiting data from the National Nurses United, Cato said 4 percent of nurses in the United States and about 30 percent of the almost 200 registered nurses who died from Covid-19 are Filipinos. \n\"Twenty-five percent of Filipinos in New York work in the healthcare industry. It is, thus, ironic for Elliot to insist that Vilma and the AAPI community she belongs to do not belong here,\" he said. \n\"As we pray for an end to hate, racism, and discrimination, we urge everyone not to shut their doors to victims of hate crimes like Vilma,\" he added. \nCato urged all Filipino hate crime victims to get in touch with the PCG as well as immediately report incidents to 911 or the Crime Stoppers hotline of the New York City Police Department."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193600, Requested 7240. Please try again in 251ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193600, Requested 7240. Please try again in 251ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "When news broke of the latest violent attack against an Asian-American in New York City on Monday, it ricocheted across the South Korean mediasphere, generating countless articles and news bulletins. \nThe brutal attack on a 65-year-old Filipino woman, documented in graphic footage, was the latest example of the rising anti-Asian violence in the US that has captured attention in South Korea, where media outlets have highlighted the Korean background of some of those targeted. \nFour of the eight people killed in last month's Atlanta spa shooting - which prompted widespread soul-searching about anti-Asian bias in the US - were women of Korean descent, while two were ethnic Chinese. \nOn Thursday, an article about Brandon Elliot - the suspect in Monday's attack - ranked as the most-read piece on the website of Korean broadcaster JTBC. \nElliot, a 38-year-old homeless man who was previously convicted of killing his mother, has been charged with three federal hate crimes after allegedly kicking and stomping on his victim in front of a luxury housing estate. \nYet the flurry of attention in South Korea, one of the world's most ethnically homogenous countries, at the uptick in violence has differed from coverage elsewhere in one notable respect: a heavy focus on the race of the suspects involved, many of whom have been African-American. \nWhile US news reports did not highlight the race of alleged assailants in many recent attacks, South Korean media outlets have readily identified suspects by race in articles and even headlines. \nIn one article, the state-funded Yonhap News Agency mentioned Elliot's race and that of another man who was reported to have attacked an Asian man on the New York subway on Monday no fewer than 26 times. Both Elliot and the suspect in the subway incident - which a witness later said appeared to have actually involved a Hispanic man who used a racial slur - are black. \nThe stark contrast in framing highlights differing sensitivities around race that permeate the two cultures, amid discussions in the US and elsewhere about the causes of violence against Asians that have thrown up explanations ranging from the Covid-19 pandemic to white supremacy. \n\"Race holds different meanings and different power in different countries - it is just more sensitive and less tolerant to point out one's race in the US due to its long and contentious history of racism,\" said Gi-Wook Shin, a Korean-American sociology professor at Stanford University. \nFor Koreans, ethnicity is so integral to identity that \"it seems just a natural thing for Korean media to mention race along with other socio-demographic characteristics\", Shin said. \nMichael Hurt, a sociologist of African-American and Korean descent who lives in Seoul, said South Korean society had a history of linking African-Americans with violent crime because of the tensions exposed between black and Korean-American communities in the 1992 Los Angeles riots. \n\"Koreans want to know the real deal. Is it safe? And safe in America definitely brings up black faces in the Korean mind,\" Hurt said. \"Do Black people inherently hate Asian people? I don't think that's true, I don't think that's ever been true, but I think there's a perception of that as true in Korea, and how the media has to speak to that is a pretty tricky thing.\" US media reports have largely avoided mentioning the race of the alleged assailants in recent attacks. Hurt said he did not believe US media outlets were guilty of hypocrisy in being sensitive in covering the race of perpetrators, but acknowledged the possibility of \"overcorrection\" in some cases. \n\"If it's just straightforward white supremacy, white people acting out, that just makes more sense, but I think when you see ... people of colour beating up other people of colour you kind of scratch your head and think I don't really understand what's going on,\" he said."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193984, Requested 7030. Please try again in 304ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193984, Requested 7030. Please try again in 304ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "In June, a 70-year-old Filipino American man was punched in the face at Don Knabe Community Regional Park in Cerritos. \nThe assailant told the man to \"go back to your country.\" \nA month later, dozens of Asian American senior citizens practiced self-defense moves in the same park where the attack occurred. \nWearing sun visors and fanny packs, the seniors kicked, punched and jabbed. \nSovanna Yeang, 65, was practicing groin kicks, slamming her leg into a foam pad that a volunteer held in front of his crotch. \nYeang, who is Chinese Cambodian, said she is concerned for her safety after seeing television news reports about attacks against elderly Asians. \n\"If somebody attacked me, I didn't know what to do,\" she said. \"I'd just run. I didn't know how to yell or anything. Now, I learned some things. I'm not scared anymore.\" \nThe self-defense class was organized by Seniors Fight Back, which formed early this year as hate crimes against Asians rose during the pandemic. \nViolent attacks, captured on video, against older Asians in Oakland and San Francisco Chinatowns have left some senior citizens so fearful that they barely leave the house. So far, Seniors Fight Back has hosted five classes providing seniors with basic grounding in how to defend themselves against an attacker. \n\"There were so many attacks on our elders, and I was just getting really frustrated,\" said Tony Dang, 32, a co-founder who owns a Vietnamese pancake catering company in Fountain Valley. \nWhile some Asian Americans have organized volunteer patrols to escort seniors to their destinations, the answer for Dang lay in self-defense. \nA friend, Andy Luong, had shared an Instagram post about boxing classes that raise funds for Asian American and Pacific Islander organizations. \nDang asked Luong to connect him with Box for Change, the group that runs the classes. Dang spoke with Hong Lee, the 35-year-old victim of a racist verbal attack at a Los Angeles restaurant, who had been taking the boxing classes. \nThe three began discussions that expanded to include a network of friends and acquaintances, including mixed martial arts fighter Ron Scolesdang. \nTheir first class, in Hawthorne on May 12, drew about 20 seniors. A few weeks later, in Little Saigon, more than 200 showed up. \nThe group of about 10 volunteers is seeking nonprofit status and is hoping to expand to more cities in Southern California and across the country. \nIn California, anti-Asian hate crimes more than doubled last year, according to the state attorney general. \nThen-President Trump has emphasized the coronavirus' Chinese origins, and some people have attacked Asian Americans with rhetoric asserting that they are to blame for the pandemic. \nElderly Asian immigrants who are assaulted are often reluctant to speak out, said Linda Vo, a professor of Asian American studies at UC Irvine. \nMany are wary of the police, uncomfortable about communicating in English or ashamed about being victims, she said. But the rise in anti-Asian attacks has mobilized many Asian Americans and encouraged them to share their experiences, according to Judy Wu, a professor of Asian American studies at UC Irvine. \n\"It's been such an intensity of racialized attacks that more and more people are understanding that there's a need to speak up,\" Wu said. \nSelf-defense classes for senior citizens have also caught on in Little Tokyo. \nJanette Lee, a second-degree black belt in tae kwon do who served as senior activities coordinator for the Little Tokyo Service Center, organized five self-defense classes this year. \nMost of the participants were low-income, monolingual Korean, Japanese or Chinese American senior citizens, Lee said. Almost all were female, and some told stories of being attacked. \n\"I started because I was really angry about what was happening, because when I was looking at the news, I saw that most of the hate crimes were targeted toward seniors,\" Lee said. \nArt Ishii, who has taught martial arts in Little Tokyo for three decades, hosted two self-defense workshops in June. \nAbout 30 people, mostly elderly Asian American women, attended each class. He is considering holding a third. \n\"It's sad there is suddenly a need for more self-defense,\" Ishii said. \"It's sad that we can't just do martial arts or go about our business and exercise and walk and take life for granted.\" \nAt the July Seniors Fight Back class in Cerritos, Shirley Green said she wants the self-defense workshop to become a regular offering. \n\"This is excellent,\" said Green, 72, who is third-generation Japanese American. \"They should have it all the time.\" \nA husband and wife from Rossmoor said they no longer take walks in their neighborhood after seeing attacks against Asian American seniors on the news. \n\"It's always in the back of our minds,\" said the man, 77, who gave only his last name, Jen, because of fears for his safety. \"We've lived in the U.S. for over 30 years. This is the first time we've felt uncomfortable living here.\" \nAs the event wrapped up and the seniors delivered their final kicks, Cerritos Mayor Pro Tem Chuong Vo said, \"You're all trained fighters now.\" Everyone burst into laughter. \nCesar Echano, the Filipino American man assaulted at the park earlier this year, has fully healed from his injuries. \nHe hasn't gone back to the park yet but said he will soon. He said his attacker has not been arrested. \nAt another Seniors Fight Back class Aug. 5 in downtown Los Angeles, Scolesdang, the MMA fighter, barked out instructions that were translated into Korean. \nMore than 100 people, mostly elderly Korean women, listened as he described"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194602, Requested 8054. Please try again in 796ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194602, Requested 8054. Please try again in 796ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "When Sam Chen was growing up, his immigrant parents from Taiwan were clear about how he should handle any anti-Asian bias he might confront at school or in the community.\nKeep your head down and work hard.\n\"Their mindset was that we were guests in this country,\" Chen said. \"We were to swallow injustices because we have an opportunity to live in a free country.\"\nChen, an Allentown-based political analyst, professor and TV host, said he wonders how this attitude has helped make Asian Americans an easier target for those acting out their racism and hate.\n\"I tell my Asian colleagues and community members that it is incumbent on us to speak,\" Chen said. \"The first step to activism is awareness. We cannot expect our white friends, our Black friends and our Latino friends to have experienced the same things we've experienced.\"\nAnti-Asian crimes and incidents have spiked across the country over the last year as some leaders, including then-President Donald Trump, used inflammatory language to describe the coronavirus, which started in China. Stop AAPI Hate, a nonprofit social organization tracking incidents of discrimination, hate and xenophobia against Asian Americans and Pacific Islanders in the U.S., reported 3,795 incidents from March 2020 to February of this year. Incidents ranged from shunning to vandalism to physical violence.\nCalifornia and New York led the nation with incidents, with 1,691 and 517 respectively, but Pennsylvania ranked fifth with \n97 reported incidents. According to U.S. Census data, the Lehigh Valley's Asian population stands at a little over 3%. Within that population are dozens of different ethnicities, many with their own languages and customs.\nBut though headlines across the country have kept them tense and vigilant, Asian American business owners and community leaders in the Lehigh Valley say they've felt fortunate to escape the wave of anti-Asian incidents and crimes.\nBut this doesn't mean these things don't happen, according to Mohan Seshadri, co-executive director for Pennsylvania's Asian Pacific Islander Political Alliance. Historically, incidents like these are woefully underreported, he said, with issues like language barriers creating enormous hurdles.\nSome of the most harrowing incidents of anti-Asian crime have happened this year. Recent headlines include a New York City stabbing and a brutal robbery and beating in Oakland, California.\nBut the most horrifying incident so far is the March 16 shooting spree at Atlanta-area spas where eight people, six of Asian descent, were killed. Police have been criticized for reporting that alleged shooter Robert Long's claim that he lacked a racial motivation could absolve him of hate crimes.\nRacism directed at Asian people is nothing new in America, Seshadri said, but the rhetoric around COVID-19 created a convenient new scapegoat.\n\"The pandemic created this emotional response, and then to point those folks who are fired up at Asians, it was just going to lead to violence,\" he said. \"This was a very simple equation that I don't think anyone should have been surprised about.\"\nBryan Lu, of Bethlehem, is the son of Rakkii Ramen chef and founder Marco Lu, and he helps his father run the family's expanding Japanese noodle house business. Last March, when Lu heard Trump beginning to refer to COVID-19 as \"the Chinese virus\" and \"kung flu,\" he didn't think much of it.\n\"I didn't think people would take it literally,\" he said.\nBut it became clear that many did. Fortunately, Lu said, he spent most of the time reading about incidents elsewhere rather than seeing them play out in his own community.\n\"We're just thankful it didn't happen here,\" he said.\nLobynn Cha, owner of Little Miss Korea in the Allentown Fairground Farmers Market, said she and her family arrived in the U.S. when she was 9 and were quick to immerse themselves into the culture. She suspects her fluent English saves her from potential harassment, and though she's felt safe in the Valley, the recent attacks have her feeling less secure.\n\"I'm more vigilant these days for sure,\" Cha said. \"I wish people would see that we're all humans. We all came here from somewhere or another and we're all scared of COVID together.\"\nFor some, a sense of exasperation at years of turning the other cheek has set in. Jay Ho grew up in the Lehigh Valley and has taken over the family business, Ho Ho Incorporated, which includes Hunan Springs restaurant in Lower Macungie Township. He now spends most days at his new South Philadelphia restaurant Mei Mei, where he finds the camaraderie of other Asian restaurateurs refreshing.\n\"I know my parents didn't want any kind of altercation, but now it feels like we're easy targets,\" said Ho. \"There is a generational change with that mentality. We live here. We're as entitled to be here as anyone else.\"\nHo said Philly's support network of Asian neighborhoods is a far cry from the feeling of isolation he was accustomed to in the Lehigh Valley, where Asian Americans are scattered throughout the region.\nMark Cho, senior pastor at the Korean Church of the Lehigh Valley in Whitehall Township, tries to combat that sense of isolation among his congregation, encouraging them to reach out to one another.\n\"I preach and remind them, you are not alone,\" Cho said. \"We are in this together.\"\nCho said he encourages his parishioners to report incidents of anti-Asian bias, violence or vandalism to him if they do not feel comfortable turning to police. He's been relieved so far to see that it's not been a local problem, but Atlanta's violence has him bracing in a new way.\nHe hopes that people outside of the Asian community can understand how unsettling these incidents have been for so many.\n\"If your neighbor fears for their safety, that is not a good community,\" Cho said.\nThe solution to that, Chen"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197685, Requested 7863. Please try again in 1.664s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197685, Requested 7863. Please try again in 1.664s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Abby Bogart was called a virus at the start of the COVID-19 pandemic in March 2020. \nThe 13-year-old, who is both Asian American and Black, was deeply hurt by the remark, which she says was made by someone she had considered to be a close friend at \nthe time. \n\"I remember that day bawling to my mom,\" she said. \"We were just sitting there hugging each other and talking it out. We were just very emotional and it was a very sad night.\" \nOne in eight Asian Americans reported hate incidents in 2020, according to a national coalition battling the problem. And one in 10 reported anti-Asian hate incidents in the first quarter of 2021, including physical assaults, verbal harassment, civil rights violations and online harassment. \nIt's an issue that affects adults and children, according to the latest data from the San Francisco-based nonprofit Stop AAPI Hate. \nOf those who reported hate incidents in 2020, 16% came from Asian Americans ages 12 to 20. And about 80% of them reported being bullied or verbally harassed, often at school, Stop AAPI Hate found. \nRacism against people of Asian descent is an ongoing problem, \"but this pandemic, it has been more active,\" said Bogart, an eighth-grader at Metro Early College Middle School, a public STEM school on the Northwest Side. \"Since the pandemic, I feel like it's more blatant.\" \nHate crimes against Asian-Americans overall reached an \"alarming level\" in the United States, according to an August report by the United Nations. More than 1,800 racist incidents against Asian Americans were reported from March to May. \nThe attacks included physical assaults, vandalism, verbal harassment, denial of access to services and public spaces, the report said. Victims also reported being spat on, blocked from public transportation, discriminated against in workplaces, shunned, beaten, stabbed, and insulted as transmitters of the coronavirus. \nIn March, a Georgia man went on a shooting spree at three Atlanta spas, killing eight people, including six Asian women, provoking fear in a community already feeling fearful and intimidated in the wake of the coronavirus pandemic. \nA month earlier, a 61-year-old Filipino American was slashed across the face with a box cutter and left bleeding inside a New York City subway train as his assailant fled. \nAnd in late January, an 84-year-old Thai immigrant in San Francisco was violently shoved to the ground and died after what his family called an attack \"driven by hate\" and prosecutors said was horrific and senseless. \nWith the rise in hate incidents nationwide and the new school year less than a month away, many Asian American students in central Ohio are bracing for the potential of new or continued verbal assaults by fellow students when they return to school in the fall. \n\"I am obviously concerned about possible instances that could occur regarding my race,\" said Lou Cariello, who is Asian American. \nThe 17-year-old senior at Bexley High School chose to do her schooling remotely last year because of concerns about contracting COVID-19 and xenophobic comments by classmates about Asians and the origin of the virus. \n\"I was scared to even walk around my neighborhood with my dogs because I didn't want anyone to look at me as they were driving by or say something,\" she said. \nAbout 2% of Bexley school students are Asian American or Pacific Islander, according to the most recent data from the Ohio Department of Education. At Columbus City Schools, the state's largest district, the percentage is nearly 4%.\nCariello, who was adopted from China when she was a year old, remembers some students asking her racist questions even before the pandemic such as, 'Do you support eating dogs?', 'Why are you eating that weird food?' and 'Are you a communist?' \n\"I didn't want to imagine what it was going to be like if I went back to school in-person in the midst of everything going on,\" she said.\nShe currently plans to go back to school in person for her senior year and said she has a couple of teachers she feels comfortable confiding in if something were to happen. \nRacist incidents targeting Asian Americans occurred before COVID-19, but the pandemic increased the frequency and level of bullying, said Sharon Kim, a member of Ohio Progressive Asian Women's Leadership, a grassroots community group committed to empowering women of Asian and Pacific Islander descent. \n\"Being targeted because of someone's race or ethnicity or whatever difference, these are concerns that us parents have had even before the pandemic,\" she said. \"The sad fact of the matter is the pandemic ... just really increased the bullying that was already present.\" \nEven before their son Elliot was born, she and her husband worried about him being harassed and bullied because of his Asian ethnicity. \n\"It's not if it's going to happen, it's when it's going to happen,\" Kim said. \nElliot, now 5, is going to be a first-grader at Ecole Kenwood French Immersion Elementary School in Columbus City Schools. He recently told his mother, she said, how someone pulled back their eyes at him as part of a racist gesture in 2019 when he was in preschool. \n\"It's pretty heartbreaking,\" she said. \"It's just a reminder that this kind of racialized experience is already in his life experience as something that's probably going to happen again.\" \nThe pandemic has \"really shown how the world views Asian Americans and it's been incredibly disheartening,\" said Annie Johnson, a junior at Dublin Coffman High School. \nShe said schools should take action against students who make racist remarks. \n\"My race, my heritage, my culture is not a joke. It's not your punchline,\" said Johnson who is Asian American and"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197301, Requested 7367. Please try again in 1.4s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197301, Requested 7367. Please try again in 1.4s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Asian Americans have been fighting two viruses during the pandemic: COVID-19 and hate.\nThat was the message Tuesday night at a vigil at the University of Central Florida honoring the eight victims, including six Asians, who were shot to death by a gunman last week at Atlanta-area spas.\n\"Hate is a virus. Hate can spread just as quickly as Covid-19 if we let it,\" said Mimi Chan of the Wah Lum Kung Fu and Tai Chi Temple in Orlando.\nAn estimated 150 to 200 people gathered at the event thatincluded a moment of silence, live music, prayer, speakers, a lantern ceremony and an open forum.\nTogether Chan urged that legislation condemning hate crimes against Asian Americans to be passed. Law enforcement agencies should also take an active role in hate crimes as well as hate incidents, which can be instances of harassment or verbal abuse, she said.\nThe livestreamed event brought together Asian Americans on campus and local community groups. Participants sat on the grass around the Reflection Pond facing Millican Hall.\n\"Hate in any form will not be tolerated on our UCF campuses, because our commitment to valuing excellence that comes from our differences is stronger than hate,\" said Dr. Edwanna Andrews of UCF Student Development and Enrollment Services.\nPastor Danny Chen, director of the campus InterVarsity Christian Fellowship, described Asian Americans as historically voiceless, politically powerless, and shown last week to be disposable.\nHe shared a painful family secret that his grandmother,who out of desperation as a single mother, worked in the sex indus\ntry at a massage parlor to put food on the table.\n\"I think we're so quick to attach some kind of social stigma against women in this industry,\" he said. \"It's much easier for us to see them as morally loose, as sexually promiscuous rather than to actually consider the reality that these are mothers and sisters and daughters who are gripped by the bondage of poverty and sexism and misogyny subjected to the exploitation of a broken humanity. We often forget that they too were created with dignity whose bodies reflected the very image of God.\"\nImam Abdullah Saqib of the Islamic Center of Orlando and President of the Muslim student association at UCF said he stands with \"our brothers and sisters from the Asian community.\"\n\"We also stand with anyone who is targeted, persecuted, abused, discriminated against or hurt because of their race, ethnicity or religion,\" he said.\n\"I say terrorism and violence has no religion,\" said Saqib. \"The only thing that could be associated with it is hatred and ignorance.\"\nSome of the students attending the event said they were shaken by the Atlanta spa shooting.\nJanine Do, a third-year marketing major at UCF, she said her mom owns a nail salon in the Tampa area. Since the Atlanta homicides, she locks the door between customers. Her mother has also put a sign up at the store stating she is not Chinese, but Vietnamese.\n\"It didn't really hit me until now,\" said Do, tearing up.\nAnother student said since the pandemic started, she avoids going to the grocery store alone. The Atlanta shootings have made her feel frustrated, angry and numb.\n\"It's been a difficult ride,\" said Juli Meyer, a psychology major at Valencia College who identifies as Thai American.\n\"My hope for today is to be a supporter, and to be in a safe space, and be welcomed,\" she said, \"Hopefully this event will bring more awareness to hate crimes in the Asian community.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193571, Requested 7046. Please try again in 185ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193571, Requested 7046. Please try again in 185ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "\"People tend to think that we're fine, that Asians are fine because we tend to stay quiet because we don't speak up as often as others, but we are not OK. We have lived in fear for so long. We love the United States, this is our home but now we don't feel safe.\"\nSunny Shuai\nChinese immigrant and longtime Indiana resident\nDays before a shooting in Atlanta that left eight people dead, six of them Asian women, Asian-American and Pacific Islander women sounded the alarm in Indiana.\n\"This is something people need to recognize,\" Ariana Cavallini said.\n\"These hate incidents, this white supremacy is hurting us. It's happening across the country and it's happening locally to Asian Americans.\"\nA suspect was arrested and charged with murder and assault after shootings at three Atlanta-area spas on Tuesday.\nLocal law enforcement stated the suspect has said his alleged actions were not motivated by race, but anti-Asian harassment and hate crimes have surged to more than 3,000 nation-wide in the past year.\nThe incidents are tied to the spread of the COVID-19 pandemic and many were violent.\nThese events have ignited some Asian Hoosiers, many of whom have long felt invisible and not heard, to speak out and advocate on behalf of others in their community about the discrimination and racism Asians are facing and have been facing for years.\nAnd they want Indiana Governor Eric Holcomb to act.\n\"It's unfortunate that it had to take death and murder,\" Cavallini said, \"for people to start listening to us.\"\nThe 24-year-old Filipino American Hoosier is a member of the Indiana Chapter of the National Asian Pacific Islander Women's Forum (NAPAWF), a group based in Indianapolis and Bloomington founded last year to meet the needs of Asian American and Pacific Islander women, girls, and the community.\nThe group, which also locally and nation-wide works to make an impact on public policy to drive systemic change, gathered nearly 2,000 signatures in a petition from Asian Hoosiers, community leaders and allies asking Holcomb to establish an Asian American and Pacific Islander statewide advisory committee that can help address issues such as racism, discrimination, language access, health care resources and legal help, among others.\nThe petition, which was delivered to the governor on March 11, also asks Holcomb to implement programs and resources to support the Asian American and Pacific Islander communities in Indiana without escalating law enforcement and to condemn these hate crimes publicly.\nAs of Thursday, the group had not gotten a response from the governor's office on the petition or guidance about how to move forward with an advisory committee.\nOn Wednesday, during a weekly COVID-19 press conference Holcomb spoke on anti-Asian hate crimes after a journalist asked him what he thought about the incident in Atlanta and the rise of anti-Asian hate crimes but he didn't address the petition.\n\"There is no place for it,\" he said.\n\"Not just in Indiana, but in the country.\"\nAnti-Asian hate crimes\nIn an emailed statement to IndyStar, Holcomb referenced Indiana's hate crimes bill, which he signed in 2019. The bill allows judges to consider bias when handing down criminal sentences based on a victim's real or perceived traits. That includes skin color, creed, disability, national origin, race, religion and sexual orientation.\nThe bill excludes gender, gender identity and age.\n\"Anyone that seeks to terrorize or cause fear needs to be held accountable for that. Racism is counter to Hoosier values. It's not what we stand for,\" he said in the statement.\n\"There is no higher priority than the safety and security of Hoosiers.\"\nNationwide Asian women are targets of anti-Asian racism more often than men. According to a report by Stop AAPI Hate, women made up 68% of the reports, compared to men, who made up 29%.\nAlthough there's no data yet available about the total of anti-Asian harassment incidents in Indiana, according to NAPAWF, some incidents have been documented in Plymouth, Chesterton, Martinsville and Mooresville.\nThe Anti-Defamation League tracked increases in white supremacist hate propaganda activity. In 2020, 57 such incidents were reported in Indiana, which is a 24% increase from 46 incidents in 2019.\n'We are not OK'\nSunny Shuai, 44, said Asian Hoosiers have been scared to go out in public since the pandemic started due to the rise in discrimination.\nShuai immigrated from China and has been calling Indiana home for more than two decades.\n\"People tend to think that we're fine, that Asians are fine because we tend to stay quiet because we don't speak up as often as others, but we are not OK,\" she said. \"We have lived in fear for so long. We love the United States, this is our home but now we don't feel safe.\"\nSome Asian Hoosiers believe more outreach from elected officials and law enforcement is needed to build trust and prevent more fear from spreading in the community\nLiz Prevot, 35, of Greenwood said Asian Hoosiers deserve to feel protected.\n\"As we watch these hate crimes go up, not only an advisory council is needed but we should be able to get to know our police officers and to understand what our rights are,\" Prevot said. \"We should be able to trust our police officers and we need to be working with them.\"\nAccording to Census data, Asian Hoosiers make up nearly 4% of Marion County's population and Native Hawaiian and Pacific Islander Hoosiers make up less than 1%.\nThe population has grown in some Indianapolis suburbs and surrounding counties.\nAsian Hoosiers make up 6% of Hamilton County's population, 7% of Monroe County's and 4% of Johnson County's population.\nStatewide, Asian Hoosiers make up nearly"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196239, Requested 7020. Please try again in 977ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196239, Requested 7020. Please try again in 977ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "take long for us to encounter guns. My parents opened a Vietnamese grocery store in San Jose, California, in the late 1970s, where they were shot in their store one Christmas Eve. Flesh wounds, fortunately. A few years later, a gunman pushed his way into our house and pointed a gun in all our faces. I remember that the barrel was thin and long. My father and I did as we were told and knelt. My mother saved us all by running past the gunman and into the street, screaming for help. \n \n- \nThere is a long history of anti-Asian violence in the US. The positioning of Asian Americans as a model minority since the 60s in other words, as desirable neighbours, classmates, co-workers and sexual partners has masked the latent anti-Asian feeling that has existed in this country as long as there have been Asian immigrants and Asian Americans. \nI write from Los Angeles, where, in 1871, 19 Chinese men were massacred by an armed mob of hundreds. Throughout the American west of the 19th century, Chinese immigrants were targeted for murder, beatings and violent evictions. They had been brought to the US to help build the transcontinental railroad, but when their usefulness was over, newspapers and politicians stoked white working-class fear of Chinese economic competition; the result was violence. This climaxed in the 1882 Chinese Exclusion Act. This act was prefaced by the 1875 Page Act, which prevented women suspected of low morality from entering the country, the assumption behind it being that all Chinese women were potential sex workers. \nThis idea that Chinese women specifically, and Asian women in general, are both sexual temptation and sexual threat, to be used or to be punished, or both, seems directly connected, a century and a half later, to the self-proclaimed impulse that drove the Atlanta gunman to Asian massage parlours. Over that century and a half, with American wars in Asia bringing hundreds of thousands of American servicemen into contact with Asian women, both as sex workers and as brides, \"the Asian woman became an object of hatred, and lust, a thing to loathe, then desire, the distance between yellow peril and yellow fever measured in flashes\", as the journalist. \nYellow peril and \"yellow fever\" - the fear of an Asian invasion, on the one hand, and the desire for Asian women, on the other - are not exclusively American phenomena. The fusion of these opposites arose through orientalism: as Edward Said argued, British and French colonisers justified their conquest of the east with narratives of oriental inferiority, docility, lust and treachery. Said was mostly thinking about the Middle East and north Africa, of Arabs and Muslims, but these exoticising and eroticising narratives extend to east and south-east Asia. The oriental as Other is a mask that can be slipped on many different people for different reasons. \nBefore the pandemic, for example, the Chinese in France already felt that they were being as victims of crime. During the pandemic, Chinese and other Asians have being called bearers of Covid-19, even adopting the hashtag #JeNeSuis PasUnVirus (I'm Not a Virus). The spike in anti-Asian sentiment has been global. \"From the UK to Australia, reports of anti-east and anti-south-east Asian hate crimes have increased in western countries as the pandemic took hold this past year, reports, including a 96% increase in hate crimes against people of East Asian appearance in the UK between June and September 2020. In Germany, where Vietnamese migrant workers were the subject of racist riots in the 90s, there has been a in anti-Asian sentiment,. There has been harassment, shunning and bullying of, and more of the same in, and, with women being the majority of the victims in the latter two countries. And these are only the reported incidents. \nThe pervasiveness of these verbal and physical attacks, triggered in part by Donald Trump and others characterising Covid-19 as the \"China virus\" and the \"Kung flu\", suggests a deep well of anti-Asian racism. When I was a child, a couple of my pre-teen classmates were familiar enough with Asian stereotypes to pull their eyes into slants and ask me if I had carried an AK-47 in the war in Vietnam (I was four years old when it ended). During the 15 months I have spent in France over the years I was called Chinois by both a black woman and a white woman and mocked with a ching-chong accent by teenagers in Provence, which was more times than I had been called \"Chink\" in a lifetime in the US. \nRacism and sexism spin like coins, two-faced. With one spin of the coin, we can be the model minority and the object of 'yellow fever' \nIn America, of course, as elsewhere, I was bombarded by long-distance racism in the form of jokes on the radio, cliches in the movies and the fear-mongering of politicians. All of this occurred while Asian Americans were also being portrayed as the \"model minority\" that knew how to study hard, work diligently and keep quiet. The Committed continues the story of the man of two faces as he relocates to the immigrant quarters of Paris in the early 80s. The French people of Vietnamese descent that I spoke to mostly agreed that the French, as a whole, liked them because of these qualities. Almost no one discussed the idea that their acceptability was due not only to who they were, but who they weren't: Arab, Muslim or black, the usual targets of French racism. \nAsian acceptability has always been contingent, whether we are the model minority in the US or the \"\" in the UK. Racism and sexism spin like coins, two-faced."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197422, Requested 6640. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197422, Requested 6640. Please try again in 1.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "French people of Vietnamese descent that I spoke to mostly agreed that the French, as a whole, liked them because of these qualities. Almost no one discussed the idea that their acceptability was due not only to who they were, but who they weren't: Arab, Muslim or black, the usual targets of French racism. \nAsian acceptability has always been contingent, whether we are the model minority in the US or the \"\" in the UK. Racism and sexism spin like coins, two-faced. With one spin of the coin, we can be the model minority and the object of \"yellow fever\". With another spin, we can be the Asian invasion and the yellow peril. The French who colonised Vietnam, Laos and Cambodia were just as prone to these fevered fantasies as the Americans. \n... \nThe ease with which \"racist love\" can turn to \"racist hate\" for an Asian minority or the Asian colonised is mirrored, in the, in the way that Europe and the US have often looked to Asia as the source of wealth and danger. Take the American attitude towards China. In the 19th century, China was the ultimate goal of American expansionism; now that it is not so much a country to be exploited but a major competitor, the Obama, Trump and Biden administrations have adopted a consistent anti- China stance. \nPositioning China as America's No 1 threat will inevitably. And since many Americans \n- \nand Europeans - cannot tell the difference between Chinese and other Asians, all Asian Americans and Asian Europeans will suffer the consequences. In the US, the most notorious incident of such racist misidentification occurred in 1982 in Detroit, when two white autoworkers beat Vincent Chin to death, mistaking the Chinese American for a Japanese. The autoworkers were upset at Japanese economic competition in the auto industry, a fear stoked by widespread discussion of a trade war with Japan. \nThe systemic violence of a US foreign policy designed to kill Asians in large numbers, or threaten to kill them, from the Philippines to Japan, from Korea to Vietnam, from Laos to Cambodia, reinforces the domestic, everyday racism and sexism with which many Asian Americans are familiar. The acceptability of microaggressions, racist jokes, casual sexual fetishisation lays the groundwork for an explosion of racist and sexist violence that can be literally murderous. \nCalls to stop anti-Asian hate will have limited impact without an awareness of the enduring history of anti-Asian violence carried out in American wars in Asia and European colonisation. The French navy in 1946 and killed 6,000 Vietnamese people. I mention the shelling in The Committed: \"The French saw our shared past as a tragic happenstance of history, a romantic love story gone wrong, which was half correct, whereas I saw our past as a crime that they had committed, which was completely correct.\" Ironically, many Asians fled or migrated to the very countries that had colonised them or fought wars on their lands. \nFollowing these paths, the gunman's Asian victims and found varying degrees of success. A few were working class, a couple were on the lower end of the middle class, with only one - Xiaojie Tan - the owner of her own business, Youngs Asian Massage. Of the two other victims, Delaina Ashley Yaun was a customer at Youngs and Paul Andre Michels was its handyman. Youngs is located in a shopping centre called Cherokee Village, named after the Cherokee people that once lived in Georgia. The American military forcibly expelled the Cherokee from Georgia in 1838 and compelled them to migrate west on what became known as the Trail of Tears, or, as the Cherokee called it, the Trail Where They Cried. More than 4,000 perished. \nYong Ae Yue, Hyun Jung Grant, Suncha Kim, Soon Chung Park, Daoyou Feng and Xiaojie Tan may or may not have known of this history. But when Asian immigrants and refugees come to claim our share of the American Dream, this history is also what we daim. And sometimes this history claims us. \n The Committed is published by Corsair (18.99). To order a copy go to. Delivery charges may apply."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196711, Requested 7768. Please try again in 1.343s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196711, Requested 7768. Please try again in 1.343s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "When I found out about the shootings in Atlanta, the massacre of six women of Asian descent and two others by a white man, I started crying so hard I couldn't stop. I thought about my Asian American friends, my parents who work in small Chinese businesses, all the Asian women I know who raised me, who taught me, whose immigrant blood runs through mine. I thought about myself, how I grew up wanting to be American, not realizing I already was by my birthright, because as the extreme violence in Atlanta showed, the country does not recognize Asian people as part of the American identity.\nFirst, let me say the women's names. Xiaojie Tan. Daoyou Feng. Soon Chung Park. Hyun Jung Grant. Suncha Kim. Yong Ae Yue. These were people with full lives. They had families who love them and now must mourn them. It doesn't matter what these people might have done for a living or what their citizenship status was or how \"American\" they were. The media's reluctance to call their murders a hate crime and their participation in racist stereotypes that sexualize Asian women is egregious. The superficial demonstrations of solidarity and understanding on social media are downright insulting.\nOur failure to do the victims justice is indicative of how poorly we understand race in the United States. It is the product of our erasure of Asian Americans in U.S. history. We are not taught the contributions of Asian Americans in school. We do not read Asian American stories or see Asian Americans on TV with any regularity. The way we talk about and portray race in America is Black and white and renders Asian Americans invisible. We do not get to humanize them and see them as real people. So when we as a country are forced to confront the racist attacks against Asian Americans, we fail miserably.\nThe rise in anti-Asian sentiment has been going on for a full year, but it's important to remember that this is not the first wave. It frustrates me how quickly we forget that less than 80 years ago, one of our most progressive and revered presidents, Franklin D. Roosevelt, ordered all Japanese Americans into concentration camps. Or how one of the most violent lynchings in American history was done to the Chinese. And let's not forget the incredible abuse and racial profiling South Asians endured and continue to endure post 9/11.\nEvery time I look at this country's history, I see that women like me have always been denied a place. Afong Moy, in 1834, the first recognized Chinese woman to come to the U.S., was turned into an exhibition to American people, carted off to cities where people paid 25 cents to see her. This is also not the first time the media has failed Asian Americans. News outlets have long been criticized for failing to cover issues in Asian American communities, further marginalizing them in local decision-making, or when they do, for perpetuating stereotypes of the model minority. Likewise, while some blame former President Donald Trump for contributing to the rise in anti-Asian sentiment by calling COVID-19 \"Kung-flu\" or the \"China Virus,\" he's not the first politician to use such language. Senators called Chinese immigrants \"rats,\" \"beasts,\" and \"swine\" before passing the 1882 Chinese Exclusion Act, the first and only federal law to limit a group's entry based on race.\nWords have power. The stories we select to construct our national identity have consequences. While Asian Americans have been deemed perpetually foreign, many would be surprised to know that the first Asians in America can be traced back to as early as the 1500s. Why Asian Americans don't make up a larger part of the U.S. population is a result of centuries of discriminatory immigration laws, not to mention labor and education ones.\nA friend asked me recently, \"Well, how do you want people to respond?\" I didn't know what to say. I still don't. Any chance at racial justice will require a radical reformation of how we think about race in America. It will require partnerships with other minority groups. Statements of solidarity must be backed by action. As history shows, anti-Asian sentiment repeats itself. The current terms we use, \"exotification,\" \"sexualization,\" \"exclusion,\" \"model minority,\" help us conceptualize and label the racism that Asian Americans experience, but I worry they are clinical and prevent us from feeling the empathy that is so needed at this moment.\nWhat I will say is this. I want to see hate-crime reform. I want Asian American history taught in classrooms. I want to see more Asian Americans in government, in Hollywood, on everyday bookshelves, so that one day someone might look at me and not question whether I belong here. So that someone might look at me and not make assumptions because of my race. I already know people like me can serve the country well, that they can be dynamic and brilliant and brave. America must see that too."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195429, Requested 7217. Please try again in 793ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195429, Requested 7217. Please try again in 793ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "President Biden has sought to blunt a reported surge in anti-Asian bias incidents by ordering the federal government not to use xenophobic language to describe the coronavirus and by calling \"vicious hate crimes\" during the pandemic \"un-American.\" \nBut Asian American leaders are warning that a deepening geopolitical confrontation between the United States and China is contributing to the heightened suspicion, prejudice and violence against their communities in ways that could continue to intensify even after the pandemic begins to subside.\nAdvocates called Biden's rhetorical efforts a welcome corrective to President Donald Trump, who railed against the \"China virus\" and \"kung flu.\" Yet the broadening conflict among the world's two largest economies - on trade, defense, 5G networks, cybersecurity, the environment, health security and human rights - has contributed to a growing number of Americans calling China the \"greatest enemy\" of the United States, according to a Gallup poll this week. \nIn the survey, 45 percent of respondents named China as the top threat, more than twice as many as a year earlier, when the country was ranked on par with Russia. Democrats and Republicans have voiced bipartisan support for a tougher U.S. policy, including economic sanctions on Beijing over cyber-intrusions, human rights violations and crackdowns on democracy in Hong Kong. \n\"When America China-bashes, then Chinese get bashed, and so do those who look Chinese. American foreign policy in Asia is American domestic policy for Asians,\" said Russell Jeung, a history professor at San Francisco State University who last year helped found Stop AAPI Hate (AAPI stands for Asian American and Pacific Islanders). The advocacy group has tallied more than 3,000 incidents of bias and hate during the pandemic. \n\"The U.S.-China cold war - and especially the Republican strategy of scapegoating and attacking China for the virus - incited racism and hatred toward Asian Americans,\" Jeung said.\nA number of violent assaults on Asian Americans over the past two months, including some that went viral after being caught on video, have drawn political and media attention to escalating fears over public safety. The slaying of eight people, including six women of Asian descent, at three Georgia spas on Tuesday sparked demands for an urgent response from authorities. Police arrested Robert Aaron Long, a White man, in connection with the killings and cited as a potential motive Long's interest in eliminating \"sexual temptation.\"\nNot all of the cases appear to have a link to anger over the pandemic or were necessarily motivated by racial resentment. But advocates said they have collected enough anecdotal evidence through self-reporting portals set up last year by community groups to illustrate that attacks are spiking. \nAnd they are fearful that the intensifying competition between Washington and Beijing is contributing to scapegoating of Asian Americans in echoes of earlier periods of widespread hostility during geopolitical tumult and heightened nationalism in the United States. \nThey pointed to the Chinese Exclusion Act in 1882 that banned the immigration of Chinese laborers amid national economic anxiety, the internment of more than 120,000 Japanese Americans during World War II, and attacks on mosques and Muslim Americans in the wake of the 9/11 attacks.\n\"For as long as Asians have been in America, we've been scapegoated, treated as outsiders and seen as untrustworthy. Too often, these prejudices are exploited for political gain,\" said Christopher Lu, who served as White House Cabinet secretary and deputy labor secretary in the Obama administration. \"As troubling as our current situation is, I am concerned that things are going to get much worse as U.S.-China tensions grow.\"\nAsian American leaders acknowledged the need for the United States to develop a tougher strategy to counter the Chinese Communist Party's influence campaign across the globe. The question, activists said, is how the federal government and elected officials speak publicly about the challenge and how far they go to counter it. \nTrump sought to blame Beijing for the outbreak of the pandemic, employing xenophobic language that was echoed by his supporters and other Republican officials and condemned by Democrats. \nBut in April, weeks after the onset of the pandemic, Biden also drew heat from Democratic Congress members and community groups for a campaign advertisement that accused the 45th president of having \"rolled over for the Chinese\" in managing the coronavirus and cast China as a looming threat. \nBiden's campaign apologized for the language and aired a revised version of the ad. But the episode illustrated the intensifying effort of both parties to appear tougher on Beijing. \n\"It is clearly a difficult line to walk. However, I do believe there is a way of disagreeing with China's policies without denigrating the Chinese people themselves,\" said Rep. Judy Chu (D-Calif.), chair of the House Congressional Asian Pacific American Caucus. \nChu praised Biden for issuing an executive action in January aimed at barring federal agencies from blaming China for the pandemic and instructing the Justice Department to improve data collection on hate crimes. In a prime-time address to the nation last week about his administration's coronavirus response, Biden decried attacks and harassment against Asian Americans who have been \"forced to live in fear for their lives just walking down streets.\" \n\"It must stop,\" Biden said. \nWhite House aides, including domestic policy adviser Susan Rice and senior adviser Cedric L. Richmond, met virtually with Asian American advocacy groups two weeks ago to hear their concerns. They pledged to use the power of the administration to combat violence but offered few specifics, according to activists who participated, who, like others, spoke on the condition of anonymity to discuss a private conversation.\nChu and others have pushed Biden to elevate more Asian Americans to high-level jobs in his administration, noting there is only one Cabinet-level official of East Asian descent, Katherine Tai, who was confirmed Wednesday as U.S. trade representative.\nAttorney General Merrick Garland met with advocates on a video call Wednesday that lasted about 45 minutes, telling them he recognized that regardless"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199107, Requested 8060. Please try again in 2.15s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199107, Requested 8060. Please try again in 2.15s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Mira Yusef was flooded with terror, anger, frustration and grief as she watched from her Des Moines home while the news of Tuesday's shooting rampage in Atlanta unfolded.\nRobert Aaron Long, 21, of Woodstock, Georgia, is accused of opening fire at three spas in the Atlanta area, killing eight people  most of whom were Asian women. Authorities identified the victims on Friday as Soon C. Park, Hyun Jung Grant, Suncha Kim, Yong A. Yue, Delaina Ashley Yaun, Paul Andre Michels, Xiaojie Tan and Daoyou Feng.\nAtlanta police say it's too soon to label the killings as a hate crime, despite a national outcry to do so from experts, activists and Asian Americans across the country.\nWhile Long claimed the killings were motivated by a self-proclaimed sex addiction, many Asians and Pacific Islanders nationwide, including here in Iowa, have pointed to the carnage as yet another example of a rise in violent attacks against Asian Americans. They view the attacks as fueled by racism and xenophobia during the COVID-19 pandemic and anti-China rhetoric conveyed by former President Donald Trump and other prominent conservatives.\nIn Iowa, recent reports of anti-Asian racism have mainly taken the form of increased microaggressions against people perceived to be from China, where the initial outbreak of COVID-19 was first disclosed in January 2020. But Tuesday's killings were not the first time Yusef, who is Filipina, has had to join in collectively grieving a violent attack against her community.\nYusef said Thursday that, while the increase of violence against Asians and Pacific Islanders is \"terrifying,\" it's worth noting that racism against those groups is nothing new. Rather, the increase in anti-Asian attacks during the COVID-19 pandemic has only highlighted deeply rooted historical and transnational racism against Asians and Pacific Islanders and violence against Asian women, she said.\nYusef cited many examples, including the U.S. Chinese Exclusion Act of 1882; American colonialism in Asian countries; domestic and sexual violence due to harmful, hypersexual stereotypes of \"docile, submissive\" Asian women; and, during the pandemic, the stabbing of three members of an Asian American family  including two small children  in Texas because the suspect thought they were \"Chinese and infecting people with the coronavirus,\" according to an FBI intelligence report.\n\"There is a lack of analysis  we cannot forget that anti-Asian sentiment is also deeply connected with anti-Blackness, xenophobia, white supremacy,\" said Yusef, co-founder of the Des Moines-based organization Monsoon Asians and Pacific Islanders in Solidarity.\n\"If (authorities) will just say, 'he is a sex addict or he had a bad day,' then we ignore the right questions: Why would he choose a massage parlor with low-wage Asian women employees? As Asian women, we are seen as invisible, submissive. It's like we're nothing.\"\nAdvocates: Microaggressions, racist attacks against Asian Iowans under-reported\nJust one month into the pandemic, Iowa's Asians and Pacific Islanders were already sharing stories among each other, and publicly, of increased racism against their communities and a growing fear of anti-Asian violence both locally and nationally.\nAsian Iowans previously shared with the Register information about microaggressions they had experienced. Some examples appeared explicit and included individuals being yelled at with \"coronavirus,\" a white couple leaving a grocery line to avoid standing next to an Asian American mother buying a birthday cake for her son, and a white woman leaving a Chinese restaurant after asking the owners if they were Chinese.\nOther examples were more subtle, such as tense looks or stares.\nIn 2019, there were 29,370 Asians and Pacific Islanders in the Des Moines metro area, according to the U.S. Census Bureau, or 4.2% of the population. That was up from 3.7% in 2015.\nIn 2017, 2.6% of Iowa's total population was Asian, according to the State Data Center, with Des Moines, Ames and Iowa City having the largest numbers. That percentage is projected to grow to 4.4% by 2050, the center reported.\nNu Huynh, executive director of the Iowa Asian Alliance, has been keeping tabs on potential racism and xenophobia tied to COVID-19 with an online form people can fill out for complaints. Since the start of the pandemic, she has been encouraging the community to report any type of racism or xenophobia, from microaggressions to violent attacks.\nSo far, she's received only a handful of complaints, Huynh said, the most violent being an egg thrown at an Asian Iowan.\n\"We just have been very fortunate here in Iowa that we have not seen the level of violence that we are seeing around the world,\" Huynh told the Register, acknowledging that it's possible there could be attacks and microaggressions that haven't been reported.\nDes Moines police data shows that aggravated assaults against Asian victims decreased from 33 incidents in 2019 to 24 in 2020. The Des Moines Police Department was still compiling data of other crimes against Asians, including harassment, as of Thursday night.\nLast year, the Iowa Civil Rights Commission took nine discrimination complaints from Iowans who identify as Asian. None has been filed in 2021.\n\"There's probably a lot of people that are experiencing (racism and xenophobia), but they're not saying anything,\" Huynh said. Obstacles to filing reports include fear of retaliation and victims not believing that will make a difference, she said.\nNationally, more than 3,800 incidents of racism and xenophobia against Asians and Pacific Islanders were recorded between March 2020 and February by Stop AAPI Hate, a San Francisco-based reporting center.\nThe majority of discrimination reported involved verbal harassment"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193467, Requested 7844. Please try again in 393ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193467, Requested 7844. Please try again in 393ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "native who lives in Cherry Hill. \n\"It is a way to wedge us out of the mainstream: You can take care of yourselves, you all live in these Chinatowns. We give you this glorious title, but it keeps us away, it makes us feel very foreign.\" \n\"We're thought of as self-supporting and people who make a lot of money, and that makes us targets,\" said Wen Gu, a librarian from Cherry Hill. \n\"Many Americans don't want to take the time to really leam about us, so they lump us all together,\" said Gao, who is on the faculty of Rutgers University's biomedical and health services department. \"They think we are all the same, we all eat weird food and speak a weird language, that we are not Americans.\" \n\"People don't notice us because we keep our heads down and we work hard,\" said Doris Zheng, a 21-year-old Rutgers-Camden student. Her family emigrated from a small city in Southeast China more than 20 years ago; her father is a chef and her mother is a home health aide. \nZheng, vice president of the university's Student Government Association, said she's supportive of Black and brown people's struggles with systemic racism in American. \"But Asian Americans are often hidden, in the dark, and there's this idea that we're smart and work hard and we don't need any help. \n\"There's this perception of us as prim and proper and perfect. We seem to be doing fine, but really that's because we are working so damn hard.\" \nThe model minority idea, she noted, is not only damaging to Asians it also minimizes the very real problems faced by Black and brown Americans, making others think they should overcome obstacles as well as Asians do, without understanding the differences in their histories, and the kinds of racism they encounter: \nIs Atlanta a tipping point? \nKim pointed to \"this moment of mourning\" for the victims of the Atlanta shootings and initial news of the shootings, which first characterized the shooter's sex addiction as a motive, drawing a strong backlash as an event that brought the problem of anti-Asian hate to light. \n \nIn a February address to the nation to acknowledge the loss of 500,000 Americans to COVID-19, President Joe Biden urged an end to attacks on Asian Americans, Kim noted. The President and Vice President Kamala Harris met with Asian American leaders in Atlanta on March 19. \n\"People have told me they finally feel heard and seen,\" said Kim. \"The question now is, is it going to be sustained, even after this tragedy falls out of the spotlight? Is this something durable that can lead to change? Many of us have been calling for that for at least a year.\" \n\"What bothers me about Atlanta is blaming a hate crime on a sex addiction, which this is clearly not,\" said Gao. \"This was obviously targeting Asian women.\" \nJinwei Cao recalled an incident in June, when apartment residents in Newark, Delaware, found fliers that said \"Kill Chinese virus\" at their doors. \nThe incident rattled students at the University of Delaware, where Cao teaches. While she was satisfied with the police and university responses to the fliers, she said \"it was really unacceptable.\" \n\"The Atlanta shootings, I heard from so many friends and students who felt like this is a breaking point,\" said Gao. \n\"It's threatening our very existence in this country.\" \nStepping up and weighing next steps \nEdward Wang said he's been in conference calls with other Asian American groups nationwide. \nThere is talk of a national march, like the Black Lives Matter marches that took place in the wake of the George Floyd killing in May. \nHe's hoping more attention - from politicians, policymakers, media and will lead to more knowledge of the struggles of Asian Americans, as well as a greater educators appreciation of their many different cultures, histories and contributions.\nAsian Americans, like so many other Americans, have stepped up to help in myriad ways during the pandemic. \n\"There are so many Asian Americans on the front lines,\" said Kim. \"Like the very large community of Filipino American nurses. Doctors. Small business owners.\" \nThe Asian American Alliance of South Jersey has organized food giveaways and collected donations for personal protective equipment to hospitals and first responders, its members said. They've delivered meals to local police departments and helped furloughed workers. \nXiaoping Ma, a physician who lives in Cherry Hill, heard alarming news from her native Wuhan, China, as the virus began to spread there. She recalls being among the first to always wear a face mask, warning others to do the same, and hearing the fear and desperation from friends, family and medical school classmates there. \nShe helped organize donation drives through the Chinese American Physicians Association, connected with colleagues in Wuhan as they learned more about how to contain the virus, sharing information and insight all while dealing with her own fear for family members and fellow medical professionals here and in Wuhan. \n \nMa, who has been in the United States since 1989 and has been a citizen since the 1990s, said she felt guilty that Wuhan was the place where the virus originated - but angry at the way it was characterized here. \n\"When the previous president called it 'the China virus,' the 'Wuhan virus,' it made me so angry, so afraid. How can an American president be so uneducated and say those things? But I tried to stay anchored on my work.\" \n\"We want people to know we are part of the community,\" said Gao. \n\"I see a lot of hope; a lot of people have reached out\" to show support, said Sharon Xiaoyun Hou. She's received support from the University of Pennsylvania, where she works. \"People checked in on me to see if I was"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194854, Requested 7363. Please try again in 665ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194854, Requested 7363. Please try again in 665ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK  A man attacked an Asian American woman in broad daylight and hurled anti-Asian insults at her as she walked down a midtown Manhattan street, New York City police said Tuesday.The woman, 65, was hospitalized with serious injuries after the attacker punched, kicked, and stomped on her Monday in front of an apartment building on 43rd Street, police said. Staff in the building did not intervene though a union representative said they called for help.In a statement, NYPD called the incident \"a hate crime assault\" and said its Hate Crime Task Force was investigating.Unsettling surveillance video shows the woman passing the building's open front doors when the attacker appears in the frame and immediately kicks her, knocking her to the ground. The man kicks and stomps on the woman multiple times before walking away.The incident occurred just before noon on Monday. NYPD released photos of the suspect Tuesday seeking tips from the public about the attack.The woman was taken to NYU Langone Hospital and was in stable condition, police said. She was discharged Tuesday evening, a hospital spokesperson said. Her name has not been released.Mayor Bill de Blasio called the incident absolutely disgusting and outrageous and said it was absolutely unacceptable that witnesses did not intervene.I dont care who you are, I dont care what you do, youve got to help your fellow New Yorker, de Blasio said Tuesday at a news conference. If you see someone being attacked, do whatever you can. Make noise. Call out whats happening. Go and try and help. Immediately call for help. Call 911. This is something where we all have to be part of the solution. We cant just stand back and watch a heinous act happening.Gov. Andrew Cuomo called the attack \"horrifying and repugnant\" and offered the NYPD assistance from the state polices Hate Crimes Task Force.Police Commissioner Dermot Shea told NY1 that the attack was \"disgusting.\"\"I dont know who attacks a 65-year-old woman and leaves her on the street like that,\" Shea said in an interview with the TV station.\"This is absolutely vile. These attacks against Asian-American New Yorkers must end. Hate has no place here and we must always call it out when we see it,\" City Council Speaker Corey Johnson tweeted.In a statement, the Brodsky Organization, the apartment building's management company, said it suspended the staff members who did not intervene in the incident pending an investigation.\"The Brodsky Organization condemns all forms of discrimination, racism, xenophobia, and violence against the Asian American community,\" the company said.In a statement, the head of the union representing the building workers disputed the allegation that they did not act and said the workers called for help immediately.Our union is working to get further details for a more complete account and urges the public to avoid a rush to judgment while the facts are determined, union President Kyle Bragg said.The incident follows a spike in anti-Asian hate crimes since the start of the COVID-19 pandemic.More than 3,795 incidents were reported to Stop AAPI Hate from March 19, 2020, to Feb. 28, 2021. The organization said that number is only a fraction of the number of hate incidents that actually occur.This month, a gunman opened fire at three Atlanta-area spas, killing eight people, mostly women of Asian descent.In the wake of the shootings, donations to Asian American and Pacific Islander groups spiked. About $24 million was pledged by nearly 30 philanthropic donors, according to an Associated Press review of an analysis by the philanthropy research group Candid."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196195, Requested 6999. Please try again in 958ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196195, Requested 6999. Please try again in 958ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "The recent rise in anti-Asian hate crimes are horrific and heartbreaking, but also just the latest instance of anti- Asian hate in our country. \nIn the past month alone, stories of hate crimes and racist language against Asian-Americans have become commonplace on social media and news outlets. These attacks don't just harm their victims, they also harm the mental and physical health of Asian-Americans and all Americans. \nAs an Asian-American physician, I would know. \nOf the 2,000 students who attended my high school in Omaha, Nebraska, there were a total of seven or eight Asians in the whole school - amid only 12 students of color. I felt the pressure of wanting to fit in of wanting to be white - and that pressure of course took a toll on my well-being. \nThe numerous instances of anti-Asian racism we experience - from seemingly less serious jokes about our eyes; to having, \"Go back to [insert wrong country here]!\" yelled at us; to being the victim of or witness to physical attacks have dire consequences for our health. Trauma after trauma compounds, and racist barriers to health \ncare make it even harder to treat. \nBut it's not just Asian-Americans who are harmed by these racist actions, it's all of us. \nTake, for example, the Asian hate enabled by many during the COVID-19 pandemic. We are all tired of the pandemic: having to quarantine, avoid loved ones, and wear a mask all the time bothers us - it's only human. \nWhen our lives are interrupted and filled with worry and frustration, we're faced with a decision about how to move forward. Unfortunately, in American history, it's been common for some to spiral into negativity and seek someone to blame - as we've seen with Japanese internment camps, McCarthyism and Islamophobia following 9/11. \nLike a nervous athlete who gets inside his or her head and 'chokes', some people make a worried situation worse by becoming negative and hateful contributing to the heavy mental and physical burden already carried by \nAsian Americans. \nBut think also of where we could be as a nation had those Americans channeled their energy into eradicating the pandemic and easing the economic burden it caused. \n- \nNervous athletes don't have to choke with the right mindset they can use this energy to instead have sometimes unbelievable performances - when you hear of an athlete \"in the zone,\" this is what they mean. \nIf we want to be stronger if we all want to be healthier and happier - we have to learn how to channel our energy not into hate, racism, or blame, but into positive solutions. \nFor resources on anti-Asian hate please visit stopAAPIhate.org or anti-asianviolence resources.carrd.co/. \nChristopher Chiou is a family physician in Lansing, he is also Taiwanese-American. \nYour Turn \nChristopher Chiou \nGuest columnist"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196996, Requested 7471. Please try again in 1.34s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196996, Requested 7471. Please try again in 1.34s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": " Two days after the Atlanta shootings, the House held a hearing to address anti-Asian violence.\n One Republican lawmaker pointed the finger at China and invoked a saying glorifying lynchings.\n Democrats zeroed in on Trump's rhetoric and witnesses described the US's history of scapegoating immigrants.\n* See more stories on Insider's business page.\nA House judiciary subcommittee held a hearing Thursday to address discrimination and violence against Asian-Americans. The nonprofit organization Stop AAPI Hate\nhas documented nearly 3,800 incidents of physical assault, shunning, verbal and online harassment, and civil rights violations against the AAPI community in the US\nsince March 2020, when COVID-19 cases began to surge.In addition to taking place amid a spike in anti-Asian violence across the country, Thursday's hearing came days after a series of deadly shootings at three Atlanta-\narea massage parlors that killed eight people, six of whom were Asian women. Robert Aaron Long, a 21-year-old white man, has been arrested and charged withmurder in connection to the shootings.\nHere are 5 key moments from Thursday's hearing\n Rep. Chip Roy employed whataboutism to point the finger at China.\n* \"I think the Chinese Communist Party running the country of China, I think they are the bad guys,\" the Texas Republican said. \"I think that they are harming people\nand I think they are engaging in modern day slavery.\"\n What they are doing to Uighurs ... what they are doing targeting our country ... what they are doing to undermine our national security, and what they are doing to\nsteal our intellectual property, and what they are doing to build up their military and rattle throughout the Pacific, I think it's patently evil and deserving of\ncondemnation,\" he added. \"And I think that what they did to hide the reality of this virus is equally deserving of condemnation.\"\n Roy quoted an old saying glorifying lynchings at a hearing about racist violence.\n* All \"victims of race-based violence and their families deserve justice,\" Roy said. He then tacked on: \"There's old sayings in Texas about find all the rope in Texas and\nget a tall oak tree. You know, we take justice very seriously, and we ought to do that. Round up the bad guys.\"\n-Aaron Rupar (@atrupar) March 18, 2021\n Rep. Grace Meng grew emotional while firing back at Roy.\n* \"Your president and your party and your colleagues can talk about issues with any other country that you want, but you don't have to do it by putting a bullseye on\nthe back of Asian-Americans across this country, on our grandparents, on our kids,\" Meng said.\n The New York congresswoman choked up as she continued, \"This hearing was to address the hurt and pain of our community and to find solutions, and we will not let\nyou take our voice away from us.\"\n Democrats accuse Trump and Republicans of fueling anti-Asian hate by using inflammatory rhetoric about COVID-19.\n* Several Democratic lawmakers skewered the former president and his allies for using terms like \"Wuhan virus,\" \"China virus,\" and \"Kung flu\" to describe the\ncoronavirus pandemic.\n \"As we look at the outrage, let me put into the record: the 45th president always referred to coronavirus as the 'China virus' or 'Kung flu,'\" said Rep. Sheila Jackson\nLee of Texas. \"Let me call his name: President Trump.\"\n \"The rise of hate crimes against Asian-Americans is inherently tied to anti-Asian American rhetoric, some of which has come out of this very chamber,\" said freshman\ncongresswoman Cori Bush. She went on to say that when such rhetoric is used by historically privileged groups, they \"have to own that it causes harm to people,\nespecially people of color\" because there are \"lives at stake.\"\n Witnesses highlighted the US's long history of scapegoating immigrants and minorities in times of crisis.\n* \"As shocking as these incidents are, it is so vital to understand that they are not random acts perpetrated by deranged individuals,\" said Erika Lee, a professor of\nhistory and Asian-American studies at the University of Minnesota. \"They are an expression of our country's long history of systemic racism targeting Asian-Americans\nand Pacific Islanders.\"\n \"We have heard in the past 24 hours many describe anti-Asian discrimination and racial violence as un-American,\" she added. \"Unfortunately, it is very American.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195786, Requested 8172. Please try again in 1.187s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195786, Requested 8172. Please try again in 1.187s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Allison Wang, a Cherry Hill resident since 2007, came to this country because she, like generations of immigrants before her, believed in the American ideal. \n\"What made America great is the open and welcoming culture of diversity,\" said Wang, an accounting director with a healthcare company and a mother of two. \"That's why it attracts so many talented and diverse people.\" \nBut recently, she and other Asian Americans like her have wondered, \"Is this really the country we signed up for?\" \nRecent attacks on Asian Americans, many brought on by hatred and misplaced blame for the COVID-19 pandemic, have created a sense of fear among a diverse community made up of several cultures, nationalities, languages and histories who are all nonetheless often seen as a monolith. \nAnd the brutal murders of eight people  six of them Asian women - in Atlanta have fueled more fear among Asian Americans in South Jersey, but also a sense of resolve that the time to act to end hate is long overdue. \nDuring a Zoom meeting arranged by the Asian American Alliance of South Jersey, several people talked about their experiences with discrimination and racism and their fears for their families and themselves. \nThey and others also expressed hope that, in the same way that the killing of George Floyd sparked a reckoning over treatment of Black Americans, the Atlanta shootings might also bring recognition, support and action to stop the targeting of Asian Americans. \nNothing new, but worse than ever \nEdward Wang is a professor of Asian history at Rowan University who has lived in the United States for more than 30 years. A former member of the Cherry Hill Board of Education and a native of Shanghai, China, he said anti-Asian sentiment is nothing new. \n\"Looking back in this country's history, Asian Americans have been discriminated against since at least 1882, with the Chinese Exclusion Act,\" he noted. Before 1965's Immigration and Nationality Act, quotas kept most Asians from becoming American citizens. \nBut even for those who are citizens, including Asians born in the United States, there is discrimination. \n\"We are constantly being asked, 'where did you come from?\"\" said Wang. \"People assume that if you are Asian you must be a recent immigrant. It's not very comfortable for us.\" \nU.S. Rep. Andy Kim represents New Jersey's Third District and is the son of Korean immigrants. \n\"The short answer is yes, I have experienced racist discrimination, and so have members of my family along with many others in South Jersey,\" Kim told the Courier-Post. \nPeople avoided him, told him they worried he could give them the coronavirus; he had heard from Asian restaurateurs whose businesses were targeted with racist graffiti. \nRhetoric used by former President Donald Trump, especially his use of terms like \"China virus\" and \"kung flu,\" made things worse, creating a sense of blame as COVID-19 forced lockdowns, and the lockdowns caused the economy to crater. \n\"We know that discrimination existed before coronavirus and sadly it will continue after this is over,\" said Kim, a Democrat. \"But this has been gasoline poured over a fire.\" \nKim worries that as tensions escalate with China over issues like trade and human rights, xenophobia, hate and discrimination could escalate as well. \nAnd many Asian Americans worry not only for themselves, but for their families. \n\"All Asian American kids have experienced this kind of discrimination, but the pandemic has raised it to a level that cannot go unnoticed, and must be stopped,\" said Xiufang Chen, a Cherry Hill resident and mother. \n\"This is the first time I don't feel safe,\" said Li Li, the principal of Cherry Hill Huaxia Chinese who is originally from Xi'an, China. Some parents she's spoken with, she said, \"don't want to make a big fuss, or want to let it go, but really we shouldn't. We should let our kids know what (behavior) is right and what is wrong.\" \nThe 'model minority' problem \nOften seen as \"model minority,\" a monolith of hardworking, industrious and entrepreneurial people, Asian Americans are really a diverse race from countries with very distinct cultures, languages, histories and cuisines. \nFor example, there are 56 ethnic groups in China and linguists estimate 297 living languages are spoken among the 1.21 billion people in Mainland China, Taiwan, Hong Kong and Tibet. The Philippines is a nation with more than 7,000 islands and more than 170 ethnic groups, influenced by centuries of migration and conquest from Malaysia, South Asia, Spain, China and the United States. \n\"The model minority is a damaging perception, but it's one that lends itself to people thinking they can ignore (Asian Americans), that they are off doing their own thing, that if they don't complain then everything must be going well,\" said Kim. \nMany Asian Americans don't speak up, he acknowledged, \"because they don't think anything will happen or that anyone will care,\" leading to greater feelings of isolation. \n\"Many in the Asian community struggle,\" said Chen. \"Sometimes when I think of this tragedy (the Atlanta shootings) I get angry, because did it take this happening for people to realize Asian Americans deserve attention?\" \n\"It dodges the real problem we have,\" said Nina Gao, president of the Asian American Alliance of South Jersey and Shanghai native who lives in Cherry Hill. \n\"It is a way to wedge us out of the mainstream: You can take care of yourselves, you all live in these Chinatowns. We give you this glorious title, but it keeps us away, it makes us feel very foreign.\" \n\"We're thought of as self-supporting and people who make a lot of money, and that makes us targets,\" said Wen Gu, a librarian from Cherry Hill. \n\"Many Americans don't want to take the"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198095, Requested 7024. Please try again in 1.535s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198095, Requested 7024. Please try again in 1.535s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Tran Nguyen Wills's family has long worked in nails. Her mother and other relatives who moved to the United States after the 1975 fall of Saigon found employment in a beauty industry that's open to immigrants with limited English but involves constant exposure to noxious chemicals, with modest remuneration.\n\"They worked for us so we could do something else,\" says Wills, 39, on the phone from Denver. \"There was almost a shame of what they did, which I felt. I hope my mom knows it's not a shame.\"\nThe oldest of six, Wills did not heed her parents' wishes. She went big into nails, opening two Base Coat Nail Salons in Colorado, 13 more nationwide in partnership with Nordstrom, her nontoxic polish sold at two regional Whole Foods. Wills promotes safe products and fair labor practices, correcting the treatment experienced by her family and fellow Asian American workers.\nThe pandemic devastated small-business owners like Wills, who was forced to shutter two salons in Southern California and saw revenue plummet 80 percent. \"All of our lives fell apart,\" says Wills, a mother of four.\nFor many Asian Americans, it has also been a long, hard year of hate, hurt by \"China virus\" and \"Kung flu\" references uttered by former president Donald Trump, incessantly repeated in conservative media and accompanied by an escalation of hate incidents, almost 3,800 from March 2020 through February, according to a Stop AAPI Hate national study. Experts believe that most hate crimes against Asian Americans are not reported.\nThen came the mass shootings in Atlanta, which targeted three Asian spas, leaving eight people dead.\nNow, Wills and others in the nail industry are in shock and grieving for the victims, six of whom are women of Asian descent. Salons are a service field dominated by Asian American women who are similarly vulnerable targets, and many workers fear for their safety. They worry about whether to speak out - and what can be done.\nTo Wills, Atlanta was not a matter of if but when. \"Before I read past the headline, I knew what had happened. I knew it was going to be Asian American women who were killed,\" she says.\n\"I saw my own face, my daughters, my mother, my grandmothers, my aunties, my sisters, my staff, my friends,\" she posted on Instagram. \"I am profoundly sad, scared, furious with rage and simply just heart broken.\"\nMasks do not shield her identity. \"My face is Asian. It doesn't matter what I own or what I do,\" she says. \"I am a target. That is the reality. I'm terrified for me. I'm terrified for my workers.\" She starts to quietly weep.\n\"I've had people move away from me in stores to another line,\" she says. One client requested a nail technician who wasn't Asian American. She was told to get her nails polished elsewhere.\n\"I still feel very invisible in an industry dominated by Asian American women. I get left out of conversations,\" Wills says. Until it was called out earlier this year on social media, Nailpro magazine didn't include a single Asian American on its 14-member advisory board. The industry is three-quarters immigrant - and 76 percent of that group is Asian, according to a 2018 study, and predominantly Vietnamese in California. \n\"Most of our culture tells us to put your head down and keep moving forward and stay out of the limelight. They don't want the attention. They just don't want the negativity,\" Wills says.\nSince the shootings, she has placed additional Mace canisters in her salons. Wills restricted patronage to appointments only, with no foot traffic to purchase products, which will further reduce revenue. She's already operating at 50 percent occupancy, following state guidelines.\nWills is also considering active-shooter training for her staff. In eight years of operating nail salons, this was never on her to-do list.\nHer escalating fears are shared by other Asian Americans in her industry. Khanh Tran has long begged his mother, an Oakland resident and Vietnamese refugee, to retire from doing nails. The chemicals, the physical labor.\nShe \"has always been scared of the world, always worried about bad things happening. She always told us to keep our head down and don't pay attention to any of this,\" says Tran, 27, a computer hardware technician.\nDuring the pandemic, the Bay Area has been besieged by anti-Asian American attacks, several of them violent. Days after the shootings, Tran posted a tweet that received more than 105,000 likes: \"Was told that my mom is finally gonna stop doing nails and retire. Not because she's 65. Not because she can't stand the ammonia/fumes. Not because she has arthritis and back aches, but because she's scared to work after what happened in Atlanta.\"\nFew women become wealthy by doing nails. Technicians made a median of $12.39 an hour in 2019, according to the Bureau of Labor Statistics, $25,770 a year. Last year, when the pandemic forced salons to close for months, was surely worse.\n\"It's been a really intense and traumatic year for the nail industry. There have been instances of vandalism. One of our members' salon was broken into last week after the murders. It's hard not to see the connection,\" says Lisa Fu, executive director of the advocacy group California Healthy Nail Salon Collaborative. \n\"A lot of issues related to nail workers is about being ignored and being made invisible. My work has been dedicated to making this community visible,\" Fu says. \n\"The level of anxiety continues to grow and grow. Asian spas and salons are easier targets,\" often identifiable by exterior signage in other languages, Fu adds. \"Now, they're going to be afraid for"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194805, Requested 7940. Please try again in 823ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194805, Requested 7940. Please try again in 823ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "And with Tuesdays attack in Atlanta targeting Asian women, the intersection between misogyny and anti-Asian discrimination remains ever-present in Texas and across the country. The labels that people attribute to Asian women perpetuates a culture of hypersexualization, Mok said.\nLook at how we label Asian women: madame butterfly, mail-order bride, Mok said. The exotic, the erotic Asian woman, so we are reduced to be what? Sex objects.\nA harsh reality and an uncertain path forward\nTakasaki noted how structural racism within a society ultimately cuts off a populations access to basic resources like federal assistance and health insurance, which in turn can lead to a lack of solutions when peoples mental and physical health worsens.\nAsian American businesses often face a much longer road to recovery after a crisis or disaster, according to Russell Jeung, a San Francisco State University professor and one of the founders of Stop AAPI Hate. Asian restaurants and stores took the longest to recover after the 2008 recession, Jeung said, adding that Chinatowns across the country were already experiencing economic problems before the pandemic from the effects of gentrification.\nAsian Americans have really been hit hard with joblessness during COVID-19, and thats partially due to a lot of us work[ing] in the service sector, and those businesses have closed down, Jeung said. Secondly, a lot of us work in ethnic enclaves and ethnic neighborhoods, ethnic markets, and because of racism, people have shunned going to Chinese restaurants, Vietnamese nail salons, and so those businesses have had to close.\nState Rep. Gene Wu, D-Houston, said that while he hasnt directly seen any incidents of discrimination taking place in his district, some residents may be afraid to report experiencing racism. He recently introduced a resolution in the Legislature that expresses support for eradicating racism and calls on law enforcement to fully investigate any reported incidents of anti-Asian hate.\nAsian American businesses have suffered not so much because of direct discrimination, because of race, but because of the poor handling of the response to COVID, Wu said. Is there a component of businesses are not doing well because people dont like Asian people? Im sure thats there, but theres no way for me to quantify that.\nSeveral business owners told The Texas Tribune that hostility toward Asian Texans and the struggles of the service industry during the pandemic have made them question their line of work.\nThere are days where you get up, and you literally dont feel like doing anything because it just seems like theres so much burden, and its just easier not to think about it, said Kristina Zhao, the owner of Sichuan House, a San Antonio restaurant.\nAlice Lee, the executive director of the Southwest Management District in Houston that includes the citys Chinatown, added that the pandemic combined with the rise in discrimination has hit Asian women the hardest, especially in the wake of this weeks attacks in Atlanta.\nHis attack against Asian women is just horrific. Its really women who have lost the most during the pandemic, Lee said. They are the ones who lost the most jobs. They have to stay home while their kids do online learning. They cant really pivot to other employment, so really thats a double or triple whammy for women in general.\nHigh unemployment rates and anti-Asian discrimination have also worsened institutional inequities when it comes to other areas of life like access to health care and work satisfaction, according to Zhang and Takasaki.\nBeyond the wave of violence, theres the outsize impact of the pandemic on Asian workers. Between March 2020 and January 2021, the Asian population in the state filed an average of 31,908 claims per month, representing an annual increase of 1,087%, according to data from the U.S. Department of Labor. That rise in unemployment far outpaced the increases experienced by all other racial groups in the state, although around 45% of claims since March 2020 did not report the applicants race.\nRecent research from the Bureau of Labor Statistics also suggests that the ability to work from home during the pandemic heavily depends on race and educational attainment. The non-Hispanic white population works from home at the highest rate of any racial group in the nation, and over 67% of people with a college degree have the ability to telework. On the other hand, less than a quarter of high school graduates have access to remote work.\nStill, Asian Texans are hoping that better times lie ahead, and they have found comfort in their families and coworkers for the time being, despite the immense challenges that they continue to face.\nK.C. Youn, who owns an environmental engineering company in Houston, immigrated to the United States from South Korea 61 years ago while a senior in high school. He still gets emotional any time he thinks about the partnerships made between American and Korean soldiers during the Korean War, and the connection between his Korean and American identities remains close to his heart.\nMany of my friends who traveled and immigrated to Europe and so on, they all basically want to come to the United States eventually because this is the place to be in this world, Youn said. But unfortunately, even the United States is not perfect.\nNguyen is grateful for the Asian American community having his back throughout the pandemic and especially since his restaurant was tagged with racist graffiti. Many people have asked him how they can help or show him support, and he decided to start collecting gift card donations to provide free meals for San Antonio residents. So far, hes raised over $3,000.\nTheres a saying, a proverb, that says If you want to go fast, go alone. If you want to go far, go together, said Christine Ha, who owns Xin Chao, a Houston restaurant. And I think that"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196601, Requested 7299. Please try again in 1.17s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196601, Requested 7299. Please try again in 1.17s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK  Singaporean film-maker Eunice Lau, who lives in New York, remembers the morning she was walking back to her hotel from work on a film project in Minneapolis some years ago.\n\"I remember it was about 10.30am or 11am,\" she tells The Straits Times. \"So, broad daylight, downtown Minneapolis, walking from the federal courthouse to the hotel.\n\"And I got yelled at: 'Get out of my country, go back to where you came from!'\n\"The fact is that this harassment against Asians didn't just happen yesterday, just because of Covid-19,\" Ms Lau says. \"I think people need to account for that, put that into context.\"\nWhen it is useful or convenient, Asian Americans are the so-called \"model minority\", says Dr Jerrine Tan, a visiting lecturer in English at Mount Holyoke College in Massachusetts.\n\"Otherwise you're the yellow peril or you're the China virus,\" the Singaporean tells ST. \"Or you're the Fu Manchu, or you're the evil spy, or you're the dragon lady or the femme fatale.\n\"There are all these other stereotypes that are in fact used to describe Asian people in very pejorative terms that are conveniently forgotten about when the myth of model minority is used to make sure that you don't rock the boat, in fact.\"\nSeparately, in an article in Wired, Dr Tan writes: \"After 14 years in the US, I have learnt to be vigilantly hyper aware of my skin. Racism most of the time rubs more like a rash than a gash.\"\n\"I live in a conservative part of western Massachusetts. All through 2020, houses in the area brandished frightening and violent Trump paraphernalia,\" she adds.\n\"Before mask wearing became widely practised, I feared marking myself as 'foreign' by wearing one. Then I feared being 'exposed' as Asian when not wearing one. So I covered up completely.\n\"Under my hat, behind my sunglasses and mask, who could know who or what I was?\"\nAs then United States President Donald Trump amped up his \"China virus\" rhetoric, Dr Tan and her husband decided to stop walking in the woods with their dog for fear of running into a \"gun-toting Trump supporter\".\n\"When we shared this decision with others, we always felt embarrassed, like we were overreacting,\" she writes.\nAfter the March 16 Atlanta massacre, Asian social media sites were buzzing with alarm.\nOn a Facebook page called Singaporeans in Americas, resident Josephine Lee of Raleigh, North Carolina, muses about using a self-defence alarm she had obtained long ago in Singapore.\n\"Was heart-breaking... reading a 75-year-old Asian woman and an 83-year-old Asian man were attacked... near San Francisco's UN Plaza followed by six Asian women... killed in the Atlanta Spa shootings!\" she writes.\nOne user responds: \"These attacks are unprovoked and without warning.\"\nHe adds: \"Got to be aware of your surroundings. It's sad.\"\nThe stereotype of an Asian American - fuelled by the misleading \"model minority\" trope - is that he is a member of a community that keeps its head down and does not speak up.\n\"Now we're seeing them coming out in the thousands to stand up for their dignity and I think that's awesome to see,\" Ms Lau tells ST.\nShe adds: \"And that's what I feel - this is the moment, we've got to seize this moment to say, let's fight for our dignity, because if we don't fight, nothing's going to change.\n\"Nobody's going to give it to us on a silver plate. We've got to fight for it ourselves.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193593, Requested 7103. Please try again in 208ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193593, Requested 7103. Please try again in 208ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "An Asian American woman who was attacked in downtown San Francisco last week will donate nearly $1 million to combat racism against Asians, according to her grandson.\nThe 75-year-old woman, Xiao Zhen Xie, is recovering after the brutal attack last week, which left her with two swollen and bleeding black eyes. The money comes from an online fundraiser her grandson, John Chen, organized to pay her medical bills.\nSan Francisco police said 39-year-old Steven Jenkins struck Xie, unprovoked, at 7th and Market streets on March 17.\nKPIX-TV Channel 5 reported that Xie defended herself by punching her attacker. Videos showed the woman on a street corner with a bruised face as the attacker lay on a stretcher.\nThe attack came the day after a gunman killed eight people, including six Asian women, at Atlanta-area spas. Though it is unclear whether the Atlanta massacre was racially motivated, many Asian Americans saw it as the culmination of a string of attacks against Asians during the COVID-19 pandemic.\nWith former President Trump and others using racial language like \"kung flu\" and \"China virus\" to highlight the virus' origins in China, many Asian Americans have been verbally or physically assaulted. The attack on Xie is one of several violent assaults on Asian American senior citizens in San Francisco and Oakland that have prompted volunteers to start security patrols.\nChen wrote on a GoFundMe page that his grandmother, who has lived in San Francisco for 26 years and is a cancer survivor, had a bruised wrist in addition to serious emotional distress.\nHer story and reported defense of herself spread rapidly on social media. Donations poured in -- more than $965,000 in a week.\nIn an update posted Monday, Chen noted that his grandmother had improved -- the swelling around her left eye had decreased, and she was in better spirits. He also said she was adamant about donating all the funds \"to the Asian American community to combat racism.\"\n\"She said we must not [submit] to racism and we must fight to the death if necessary,\" Chen wrote. \"She insists on making this decision saying this issue is bigger than Her.\"\nJenkins had assaulted an 83-year-old Asian man earlier March 17 at the U.N. Plaza, said San Francisco Police Department spokesman Robert Rueca.\nA security guard saw the attack and began to chase Jenkins, Rueca said.\nWhile running away, Jenkins struck Xie at 7th and Market streets.\nJenkins was arrested and hospitalized for injuries unrelated to the attacks, Rueca said.\nThe advocacy group Stop AAPI Hate has tracked more than 3,700 self-reported verbal and physical attacks against Asian Americans across the U.S. since the pandemic began.\nSan Francisco police conducted 17 hate crime investigations of all types in 2020, up from six a year before, according to SFPD data.\n\"Just the thought of leaving your house and being killed or seriously injured for being Asian is an extremely disturbing thought,\" Chen wrote. \"All the hatred that is happening towards the AAPI community has deeply angered and saddened our family. The AAPI community is bleeding from this violence and hatred. We as a community cannot stay silent nor be silenced anymore.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198020, Requested 7363. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198020, Requested 7363. Please try again in 1.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "at all. Four days after the initial diagnosis, the patient became Vo's first to die of COVID-19.\nVo didn't know what to say to the family,but they were nice enough to call me and inform me, 'DoctorVo,you triedyour best.Don't worry. Don't be sad.' ... They even encouraged me to continue to help other people.\nAsian Frontline Medics Face Hate Amid COVID-19\nThey've had to work amid a wave of pandemic-inspired anti-Asian attacks\nI've been a nurse for more than 30 years, I've been through the death and the birth, but this is an unbelievable for me... I did not follow the numbers because it's so discouraging, said AnchaleeDulayathitikul, 55, an intermediate care nurse. She arrived in the U.S. in 2014, after deciding she wanted an American education for her children.\nShe had opted for a nursing career because her grandfather thought she had a caring nature. Dulayathitikul earned her degree in nursing and midwifery in 1988 from Chiang Mai University in Thailand. More than a quarter century later, she passed all the tests in the battery needed for nursing certification in Maryland on her first sitting.\nShe works at the University of Maryland Upper Chesapeake Medical Center and after a year, she told VOA Thai, I see the flow and I seehow wecantake care of (COVID-19 patients) successfully.\nDulayathitikul plans visit her unwell mother in Thailand once the pandemic, and its travel restrictions retreat. She will return to Maryland for her children and her career because I love my career a lot.\nDaruneeRasameloungon, 41, a Bangkok native who arrived in the U.S. in 1991 with her family to join her father who had emigrated ahead of them. She wanted to be an engineer or an FBI agent until she helped care for a cousin after a bus hit him, breaking his arms and legs.\nWhen she decided to pursue nursing, her father told her he couldn't pay for college on what he earned delivering pizza. Working with her high school counselor, Rasameloungon parlayed high grades, volunteer work and school activities into scholarships to pay for all four years at George Mason University's nursing school in Fairfax, Virginia. She graduated in 2001 and is now a progressive care unit nurse technician at Fairfax Hospital, where she has worked since 2008.\nIt's always hard, this is very sad when you have to wrap someone in the bag, you know, ... theydied by themselves, she told VOA Thai. They have nobody withthem,and their family cannot be with them, (it) is really sad ...todie by yourself. Itis really overwhelming. My God, you feel sorry for them. And it's just, like a reality hit: This . . . can happen to anybody.\nAlthough Rasameloungon intends to continue nursing in the U.S. in part to remain close to her son, she wants to take a long break in Thailand when the pandemic subsides because the COVID (made) me realize that I want to spend more time with my family.\nBorn in Menifee, California, LimyiHeng, 38, is the child of Cambodian refugees. He spent three years in the Air Force, most of the time in Southern California. But I did a lot of tours all around the United States, which gave me a deep appreciation for the diversity of America.\nHis Air Force mentor guided himinto a career as a nurse practitioner, and he earned his master's degree in nursing from Columbia University, where he developed a network that informed him early about the spread of COVID-19 in New York City and elsewhere. He works at Redlands Community Hospital and San Gorgonio Memorial Hospital.\nThe surge of COVID-19 cases began in October and continued through December. There wasn't really a worst moment. I think it was this really, the long hours, the long hours, Heng told VOA Cambodian. For him, the upside at work is being part of a team that includes hospital housekeepers, the logistics people who deliver protective gear and community leaders who provide the right messaging, the right information to drown out the misinformation.\nFor Conners, similar teamwork 2,600 is what leads to his proudest moments of the pandemic, when patients leave the ICU.This kind of a big accomplishment (is) from the teamwork that we have done.\nCambodian Service Chetra Chap; Indonesian Service Naras Prameswari and Dian Widyastuti; Mandarin Service Calla Yu; Thai Service Pinitkarn Tulachom;Tibetan Service; Trinlae Choedron and Ngawang Tenzin; Vietnamese Service An Hai; and Graphics, Mark Sandeen, contributed to this report. -VOA"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199567, Requested 7179. Please try again in 2.023s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199567, Requested 7179. Please try again in 2.023s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "New York Police Department patrols aimed at stopping anti-Asian violence have been cut back even as anxiety lingers.\nThe surveillance video captures a brutal scene: A woman is thrown down a flight of stairs and smacks into the subway platform violently enough to fracture a bone in her face. It was May 28 and the woman in her 60s was among dozens of people attacked during a spate of anti-Asian violence this year.\nIt may not even have been the first such attack by the suspect John Chappell a law enforcement official said. Two months earlier Mr. Chappell who had dozens of prior arrests had been suspected of lighting an Asian woman's backpack on fire the official said. He was released just days after his arrest in May.\nSix months into a series of brutal attacks on people of Asian descent across the city Mr. Chappell's case underscores the challenges the police and prosecutors have faced in both preventing the violence and punishing those responsible.\nMany of the attacks are unpredictable and carried out by people in the throes of mental health episodes seemingly at random. Officials say they doubt many of the hate crime charges related to the attacks will stick in court and those arrested are often released quickly. And the Police Department appears to have scaled back its efforts to stop them: An undercover unit intended to prevent anti-Asian attacks has not been active since May after officers faced threats of violence themselves.\nBut the attacks have continued and anxiety and trauma still grip many pockets of the city's Asian communities where the violence feels fresh even as the spotlight on it has dimmed.\n\"There's still this fear that permeates throughout the community,\" said Chung Seto, a community leader and political strategist in Chinatown. For many she said the fear feels like a continuation of the darkest days of 2020 when city residents were afraid of going outside because of the coronavirus.\nNow shop owners in Ms. Seto's neighborhood remain concerned about staying open late and elders -- including Ms. Seto's parents -- will not venture outside.\n\"It's not so much catching Covid,\" Ms. Seto said. \"There's no vaccine for racism.\"\nAttacks on Asian Americans have shaken cities around the country: In Los Angeles hate crimes against Asian Americans more than doubled in the last year, and in Boston Asian American elders are learning how to defend themselves with canes. For New York the problem endures as the city forges ahead with its reopening and visitors once again wander the streets of Chinatown and many living in the neighborhood say they feel left behind.\nBut for New York's police stopping the attacks before they happen is particularly difficult even when the person accused has dozens of prior arrests. And even when arrests are made the defendants are often released pending trial corrections records show. Mr. Chappell for example was released just a few days after his arrest despite prosecutors seeking high bail.\n\"It's nice to know there's a task force. It's nice to go on the bus and there's this messaging of anti-Asian hate crimes,\" said Kevin Nadal, a professor at John Jay College of Criminal Justice. \"But what does that actually do?\"\nThe challenges continue even as anti-Asian violence keeps rising: As of June 27, reported hate crimes against Asian New Yorkers had increased by 400 percent compared with the same time frame in 2020, from 21 to 105 according to Police Department statistics. The psychological effects of that violence has scarred entire communities.\nIn South Brooklyn, where a community senior center just reopened after closing for the pandemic, Don Lee a community organizer, said Asian elders have been hesitant to travel to and from its programs.\n\"There are people who are excited to come back but we know many of the seniors don't feel safe to come out,\" Mr. Lee said. \"The fear's still very real.\"\nMr. Lee said he knew firsthand that some victims of harassment and hate crimes were no longer reporting the episodes to the police because they believed nothing meaningful would be done.\n\"What is the point right? What is the point?\" Mr. Lee said.\nLaw enforcement officials and experts note that it can be difficult to prosecute cases as hate crimes, which require proving the defendant's intent was based on the victim's race or ethnicity. In previous years many suspects might have been arrested on assault or harassment charges without a hate crime designation.\n\"The public is seeing this rash of attacks on Asian Americans and it is possible that there is a trend happening because of racial animus,\" said Alissa Heydari, a former assistant district attorney in New York City who now helps direct the Institute for Innovation in Prosecution at John Jay College of Criminal Justice. \"But to prove it in court when the criminal standard is beyond a reasonable doubt it is really hard to show that a victim was picked in large part because of their ethnicity or gender.\"\nThe attacks, many of which have been recorded on video and shared widely, shocked the conscience of the city. Groups of volunteers now patrol the streets of Chinatown hoping to deter potential attacks. Many Asian New Yorkers say they no longer leave home without pepper spray or established buddy systems.\nIn March the Police Department cobbled together a volunteer group of Asian American officers who work during their time off hoping to stop attacks if they see them happening -- including a pilot program where undercover officers wandered streets where anti-Asian violence had taken place and was thought to likely reoccur.\nThe plainclothes officers were meant to both lure potential offenders into confrontation and intervene if they saw anti-Asian harassment occurring. But the undercover strategy left officers in tenuous positions and some were nearly attacked according to a law enforcement official familiar with the matter.\nIn one instance an undercover officer who is of Asian descent was approached by a man on a train platform in Queens. The man waved his hand and hat in the officer's face and said \"That's why you peoples are getting beat up\" according to a police report. He was charged with aggravated harassment as a hate crime in"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195880, Requested 7938. Please try again in 1.145s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195880, Requested 7938. Please try again in 1.145s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK  Jewish Asian-Americans have been left shaken by the latest wave of hate crimes against both of their heritages. The United States is in the midst of a shattering spike in hate crimes against Asian-Americans, with nearly 3,800 incidents of hate reported to the organization Stop AAPI (Asian Americans and Pacific Islanders) Hate in the past year. The reported violence and discrimination against Asian Americans increased by 1,900% since the start of the COVID-19 pandemic, which sparked a slew of offensive anti-Asian slogans, such as 'China Virus' and 'Kung Flu' because of its origins in the Wuhan province. The tragic shooting of eight people, including six Asian women, by a white man in Atlanta on March 16 brought new national attention to the gravity of this moment. As Jewish groups rally in support of Asian Americans following the string of murders, the crimes coincide with a surge of antisemitism during the pandemic, with Jews in some cases blamed by conspiracy theorists for the spread of disease. The waves of harassment against both minority groups have left Jewish Asians feeling especially vulnerable. Zoe Kress, a 27-year-old occupational therapy student who was born in China and adopted as an infant by a Jewish family in New Jersey, expressed 'grief, anger, sadness and hopelessness' that Asian American Jews like herself are facing unprecedented discrimination. Growing up attending Jewish day schools and summer camps, Kress said shes always been vigilant about antisemitism, but before this year she didnt worry much about Asian racism. 'Weve sadly become accustomed to hearing about attacks on Jews. Its scary that now I also have to be concerned about another part of my identity, too,' Kress told The Jerusalem Post. 'Its something I never thought Id have to worry about. But now it is and I think it actually worries me even more because I dont look like a typical Jew, its harder to recognize. But I can always be identified as Asian,' she continued. \nYOSHI SILVERSTEIN, founder of Mitsui Collective, a Jewish nature and wellness organization, said the Atlanta massacre brought back painful memories of The Tree of Life synagogue shooting in Pittsburgh. 'The aftermath felt exactly the same for me. Both brought feelings of how personal it feels to have others attack a part of your identity. It felt vulnerable and visceral,' he recalled. 'Both brought isolation and loneliness.' Silversteins father is an Eastern European Jew and his mother is Chinese-American. He calls himself a 'Chinese-Ashkenazi American Jew.' Silverstein said that recent attacks on both of his identities have made him especially conscious to pass cultural traditions down to his three-year-old daughter. He does so by fusing the two heritages together, like cooking scallion latkes for Hanukkah. Last month, when the Lunar New Year fell on a Friday night, the family hosted a Chinese-themed Shabbat dinner. 'Despite the similarities between the two shootings,' Silverstein continued, 'it feels like there is a bigger gap following the Atlanta attacks, because it wasnt something that impacted all Jews. It impacted Asian folks, and of course the overlap of Asian Jews. Early on, it was like: Are any non-Asian Jews paying attention? It brought me a different feeling of isolation.' But Silverstein, 37, also found comfort in the support he has received from the Jewish community since the Atlanta massacre. 'The day after, I saw other Asians, most of whom were not Jewish, expressing how they were feeling. That hit home for me. Then I was tagged in a solidarity post from a Black-Jewish liberation collective, which brought a lot of emotions for me. I needed that,' he recalled. 'A lot of folks from different Jewish corners have reached out to me, but I think I may be unique in that because Ive worked in the Jewish world for a long time and have a lot of personal contacts.' Immediately following the Atlanta massage parlor murders, dozens of Jewish groups released statements condemning the rising levels of anti-Asian violence. Police in Georgia say the gunman may have been motivated by hatred of women and not anti-Asian bigotry, but regardless, the attack is likely to exacerbate the communitys anxiety.\n'AS HATE crimes targeting Asian Americans continue to rise, this Jew wont be silent,' said David Harris, CEO of the American Jewish Committee, on Twitter. 'As Asian Americans are attacked and, yes, killed, this Jew wont be silent. As the larger fabric of our diverse society is threatened, this Jew wont be silent.'The progressive Jewish group Bend the Arc issued a similar statement of solidarity. 'This is a national crisis fomented by politicians who have chosen to point fingers and spread fear instead of taking action to keep everyone safe from the pandemic,' the group said. 'Their words and rhetoric are once again echoing in gunshots, as they have in Pittsburgh, El Paso, and most recently in the halls of the Capitol building, and they must be held accountable. As Jews, we know that freedom and safety for any of us depends on the freedom and safety of all of us.''Institutions have put out brilliant statements that weave Torah with such acute understanding of the politics of the times,' said Rabbi Mira Rivera, a Filipino-American associate rabbi at Romemu, a popular Jewish Renewal congregation on Manhattans Upper West Side.The statements are not enough, Rivera told the Post. 'Did any of those people actually reach out and call somebody? How many congregations actually know their non-white Jews?'This is my face-to-face plea,' she continued. 'I hope and pray that Jewish organizations will actually try and spend more time and create relationships with Asian Jews. It has to take people dying, oftentimes, for Jewish professionals to reach out to non-Jews. It has to take an intersectionality of Jewishness for us"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199782, Requested 8022. Please try again in 2.341s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199782, Requested 8022. Please try again in 2.341s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "We created Stop AAPI Hate so there was a way to capture the increasing attacks on our community. One year later,the vaccine provides a light at the end of the tunnel for the pandemic. But it seems there's no end in sight for anti-\nAsian hate.News about the deadly shooting in Atlanta broke almost one year to the day since we created the Stop AAPI Hate\nreporting center.\nTwelve months ago, our country was in the early stages of the COVID-19 pandemic. Most Americans were entering\ntheir first quarantines and watching a virus spread faster than government systems could track.\nBut Asian Americans were watching something spread even faster than COVID-19: Widespread hate and\ndiscrimination against members of our community.\nIn early February, even before a single COVID-19 case was recorded in Los Angeles, a 12-year-old Asian American\nchild in the city was physically assaulted in his school by a classmate who said he was a coronavirus carrier and\nthat he should go back to China.\nAt the time, incidents like these weren't broadly reported. But they were widespread they were happening to our\nneighbors, colleagues, our family members and ourselves.\nWe created Stop AAPI Hate so there was a way to capture the increasing attacks on our community.\nOne year later, the vaccine provides a light at the end of the tunnel for the pandemic. But it seems there's no end in\nsight for anti-Asian hate.\nIn the two weeks since our country watched in horror as eight people six of them Asian American women were\nkilled in Atlanta, we've been repeatedly asked why this is occurring. Many Asian Americans do not view these\nshootings in a vacuum.\nIn the days leading up to the shooting, there were what seemed to be daily reports of violent, public attacks against\nAsian Americans especially elderly Asian Americans in places that included Oakland, San Francisco and New\nYork City.\nThe Atlanta shootings took place, in fact, hours after we published a report documenting nearly 3,800 self-reported\nhate incidents from the AAPI community.\nAnti-Asian hate and the scapegoating of immigrants is nothing new. But in looking through the thousands of\nreports that we received, several things came to the fore.\nTrump's rhetoric normalized hate\nFor one, former President Donald Trump's and other government officials' widespread use of slurs like \"Kung Flu,\"\nhis anti-China rhetoric and his exploitation of existing xenophobia played a major factor in normalizing and\nspreading hate against Asian Americans, both during the Trump administration and even now that his term is over.\nAsian Americans reported hearing these slurs like \"China virus\" thrown at them while they were refused business,\nharassed, spit on or attacked.\nThere was also a devastating intersection between racism and misogyny. In our data, women reported incidents\n2.3 times more often than men did. The reports and stories from these women often described being sexually\nharassed and COVID-19 being weaponized as part of the sexual harassment.\n\"You Asian c---,\" one man yelled at Vietnamese American Hong Lee after she declined his offer of lunch last\nAugust. \"What is the matter with these hoes ... Help her go back to f----- Asia.\"\nThis month, we've also been asked about what can be done. In the shadow of the Atlanta-area shooting, there is,\nunderstandably, a strong urge to focus on hate crime enforcement.\nFor example, the COVID-19 Hate Crimes Act, authored by U.S. Sen. Mazie Hirono and U.S. Rep. Grace Meng, would\nrequire a Department of Justice employee to facilitate fast reviews of federal, state and local COVID-19 hate crimes\nfor one year as well as mandate guidance to local enforcement to establish an online hate crime reporting form in\nmultiple languages.\nCertainly, steps such as these can help address the lack of hate crime reporting. But the fact remains that 90% of\nthe incidents reported to us are not hate crimes because no underlying crime was committed. Many are civil rights\nviolations involving discrimination in the workplace, restaurants and retail venues and can be prosecuted in civil\ncourt.\nMultiple solutions are needed\nRacism against Asian Americans is longstanding and complex, and requires a comprehensive and multi-faceted\napproach.\nWe need resources dedicated to local communities, including community safety programs and in-language\nsupport for those in need of mental health, legal and immigration services.\nWe need to build a strong civil rights infrastructure at the local level by fully funding community-based groups,\nwhich are often the first responders to incidents of hate.\nWe need to increase exposure to the voices and histories of all communities by expanding ethnic studies and\neducation.\nAnd finally, we need stronger federal civil rights laws that address discrimination in public accommodations\nbusinesses where we buy our groceries, refill our prescriptions and eat our meals.\nThese solutions are neither complex nor out-of-reach.\nA number of groups across the country are already moving this work forward. In Los Angeles, LA vs. Hate provides\ndirect assistance to victims of hate through a 211 reporting system for all marginalized community members.\nThe San Francisco-based Coalition for Community Safety and Justice focuses on victim services, intervention and\nprevention programs, community mediation and restorative justice practices.\nWe founded Stop AAPI Hate to not only raise awareness about anti-Asian hate but also to fight against it.\nThe outpouring of support and solidarity from all racial groups that has risen up in the days since this tragedy\ngives us every reason to be optimistic. With the energy and calls for change we are seeing now, we are\nfundamentally hopeful that there is a true path forward in ending anti-Asian hate.\nTo report an anti-Asian hate incident or to learn more about what you can do to help, visit: stopaapihate.org.\nManjusha K"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195699, Requested 7242. Please try again in 882ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195699, Requested 7242. Please try again in 882ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "It's hard not to see the connection,\" says Lisa Fu, executive director of the advocacy group California Healthy Nail Salon Collaborative. \n\"A lot of issues related to nail workers is about being ignored and being made invisible. My work has been dedicated to making this community visible,\" Fu says. \n\"The level of anxiety continues to grow and grow. Asian spas and salons are easier targets,\" often identifiable by exterior signage in other languages, Fu adds. \"Now, they're going to be afraid for the their lives.\"\nHong Dinh, 38, moved to the United States from Saigon in 2003. She's helped support herself and three children doing nails for 15 years. The dream was to open her own salon with her sister in San Jose - which she realized this month, her grand opening on March 15.\nThe next day, the killing rampage occurred in Atlanta.\nThe following day, Dinh ordered a steel front door for protection and closed the salon.\nIt will remain locked until the door can be installed and she feels safe that her salon won't be a target.\n\"I'm sad. I'm sad for the whole world,\" says Dinh, through an interpreter. \"I think this is the beginning of the attacks. People are mad because of covid. There's anger in them. They've been stuck at home for a year now. It also depends where they get their news from.\"\nRachel Yoon, 59, who immigrated from Korea, owns a salon in Manhattan's Washington Heights. Nail salons have long been places where women of varied backgrounds gather. The majority of Yoon's clientele are Dominican. They've been great. But she's heard unseemly comments, often from White people who are strangers.\nOutside the salon, she attempts to blend in. \"I feel, thankfully, when we are wearing masks, sometimes people can't distinguish. Sometimes they can. It's definitely something you feel in the air,\" she says on the phone, driving from her home in Queens to her salon. \"I worry about my niece, my sisters, in the subways. I'm always afraid.\"\nShe sticks to her routine and feels relatively safe. \"I tell my kids if someone approaches you and says something bad, just ignore them. You cannot confront them,\" says Yoon, who has two grown daughters. \"I'm not going to any places I'm not familiar with. I'm being careful until this thing goes away. Everyone is warning each other, 'don't look at someone in the eye.' \"\nWills, in Denver, feels something else must be done. Let Atlanta be instructive. Look at the increase in violent incidents. On March 18, two days after the shootings, Xiao Zhen Xie, 75, was brutally beaten in San Francisco but fought back her attacker. A GoFundMe campaign launched by her grandson has raised nearly $1 million dollars; the family plans to donate almost all of it, less Xie's medical expenses, to combat racism in the community. \n\"I feel like I have to be more outspoken. To tell people, 'Don't forget about me,' \" Wills says. \nShe's talking. Wills is sharing her anger and frustrations on social media and in her community. She's considering running for public office, possibly the state legislature, where she isn't represented by people who look like her or share her experiences. \nKeeping her head down isn't in Wills's nature and, as she will tell you, staying quiet isn't working."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199460, Requested 6384. Please try again in 1.753s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199460, Requested 6384. Please try again in 1.753s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "day, a man flipped her off as he drove past her. Her partner, who is also Filipino, has been told to go back to his country several times.\nDespite years of racism in her own life, this is a \"lightbulb\" moment, she said.\n\"I have always experienced some sort of anti-Asian hate in my life, but growing up I didn't see it as hate. I saw it as people making fun of me and my parents,\" she said. \"I accepted that 'Asians look funny.' There's a lot of unlearning to be done.\"\nEstrada said that until now she hadn't thought much about her safety as a young Asian American business owner. She's worried more for her mother, who works as a bank teller and has had racist encounters her whole life in public spaces.\nEstrada said she probably won't be talking to her parents about her fears or the violence, though.\nIt's a \"tricky\" conversation because of differing political views between generations, she said.\n--\nConfronting feeling of inevitability\nMichelle Nguyen Bradley said she is only now learning to have conversations about race with her friends and family, despite grappling with these issues her whole life.\n\"The model minority myth is so bad for us because it means they think all of us are either 'Crazy Rich Asians' or we're doctors,\" said Nguyen Bradley, a 38-year-old Palms resident and online show host.\nShe choked up at the thought: \"Asian Americans are taught not to take up space or talk about ourselves too much. When some of us do speak up, it feels like no one is listening.\"\nIn the past, it has been easy to bury her head in the sand, she said, because horrifying racist attacks weren't happening directly to her or on her block.\nAs she gets older, that's harder to do, she said.\n\"I was scared of it happening, and it finally did happen, but just not to me. And it's not any better,\" she said.\nAnd she knows it could happen to her.\n\"You're carrying this feeling of inevitability,\" she said.\nNguyen Bradley said she's worried about her immigrant Vietnamese parents and finds it hard to broach the subject of racism in light of the Atlanta attacks.\nThey have mostly been staying home during the pandemic -- her father is recovering from COVID-19. But she worries about the moments they have to leave their house in a mostly white Pittsburgh neighborhood.\n\"Of course, I'm worried if they go to Walmart or that kind of thing,\" she said. \"Asian kids, we don't want our parents to worry. We see how far we can get without talking to them about it.\"\nAs for herself, Nguyen Bradley started taking precautions early last year, considering the racist rhetoric at the start of the pandemic, but has since let go a little.\nShe would avoid walking her dog or getting the mail at night, asking her husband to do the tasks instead. She would clench her phone in her pocket while she was out and about, ready to call someone or take photos if she needed to.\nBut now, she's exhausted.\n\"I can barely get out of bed,\" she said. \"You can't live your life constantly on guard.\"\nEnvisioning a future where she feels safe \"is very cloudy.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198873, Requested 7069. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198873, Requested 7069. Please try again in 1.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "am scared of what would happen at school. Although Ive never experienced bullying before, Im deathly afraid of the harassment I might receive if I go back. Before Corona, there were multiple instances of racism at my school, and I fully believe that it would increase. Such as people only grouping me with Asians because were the same, people telling me I have small eyes, people mocking my food, asking if I eat dogs, etc. Im not very strong or athletic, so If I was attacked I would have no way of defending myself which is incredibly scary considering I walked home from school alone each day last year. I believe that we need to teach the next generation that any type of racism is offensive and should not be normalized. Ive had many experiences that most minorities can relate to, not just Asians. Most notably bringing a school lunch, in which one brings a school lunch of food usually from their culture. Thats when all the other kids gather around to stare and criticize the way it smells or looks. Almost every minority Ive met has experienced this, and it should be stopped. Just because it doesnt look American doesnt mean it isnt normal in any way. Another thing that needs to change is stereotypes. Not all Asians are smart, good at math, play the violin, grow up to be a doctor, or are bad drivers. Weve achieved the status of good minorities who dont cause trouble and are smart reliable citizens. In Washington, they even changed our status from a minority to White. I am not Caucasian so do not compare me to one as if it were a compliment. Being Asian is part of my identity and by you taking that away from me is racism in its greatest form.\nOlivia Quan, age 13\nPublic Charter School Student, Framingham, M.A.\nI know about all of the Asian hate crimes that have been going around, and I know for a fact that its serious and hurtful to many. I unfortunately have experienced Asian hate. This was before the whole lockdown thing. My teachers were wiping down all of our desks, and a boy shouted in the class something along the lines of You should clean Olivia because shes Asian so she has corona. Sometimes, I do feel afraid if I get bullied at school as an Asian. I know it could help if people are educated about what Asian hate is about and how it affects people. Or people need to learn how not to be a jerk.\nJack C., age 13\nPublic School Student, Glencoe, Illinois\nI have heard of the attacks going on for Asian hate. This is a problem that has to be stopped. Similar to attacks on African American people, this is a big problem and doesnt reflect well for the U.S, especially when we are the Land of the Free. Its not just bad for Asian Americans, but also for the entire United States because it impacts what other people think of our country. The attacks have not affected my life a lot. I live in a small town, so I know a lot of people in my town. I dont get afraid of being bullied, because my school has generally been accepting, and I know I have friends that would stand with me. I also dont go outside of my town a lot because of Covid. Educating can help stop Asian hate, as well as hate for other minorities and groups. Though it is nearly impossible for everybody to be anti-racist, we can help some people not be racist by educating them about racism. Just like people say we should learn about WWII or any other historical event to not repeat our mistakes, if we learn about racism, we can help not repeat mistakes in our society.\nOlivia Wong, age 17\nPublic School Student, New Hyde Park, N.Y.\nI am aware of the growing violence against Asian Americans, and I feel so disheartened that this is whats happening in our country. It feels as if there are so many divisions separating people in a nation that is supposed to be united. During a pandemic where so many have been suffering all over the world for a year now, Asians have been wrongly scapegoated, and as a result, innocent people are being attacked, especially the elderly. Fortunately, I have not encountered firsthand any verbal or physical assaults due to my race at school or around the neighborhood. However, there are people I know who do not know the extent of their words and make jokes at the expense of others, repeating phrases like the Chinese virus or Kung Flu. Not only does this rhetoric harm victims mental health, but it also leads to more serious issues when it turns out that those words werent just joking. It is understandable that people have feelings of frustration and fear, but it is unacceptable when they choose to take them out on Asian Americans. My entire family is also passionate about this lingering issue. My parents have gotten stares and have been yelled at while shopping or taking a walk, and one of my brothers was on the receiving end of a terrible verbal assault while working in a grocery store over the summer. I have not been restricted in going out, but I am on high alert to watch for my surroundings. I feel safe in my school because the faculty promotes inclusion, fosters conversations, and is not afraid of educating us on current social and political issues. I am not afraid for myself, but more for my own grandparents since the majority of attacks have targeted the elderly. If we want change, we must continue to speak out because these attacks are not stopping anytime soon. We have a responsibility to not stay silent in order to educate our fellow Americans on the strength of their words and the power of their actions. If we do our part, more and more people will become aware of our situation and begin to actively learn about the ongoing virus of racism.\nHannah, age 15\nPublic School Student"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197723, Requested 6980. Please try again in 1.41s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197723, Requested 6980. Please try again in 1.41s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "ATLANTA  Most were dedicated mothers. Some came to the USA looking for a better life. One was an Army veteran working as a handyman. Some loved to dance and sing karaoke.\nAuthorities have identified all eight people killed last week in a string of attacks at three Atlanta-area spas. Six of the women were of Asian descent including four who were Korean.\nSoon Chung Park, Hyun Jung Grant, Suncha Kim, and Yong Ae Yue were identified Friday by the Fulton County Medical Examiner's office as the four victims of the shootings at two spas in Atlanta. The medical examiner said Park, Grant, and Yue died of gunshot wounds to the head and Kim died of gunshot wounds to the chest.\nWednesday, the Cherokee County Sheriff's Office released the names of the four victims at the spa about 30 miles north of Atlanta. Those killed at Young's Asian Massage were Delaina Ashley Yaun, Paul Andre Michels, Xiaojie Tan, and Daoyou Feng. A 30-year-old Hispanic man was injured.\nMost of the women had sons and daughters who remembered them as close friends. They dedicated their lives to making sure their children were taken care of.\nOther victims included a woman who had been married less than a year and a brother who left behind more than half a dozen siblings.\nThe shooting came amid a spike in incidents of hate, discrimination, and violence against Asian Americans during the COVID-19 pandemic, said Stop AAPI Hate, a group that tracks such incidents.\nHere's what we know about the victims: Hyun Jung Grant\nHyun Jung Grant, 51, loved karaoke, dancing, and clubs, and she made the world's best kimchi stew, her son Randy Park told USA TODAY.\n\"She dedicated her whole life to raising us but even then she found time to enjoy herself with her friends,\" Park, 22, said. \"I can't articulate or express in any way to describe what she was or what she meant to us.\"\nPark said he learned only recently that his mom worked in a massage parlor  she initially said it was a makeup parlor in an effort to protect her two boys.\nThe two Atlanta spas attacked by the shooter had been repeatedly targeted in prostitution investigations in the past 10 years, according to police records. The documents show that 10 people had been arrested on prostitution charges but none since 2013.\nThe suspect told police his sex addiction drove him to commit the crimes and Atlanta authorities confirmed the man had \"frequented\" the spas.\nPark said he understands why his mother wasn't forthcoming and said he feels selfish and guilty for invading her privacy by asking about it. He said they never talked about her work which sometimes kept her away from home for weeks at a time.\n\"What's so hard about letting people live how they want? If it does no harm to you who is it harming then?\" he asked.Xiaojie Tan\nXiaojie Tan, 49, who owned Young's Asian Spa and one other in Acworth, Georgia, also went by her American name Emily. She was remembered as a curious, hardworking, and caring woman who was always filled with joy. \n\"She did everything for me and for the family. She provided everything. She worked every day, 12 hours a day, so that me and our family would have a better life,\" Tan's daughter Jami Webb told USA TODAY. \n\"She was full of smiles and laughter. She was just a pleasure to be around,\" said Michael Webb, Jami's father, who met Tan while traveling for work in China in the early 2000s.\nMichael Webb and Tan met in her native city of Nanning, which sits on the border with Vietnam. Neither spoke the other's language well but that didn't stop the pair from falling in love.Delaina Ashley Yaun\nDelaina Ashley Yaun, 33, leaves behind a 13-year-old son and 8-month-old daughter. She and her husband had been married less than a year.\nHer mother Margaret Rushing told WAGA-TV that her daughter and son-in-law went to the spa on a date. When gunfire broke out, Yaun's husband locked himself in a room and wasn't injured, said Yaun's half-sister Dana Toole. \n\"He's taking it hard,\" Toole said. \"He was there. He heard the gunshots and everything. You can't escape that when you're in a room and gunshots are flying  what do you do?\"\n\"We could really use the help to cover her funeral expenses,\" loved ones said on a GoFundMe page. \"She has two beautiful babies she is leaving behind. We just don't know how to do any of this alone. If you can find it in your heart to donate, our family will certainly appreciate all of your support.\"\nYaun's husband Mario Gonzalez told Spanish-language website MundoHispanico that they were getting massages in two separate rooms when the gunfire erupted. \nHe said he hid in the room, and when police came, he asked repeatedly if his wife was OK. He was detained and questioned with other witnesses and did not learn until later that his wife was one of the eight victims. \n\"What am I going to do?\" Gonzalez said. \"I have nothing.\" Paul Andre Michels\nPaul Michels, 54, owned an alarm company in Atlanta where he and his wife Bonnie lived for 26 years, his brother John said. Paul Michels had expressed interest in owning a spa and according to The Washington Post worked as a handyman at Young's Asian Spa.\nJohn Michels said his brother was \"just in the wrong place at the wrong time.\" They grew up with nine siblings in Detroit, riding dirt bikes and spending summer weekends at a lake and getting into mischief together, he said. They both served in the U.S. Army at the same time and his brother"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194069, Requested 7174. Please try again in 372ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194069, Requested 7174. Please try again in 372ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "She always knew her business could be a potential target of anti-Asian hate, but didn't think much of it until earlier this month, when a customer uttered a racial slur to one of her employees.\nHis wife came in and apologized, but it didn't make us feel that much better,\" she said. We are an Asian brand, and our logo itself has Chinese characters. I'm worried about that.\"\nHer fears have been dramatically heightened since last week's shootings at three spas in Georgia, which left eight people dead, including six women of Asian descent. Like other Asian business owners, Vuong was devastated but, sadly, not surprised.\nMany Asian-American business owners have been reeling from the downturn caused by the pandemic, and some are struggling with discrimination and cultural barriers that make it difficult to seek support. Some owners are calling for a commitment from civic leaders and the general public to acknowledge and denounce the rise in anti-Asian incidents, rather than trying to increase security measures around their businesses.\nIn response to the Georgia murders, Vuong said she and her husband held a staff meeting with their 14 employees to figure out how best to protect themselves. The group emerged from the conversation with new protocols, including a buddy system for when staff go outside to take out the trash or walk to their cars at the end of a shift.\nWe're going to start checking up on each other to see if everyone got home safely or not,\" she said. Vuong added that running a new business is stressful enough, and the threat of harassment and violence makes it overwhelming.\"\nAnti-Asian violence has sharply increased since the onset of COVID-19 and the xenophobic rhetoric used by Donald Trump and other politicians to blame China for its spread. Stop AAPI Hate, a national coalition formed to address anti-Asian discrimination during the pandemic, has received reports of 3,800 anti-Asian hate incidents since last March. The group found that businesses are the primary location where these incidents occur, followed by public streets and parks.\nBen Hires, chief executive of the Boston Chinatown Neighborhood Center, said the Atlanta shootings were quite the gut punch.\"\nI instantly thought about our staff and the people we support . . . The women that were tragically murdered in Atlanta . . . we know them, we serve them every day here locally,\" said Hires, who is of South Korean descent.\nHires said the killings underscore an ugly reality that many have ignored until now: that racism and prejudice have always targeted the Asian business community. He said business owners were already grappling with a loss of revenue due to xenophobia and Asians being scapegoated for the coronavirus, so the violence adds another layer of fear.\"\nWomen in our community are on the front line of the business sectors . . . They are nail salon workers, restaurant workers, they are taking care of their families,\" he said. Low-wage workers and even white-collar workers like myself . . . you are living through these days with a heightened sense of safety, being more aware of your surroundings.\"\nThao Ho, who works as a paralegal and community organizer to support nail salon workers in Massachusetts, said the industry is staffed mostly by Vietnamese immigrants, some of whom are undocumented. One salon worker who has been putting in longer hours to earn enough to get by during the pandemic told her she's uneasy about traveling home later at night than she used to.\nThe day after the murders . . . she told me she was really afraid of taking the subway,\" Ho said. Although she can't actually point to any [violence against her], she feels in her heart that it is a necessity to even switch train cars so that she is not by herself.\"\nA business owner in Chinatown, who declined to provide his name for fear of being targeted, said he feels especially anxious when walking down poorly lit streets at night. A drop in foot traffic due to the pandemic hasn't helped.\nI would be lying if I say I have no concerns or fear,\" he said through a translator. It seems like the racial tension is on the rise.\"\nThe Metropolitan Area Planning Council is in the midst of a months-long effort to reach small businesses owned by Asian immigrants, with the goal of making policy recommendations on how local governments can better serve the community. Some of the recommendations, announced in a presentation Wednesday, include more translation services for business owners so they can better access resources, as well as a commitment from local governments to denounce xenophobia and racism against the Asian American community.\nThe MAPC, which plans to formally present its recommendations to officials in the coming weeks, also suggested the state invest in tracking and researching anti-Asian acts of hate and violence.\nSome cities have already said they would increase their police presence in Asian communities following the shootings.\nBut Hires said that some Asian business owners may find it difficult to report acts of racism or violence if they do not speak English well.\nImagine trying to communicate through a system that may not feel welcoming,\" he said. I think everyone wants to have issues addressed, but people are probably weighing a feeling of whether they will be taken seriously.\"\nKaren Chen, the head of Boston's Chinese Progressive Association, an advocacy group, said increased police patrolling will go only so far in solving the problem.\nThose are temporary solutions,\" she said. Long-term, the economic viability of people in the community is important.\" She added that some owners are more concerned with paying their rent and their employees than fighting systemic racism.\nHo said even though nail salon workers know that there is xenophobia and anti-Asian racism being heightened at this time, they still have to think about the next day.\" Language and \nlegal-status barriers have hindered many from accessing the business and unemployment assistance they need.\nHires said he wants to see a community response from people of all walks of life speaking up about this, stepping in and being"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196830, Requested 7939. Please try again in 1.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196830, Requested 7939. Please try again in 1.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Last month, in the wake of the killing of six Asian women at a spa in Atlanta, Georgia, the United States woke up to the hostility Asian-Americans have lived with since the start of the Covid-19 pandemic. \nAs people grappled with the shocking surge in verbal and physical attacks targeting Asians, some major news outlets attributed the slow emergence of the worsening situation to the quiet nature of Asian people. \nBut that story was incomplete and inaccurate. What had prevented these incidents from surfacing faster was less about the shyness of Asians. What had been exposed was a fragmented Asian community. While Asians are looked upon as the same in the eyes of the West, they are not monolithic - far from it. \nEach Asian nation has its own unique heritage. Their people have different sets of struggles and at times have struggled against each other. The Chinese and the Koreans felt strong resentment against Japan for its government's mass killings during World War II. These memories are very much alive and continue to haunt trade, economic and interpersonal relationships. \nThese dynamics are also reflected among Asian immigrants in the US. Some 20 million Asian Americans trace their roots to more than 20 countries in East Asia, Southeast Asia and the Indian subcontinent, according to the Pew Research Centre. \nMany Asians in the US are first-generation immigrants, and new immigrants also typically participate less with other members of society outside their family group as they tend to be focused on making a living in their new country. \nAsian-Americans have not been united by an inclusive, singular identity. They are a collection of people from Asia who live in the same country. There has been a lack of a sense of cause or shared struggle. This is the main dynamic that has muted the many voices of US Asian communities. \nAfter the deadly Georgia shooting, that sense of common cause has been growing. Asian-Americans in Congress have loudly condemned the crimes. One memorable moment came when Grace Meng, a Democrat from New York, called out the violence and discrimination at a hearing on Capitol Hill in March. \n\"Our community is bleeding. We are in pain,\" she said. She told a panel that the rising tide of anti-Asian bigotry was fuelled partly by rhetoric from former US president Donald Trump, who repeatedly referred to Covid-19 as the \"China virus\" and \"kung flu. Other lawmakers such as Bee Nguyen and Sam Park also spoke out against race- based hate crimes. \nIt is true that Asian culture might be less confrontational than others, but that should not be confused as a sign of weakness. Anyone following the protests in Myanmar can see that their battles for democracy are loud, persistent and at times brutal. Many protesters are willing to give up their lives for a cause bigger than themselves. There is nothing shy and timid about that. \nMany in the US can attest to that quiet bravery. Senator Tammy Duckworth, a Thai-American, lost her legs serving in the Iraq war. Lee Wong, an Asian-American Army veteran, bared his chest during a meeting in West Chester, Ohio, last month to show the vicious-looking scars he sustained from his service. \nIt is simplistic for the media to explain away an invisibility problem with Asian shyness. That is a stereotype and, more accurately, a form of racism in itself. \nThe bloodshed in Georgia might be the needed moment for Asian unity. Since that massacre, Asians have grown more vigilant and more clear-headed about keeping their loved ones safe as well as responding to a call for action. More solidarity is needed on the ground level, maybe with the help of the Congressional Asian Pacific American Caucus that has been pushing for such a coalition on the Hill for years. \nNo matter how vastly different each Asian person might feel from each other, it was made abundantly clear how similar we are, or at least we are perceived. In a year of escalating attacks, people of Chinese ethnicity are not the only ones targeted. \nThe racist \"China virus\" rhetoric had nothing to do with South Korea - its government has been among the best in handling the pandemic  and yet four people of Korean descent fell victim to the senseless shooting in Atlanta. \nAs the US death toll from the pandemic continued to climb because of Washington's inability to contain the virus under Trump, more Americans bought into the false narrative that Asians were at fault and continued to look for scapegoats to soothe their disgruntled selves. \nThe worsening abuses that continue to this day should jolt Asians awake to the reality that to unite is the only way forward. It was, and continues to be, about life or death. Maybe this can be the moment when Asians will come together under the same cause - against the hate crimes targeting the group. \nAsians would then learn to have each other's back regardless of their countries of origin. Then there could be a real chance for an equitable voice that starts with breaking the stereotypical image of Asian timidity. \nJodi Xu Klein is deputy bureau chief for North America at the Post"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193035, Requested 8104. Please try again in 341ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193035, Requested 8104. Please try again in 341ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "When she first read the headlines from Georgia, Danielle Kim knew in her bones what horror had come to pass.\nA series of shootings at three spas in Atlanta and a nearby suburb had left eight people dead, including six Asian women. At least four of the six women were of Korean descent, like Kim.\nIt was clear to me this was not just a random attack on just regular small businesses, Kim said. It appeared, based on what we're seeing, that this likely may have been a lot more targeted toward the Asian community and specifically, Asian women.\nThe suspect in custody  a 21-year-old white man named Robert Aaron Long  claimed he was not motivated by racism. But to many Asian Americans, Tuesday's deadly rampage felt like the culmination of a year in which anti-Asian racism has soared.\nIn the wake of the coronavirus pandemic, Asian Americans have been insulted, harassed, spat upon, shunned, beaten, and killed. The organization Stop AAPI Hate has recorded nearly 3,800 anti-Asian hate incidents nationwide since last March, when the group first started tracking the attacks. More than two-thirds of the victims, the group noted, were women.\nLaw enforcement officials have said that Long claimed he was motivated by sexual addiction, but investigators are not ruling out the role anti-Asian bias might have played. Long has been charged with eight counts of murder and one count of aggravated assault.\nFour of the victims who were killed have been identified: Delaina Ashley Yaun, 33; Paul Andre Michels, 54; Xiaojie Tan, 49; and Daoyou Feng, 44.\nRegardless of the motive, the slayings have rattled many Asian Americans, still reeling from the recent and highly publicized assaults on Asian elders in California and New York. On Jan. 28, an 84-year-old Thai man named Vicha Ratanapakdee was brutally beaten during his morning walk in San Francisco. He died of his injuries days later. Earlier this month, an 83-year-old Korean women in White Plains, N.Y., was struck so hard she blacked out.\nAs she processed the news, Kim thought of her Korean parents, small-business owners in New Jersey, and her near constant worry for their safety. And she was reminded of an incident last March, at the beginning of the pandemic, when a middle-aged white woman snarled at her to go back to where [she] came from as they passed each other in the Seaport.\nThat comment sat with me for days and weeks to come, said Kim, secretary of the Massachusetts Asian American Commission. Every once in a while, I'll think back to that moment.\nThis past year of sickness, isolation, and death has been emotionally and psychologically taxing for nearly everyone, but for Asian Americans, grief and anxiety have been compounded by anti-Asian sentiment fueled by the pandemic. Civil rights groups have accused former president Donald Trump of stoking the flames of hate when he referred to the coronavirus as the China virus or Kung flu. But the history of anti-Asian racism is much longer.\nHistorian Ellen Wu, director of the Asian American Studies Program at Indiana University, said Tuesday's killings stem from a wretched tradition of discrimination against Asian women, who are stereotyped not only as passive and submissive, but as hypersexual and exotic.\nThe fetishization of Asian women began with the Page Law of 1875, the first federal statute to restrict immigration to the United States. The law was ostensibly aimed at preventing Chinese prostitutes from entering the country, but in practice, it excluded virtually all Chinese women, Wu said. During and after the Korean War, American soldiers regularly frequented Asian sex workers  women, Wu notes, who didn't have a lot of options in countries devastated by war.\nFrom Madame Butterfly to Sayonara, the image of the feminine, sexual, and subservient Asian woman has been recycled generation after generation in popular culture, Wu said, and consequently, has become embedded in our collective cultural imagination. And so for many Asian American women, it's impossible to separate the Georgia shooter's apparent misogyny from the specter of racism.\nThere is often a lot of unchecked, invisible violence that exists for Asian women, whether it's because of their race, or because of their immigration status, or the type of industry they're in, said Lisette Le, executive director of VietAID, a Fields Corner nonprofit that primarily serves Vietnamese nail salon workers. They can be seen as disposable.\nBoston City Councilor and mayoral candidate Michelle Wu, a daughter of Taiwanese immigrants, recounted in a statement her own experiences with anti-Asian racism following Tuesday's attacks.\nSome of my most vivid childhood memories involve racist encounters with strangers, she wrote. People who knew nothing about me except for my appearance feeling empowered to pull eyes into slits, or to chant ching chong sounds.\nShe also described that constant feeling of needing to be aware, ready, on guard whenever out in public.\nAsian Americans represent about 7 percent of the population in Massachusetts. In the United States, Asians make up about 6 percent of the population, up from about 2 percent in 1980, William Frey, chief demographer \nat the Brookings Institution, told The New York Times.\nFeelings like those experienced by Wu are not uncommon among Asian American women. A 27-year-old Southeast Asian woman who spoke to the Globe recalled how a stranger pulled a fake gun on her while she was walking to work in Dorchester last May. The woman, who asked to remain anonymous so her family wouldn't worry, said a man was approaching her from the opposite direction when he aimed a bright green firearm at her"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195670, Requested 7701. Please try again in 1.011s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195670, Requested 7701. Please try again in 1.011s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "At a store in Alhambra, a worker refused to ring up Asian American customers, hurling a racial insult at one. \nAt a school in Texas, students picked on Asian American classmates, saying they had the coronavirus and everyone should stay away from them. \nIn Muskegon, Mich., a customer spat on an Asian American worker and said, \"Go home and take the COVID with you.\" \nA report from Stop AAPI Hate released Thursday shows the breadth of anti-Asian racism during the COVID-19 pandemic. Racist attacks, some verbal and some physical, were reported in cities like Alhambra and New York with large Asian populations and Muskegon with few Asian residents. \nThe attacks took place in schools, restaurants, stores, subways, sidewalks, fitness classes and Zoom meetings. Some involved allegations about the coronavirus, while other perpetrators used age-old racial slurs like \"ching chong.\" \nSome victims were elderly, others children. Some were coughed on or spat on, some slapped or hit, some refused service at businesses, still others stung by racist words or by people declining to interact with them. \nThe incidents were reported to Stop AAPI Hate, a group formed in March 2020 in response to attacks related to the perception that Asians were responsible for the coronavirus because of its origins in Wuhan, China. \nThe new study builds on a previous one released two months ago. \nIn March, people around the country reported more than 2,800 anti-Asian hate incidents to Stop AAPI Hate. In the entire year before that, the group received about 3,800 reports of racial incidents. \nThe sudden surge could be due in part to greater awareness that Stop AAPI Hate, founded by a coalition of Asian American advocacy groups, is collecting reports about racist incidents. But law enforcement agencies around the country are also seeing a large increase in reported anti-Asian hate crimes. \nNew York saw the greatest increase, at 223%, followed by 140% in San Francisco and 80% in Los Angeles, in the first quarter of this year compared with the same period last year, according to a report by the Center for the Study of Hate and Extremism at Cal State San Bernardino. \nHate crimes are attacks on people or property motivated by race, gender or other protected characteristics. Hate incidents involve name calling or insults but do not rise to the level of a crime. \nThe Stop AAPI Hate study includes both types of situations. Nearly two-thirds of the more than 6,600 anti-Asian incidents reported from last March to this March involved verbal harassment. About 18% involved shunning -- people deliberately avoiding Asian Americans and 13% were physical assaults. \nCivil rights violations such as workplace discrimination and being refused service at a business were 10%. Online harassment was 7%. \nOn its website, the group accepts reports in 10 Asian languages, including Chinese, Vietnamese, Korean and Tagalog, in addition to English. \n\"Racism against Asian Americans is systemic and long-standing,\" said Russell Jeung, cofounder of Stop AAPI Hate and professor of Asian American studies at San Francisco State University. \n\"The more we draw attention to anti-Asian hate, the more Asian Americans know they have a place to report what they're experiencing, and the more we can demonstrate the extent of the problem and advocate for meaningful solutions.\" \nIn some instances, attackers say nothing explicitly racist, but victims feel targeted because of their race. \nShelley Nguyen, 48, reported a series of incidents to Stop AAPI Hate last summer. \nA white woman repeatedly accosted her at a San Francisco park, demanding that she cover her face while jogging, then erupting into curses and insults, said Nguyen, a physician who is Vietnamese American. \nOne time, the woman removed her neck gaiter and coughed on Nguyen, her husband and daughter, telling the child: \"Geez, kid, I'm sorry your mom is such a b--.\" \nJasmine Li, a Chinese American from Aliso Viejo, said her 10-year-old daughter was waiting for a ride home from school in October when four or five boys asked: \"Why are you here? Are you sick? Do you have COVID?\" \nLater, a larger group of students, including the ones who had taunted her earlier, started chanting \"Corona, Corona, Corona,\" when the child walked past them on her way to the school's front office. \nLi, an information technology specialist, said she and her husband had heard about racist incidents throughout the pandemic and were worried their daughter would face further attacks. \nShe immediately reported what happened to the principal. \n\"This is not right,\" she said. \"Children should not be picked on, and thank goodness my daughter is too young to be able to tell what type of things are racially motivated, but we still need to protect her.\" \nCaption: PHOTO: THAI AMERICANS are joined by political leaders and law enforcement at an L.A. rally against hate."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196355, Requested 7671. Please try again in 1.207s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196355, Requested 7671. Please try again in 1.207s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "If you are a mom to teens like me, you know that it can be quite a feat to grab their attention for more than a few minutes at a time, or even get any meaningful details about their day without it ending up being a game of 20 questions. So, when it comes to discussing more serious matters, such as the growing anti-Asian hate and violence in our communities, it may be challenging for parents to get a good sense of what they are thinking and feeling on a deeper level.\nTo gain better perspective, AsAmNews interviewed nine Asian American teenagers from across the country to get them to talk frankly about the surge of anti-Asian attacks, how it has personally affected them, and what they believe can be done to help stop Asian hate.\nGregory Lee, age 15\nPublic School Student, Menlo Park, CA.\nThe anti-Asian hate attacks are quite troubling and make me question the integrity and morality of Americans. Personally, these attacks have not affected my day to day life, nor have they affected myself going outside or back to school. I do realize that I may be in a minority here, so I feel lucky. As of now, I am not afraid of being bullied or attacked because I am Asian, but I am definitely more aware and untrusting towards the public. Unfortunately, I do not think stopping Asian hate or really any type of hate is possible, as there will always be people who are unaccepting or scared of outsiders. The best we can do is bring attention to the problem and try to fix issues in our local communities in order to enact widespread change and allow Asians to feel safe. I find it frustrating that people always find scapegoats for their problems instead of facing their issues head-on. The adults of this world should not act like young children who have a simplistic and childish vendetta against people who havent done anything directly to them.\nJustine Schulke, age 15 \nPrivate School Student, New York City\nPeople may think that Anti-Asian violence started because of the coronavirus  since it originated in China  but racism against Asians Americans has existed in our country for years. Though Ive never experienced anti-Asian racism directly, Ive consumed the recent attacks through the media and from my familys experiences. The rise of the Black Lives Matter movement shed light on how systematic racism exists, and has always existed, in our country. Im not surprised to know something like hate crimes are now skyrocketing against the Asian community, but its frustrating to see how so many people continue to be so ignorant and heinous, hating and abusing our elders for something they havent done. There are notions that Asians arent truly American. Asians are associated with exoticity, which makes Asians seem foreign. The news of these attacks have not had a significant effect on my day to day life, but I now carry a keychain with an alarm just in case, and Im not allowed to take the subway. I feel extremely safe at school. My school has been implementing many conversations on race, racism and what we need to do as a school and community to be anti-racist. Our summer reading homework of 2020 was to read Stamped: Racism, Anti Racism, and You: A Remix of the National Book Award-winning Stamped from the Beginning by Ibram X. Kendi. However, in public, there is a subconscious thought in the back of my head whether I might be the next Asian American involved in a hate crime. I think the first step to stopping the hate is to acknowledge that there are hate crimes happening against the Asian community. Secondly, we need to address this issue to the school, or if youre uncomfortable, to spread awareness on social media and to your friends. Further, we should talk about these violent attacks within the school community, at a workplace, or wherever you are to denounce anti-Asian racism. Finally, we need to get involved, whether that be attending protests, handing out fliers, supporting Asian owned businesses, reporting hate crimes, condemning hate, and combating the prejudice and bigotry of the connection between Asians and Covid-19.\nMegan Kwan, age 16 years old\nPublic School Student, Framingham, M.A.\nIm absolutely disgusted by recent attacks on Asian Americans. Why is it that murder needs to occur before the world recognizes the discrimination against Asians every day? I hate how people are excusing their actions with trivial excuses. I hate how people are just now trying to stop the hate. Although support is always welcome, it should not have had to take death to finally get the medias attention and for people to realize just how normalized these attacks are. I fear for the safety of my family and others. I should not be scared that my grandmother might get beaten every time she goes to Chinatown, or that my sister will be called racist names when she goes back to school. Fear is just the beginning of it. It also affects me due to the media attention its received recently. Everywhere I see people arguing online because someone is so uneducated that they believe it is called the Chinese virus because its from China. I want people to stand up for Asians because they recognize that we are being harassed and assaulted. I want them to realize that asking if someone has eaten a dog before is just as racist and harmful as the attacks you see on the news. I am scared of what would happen at school. Although Ive never experienced bullying before, Im deathly afraid of the harassment I might receive if I go back. Before Corona, there were multiple instances of racism at my school, and I fully believe that it would increase. Such as people only grouping me with Asians because were the same, people telling me I have small eyes, people mocking my food, asking if I eat dogs, etc. Im not very strong or athletic, so If"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196382, Requested 7414. Please try again in 1.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196382, Requested 7414. Please try again in 1.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "for my own grandparents since the majority of attacks have targeted the elderly. If we want change, we must continue to speak out because these attacks are not stopping anytime soon. We have a responsibility to not stay silent in order to educate our fellow Americans on the strength of their words and the power of their actions. If we do our part, more and more people will become aware of our situation and begin to actively learn about the ongoing virus of racism.\nHannah, age 15\nPublic School Student, Massachusetts\nEver since COVID started, I had already noticed the uptick in hate crimes against Asian Americans and how they mostly went unnoticed with the news barely covering these attacks. It made me terrified to go outside for a walk or to get groceries with my family being fully aware that I could easily get caught up in one of those events. The growing news and coverage over the last year of these attacks has made me pretty self-conscious about every little thing I do. Every time Im out somewhere public or in school, I feel like Im being judged every step I take, and its agonizing. Before I went back to school, I do specifically remember talking to my mom about how I was scared that someone was going to bully me at school or say something passive-aggressive and that in the end I wouldnt know how to react. I still feel that way. Whenever someone is whispering to someone else or staring at me in public or at school, I think the worst. I think that theyre making a snide remark about my race and it makes me anxious. In my opinion, I know its impossible to completely stop racism, but we can start by not blaming a single race for everything thats going on now. Its just not right.\nAlexis Quan, age 18\nPublic School Student, Framingham, M.A.\nI am aware of the anti-Asian attacks that have been going on the news more recently and when COVID-19 first struck. I do not like seeing the acts of violence and hearing of the verbal tirades someone says against the Asian community. It makes me feel disturbed that people are going out of their way to harm others simply of the color of their skin. It makes me feel unsafe. Id rather not have to go outside and deal with someone yelling at me for carrying the virus when I have my mask on and they do not. I am not afraid of being bullied at school. I have a good support system of friends there who I know will stand up for me. In the beginning of the pandemic, I was afraid that some people would do the classic Go back to China!  despite the fact I have never left this continent  and make racist comments about me. Now, when I am out in public, I am just wary of others. If someone looks tense or they keep staring at me, I leave and walk away. I do not want to anger them and drive them to violence. To stop the spread of Asian hate, I would stop or prevent news outlets/sources, as well as people with political power, from saying degrading and derogatory language against Asian Americans. I feel that ex-President Donald Trump made situations worse by pinning the blame on China for not being able to contain the virus. He called it the Chinese Virus, furthering his followers dislike against China. It seems that some people took this response as an OK to use hate speech and even commit hate crimes. I would tell people to check their news sources. Is what they are getting actually verified and reliable information? To lessen peoples fears, dont give into the hate. Just because someone is of Chinese or Asian descent, does not mean they automatically have the virus; that is not how this virus works. Do not be afraid as long as you are wearing a mask and are performing safe behaviors when outside. I would also tell people and educate them using social media posts. Teens are more reachable when facts are in short, colorful blurbs. They can then repost the article on their social media accounts.\nBenjamin C., age 16\nPublic School Student, Chicago Suburb\nI am aware of the anti-Asian hate attacks and the incitement that is occurring in America today. The constant cycle of violence is disgusting. I am lucky to live where these attacks do not affect my daily life much, at least not to the extent of others. It has not changed going back to school in person. The attacks have definitely impacted my family, but luckily not our day to day life, and we are still able to do much of the things we used to. Im not afraid of being bullied for being Asian, but it is definitely present. People need to change their biases and think about the situation in anothers shoes. The attacks are a result of the lack of compassion, education, misinformation, coupled with a plethora of other reasons, and we should tackle the issue head on instead of pushing it to the side."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196364, Requested 7751. Please try again in 1.234s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196364, Requested 7751. Please try again in 1.234s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "lived for 26 years, his brother John said. Paul Michels had expressed interest in owning a spa and according to The Washington Post worked as a handyman at Young's Asian Spa.\nJohn Michels said his brother was \"just in the wrong place at the wrong time.\" They grew up with nine siblings in Detroit, riding dirt bikes and spending summer weekends at a lake and getting into mischief together, he said. They both served in the U.S. Army at the same time and his brother was an infantryman in the late 1980s.\n\"I'm the closest in age so we were basically like twins,\" said John, 52. \"We did everything together growing up.\"Yong Ae Yue\nYong Ae Yue, 63, worked at the Aromatherapy Spa in Atlanta. She came to the USA from South Korea in the 1970s with her husband Mac Peterson, The New York Times reported. Citing Peterson, the newspaper said they met while he was stationed in the Army and had a son before moving to Fort Benning, Georgia, and later had another one. The couple divorced in 1982 but stayed in touch.\n\"She was a good mother,\" Peterson told the Times. \"She was always there for her kids.\"\nThe organizer of a GoFundMe page that has raised nearly $64,000, who goes by the name of Robert Peterson, said he is Yue's youngest son.\n\"Mom was an amazing woman who loved to introduce our family and friends to her home-cooked Korean food and Korean karaoke,\" he wrote. \"Will miss joining mom on her weekly Sunday routine to the grocery store and traditional Korean dinner. She was always kind-hearted and willing to help everyone she encountered.\"\nThe Atlanta Journal-Constitution, which cited Yue's two sons, said she was a licensed massage therapist who got laid off last year amid the pandemic and was happy to get back to work at the spa.Suncha Kim\nSuncha Kim, 69, was a grandmother who worked at the Gold Spa across the street from Aromatherapy and liked to line dance, the Times reported. Quoting a relative who asked not to be identified, the newspaper said Kim had been married for more than 50 years and had come to the USA from South Korea seeking better educational opportunities and a better life for herself and her family.\nThe Washington Post reported that Kim came to the USA around 1980 and after working a number of odd jobs offered her cooking services as a volunteer to help raise funds for several organizations. Family members told the Post she made her children a priority.\nOn a GoFundMe page that has raised more than six times the original goal of $20,000, organizer Hillary Li thanked supporters on behalf of Kim's family: \"It brings tears to our eyes that you are all standing with us and our beloved halmoni (grandmother), mother, and wife. Suncha was such a strong loving presence in all of our lives, and we miss her so much.\"Soon Chung Park\nSoon Chung Park, 74, who worked at Gold Spa, moved to Atlanta after spending the majority of her life in the New York metro area.\nHer son-in-law Scott Lee told the Post that after he married Park's daughter, they all lived under the same roof in Lyndhurst, New Jersey, before Park moved to Georgia. She had planned to move back in with Lee and his wife in June, he said.\nLee said Park liked to work and stay active.\n\"She was very healthy,\" he said. \"Everybody said she was going to live past 100 years old.\"Daoyou Feng\nDaoyou Feng, 44. Not much is known about Feng who, according to a friend of Xiaojie Tan's who spoke with The Washington Post, started working at Young's Asian Massage in the past few months and was kind and quiet.Elcias Hernandez-Ortiz\nElcias Hernandez-Ortiz, 30, survived the shooting and is hospitalized in intensive care. He was intubated and is set to have surgery to remove the bullet in his abdomen, his wife Flor Gonzalez told USA TODAY. She set up a GoFundMe page to help pay for medical costs where she wrote he was shot in the forehead, lungs, and stomach. \nGonzalez told USA TODAY that her husband was on the way to a business next door to the spa where he sends money to family back home. He called her during the shooting. \n\"They shot me, they shot me, come help me please,\" she said Hernandez-Ortiz begged. Those were the last words Gonzalez heard from him, she said on the verge of tears. \nThe two are from San Marcos, Guatemala, an impoverished municipality in the Central American country's rugged mountains. Hernandez-Ortiz came to the USA almost a decade ago. His wife and their 9-year-old daughter joined him in Georgia in 2015. \nShe said the family depends on Hernandez-Ortiz's work to survive. \"There are so many people that depend on him,\" Gonzalez said. \"I know he is strong and will come out of this for all of us.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198389, Requested 6668. Please try again in 1.517s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198389, Requested 6668. Please try again in 1.517s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Asian Americans were the fastest-growing racial or ethnic group in the United States from 2000 to 2019, with a population projected to pass 35 million by 2060, the Pew Research Centre has found, based on analysis of US Census Bureau data. \nThe Asian population in the US grew 81 per cent during that period, from roughly 10.5 million to a record 18.9 million, according to Pew's analysis. \nHispanics saw the second-fastest population growth between 2000 and 2019, followed by the native Hawaiians and Pacific Islanders, at 70 per cent and 61 per cent, respectively. \nThe nation's black population also grew during this period, albeit at a slower rate of 20 per cent, Pew said. \nThere was virtually no change in the white population. \nThe US is getting more diverse. \nLast July, a Brookings Institution analysis of census data showed that America's white population share declined 10 percentage points between 1980 and 2000, and by 2019, nearly 9 more percentage points, to 60.1 per cent. \nBy around 2045, white people will themselves be a minority. \nTension \nRacial anxiety underlies much of the political tension in the US. \nA study of 2016 election voter preferences by academics John Sides of George Washington University, Michael Tesler of the University of California at Irvine, and Lynn Vavreck of the University of California at Los Angeles, concluded that white voters' preference for Mr Donald Trump, who won by a landslide, was strongly related to concerns that minorities were taking jobs away from whites. \nLong-running prejudice against Asians has also erupted anew. \nPew Research's analysis of the Asian American demographic comes amid a documented rise in incidents of abuse and hate against Asian Americans that prompted condemnation from President Joe Biden last month. \n\"Too many Asian Americans have been... attacked, blamed, scapegoated and harassed,\" the President said. \n\"Hate and violence often hide in plain sight. And it's often met with silence. \n\"That's been true throughout our history, but that has to change - because our silence is complicity. \n\"We cannot be complicit. We have to speak out. We have to act.\" \nPrejudice \nA separate Pew Research Centre survey conducted early last month \nbefore the fatal shooting of six Asian women and two other people in the Atlanta area on March 16 \n- found that 70 per cent of Americans believe there is discrimination against Asian people. \nAnd in a survey last June, 31 per cent of Asians reported they had been the subject of slurs or jokes since the Covid-19 outbreak began, and 26 per cent said they had feared someone might threaten or physically attack them because of their race or ethnicity, Pew said. \n\"The recent violence against Asian Americans is not a new problem,\" Ms Gloria Lim Steil, a Korean American who lectures in literary and critical studies at the Pratt Institute, told The Straits Times. \n\"It's a continuation, albeit a ratcheting up, of such violence going back to the mid-19th century.\" \nAlthough Asian Americans have been part of the fabric of America since the 1850s, their history is barely known to most Americans. \nMs Steil said: \"Asians are not a monolith. Grouping all Asians together, dismissing the enormous diversity of the Asian diaspora, is part of the problem. \n\"Chinese are as different from Koreans and Filipinos as Brits are from French and Ukrainians.\" \nShe added: \"When the pandemic hit, Trump's rhetoric exacerbated this perpetual prejudice. \n\"People started seeing Asian Americans as foreigners and targeted them.\" \nSpecial police squads have been formed in some places to curb attacks on Asians. \nVolunteers are patrolling Chinatowns in California. \nIn New York City last week, on an escalator entering Penn Station in the heart of Manhattan, an undercover police officer of Asian descent was verbally abused by 35- year-old Juvian Rodriguez, who said \"go back to China before you end up in the (expletive) graveyard\", and then threatened to stab him in the face. \nRodriguez was arrested. \nAlso in New York City last week, police arrested Joseph Russo, 27, for three separate attacks against Asian Americans, including two women and a 77-year-old man."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195338, Requested 6339. Please try again in 503ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195338, Requested 6339. Please try again in 503ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "to work hard to help targeted communities feel safe\" she said. \nMany people said they were trying to strike a balance that lets them feel comfortable as much as they can in public. It can be an agonizing calibration just to take a walk: Will wearing a mask act as a shield or attract unwanted attention? Is daytime safer than night? Are largely Asian neighborhoods safer or more likely to be attacked? \nMany residents have also called on the police to increase patrols and some communities have started their own neighborhood watches. \nSome Asian Americans said they were heartened by a new federal law that seeks to strengthen the law enforcement response to a nearly 150 percent increase in anti- Asian attacks many of them aimed against women and older people. \nStill many remain fearful. \"When society is more open that means more threats\" said Jeff Le a political partner at the Truman National Security Project a think tank. \nMr. Le has returned to much of his prepandemic life but said he is still anxious about getting back on a plane since the day in March 2020 when a woman at the Reno- Tahoe International Airport spit on him and said \"Go back to where you came from.\" \n\"It was a feeling of helplessness like I'd never felt before\" Mr. Le said. \"That's something I can't shake. It made me feel like I was a cancer or something radioactive.\" \nEven as Americans poured back onto planes over Memorial Day the thought of flying again made Mr. Le queasy. He has visited 85 countries and used to travel constantly for work but has been grounded since last year. \"I'm a little more nervous about it than I'd thought\" he said. \nCathie Lieu Yasuda said she felt safe walking through her hometown Folsom Calif. but said it was still too risky to take her ninth-grade daughter and fifth-grade son to a Giants baseball game. Whenever she and her children go out they follow a new rule of social distancing: Not six feet to stop the spread but arm's length to keep from getting shoved or punched. \n\"The sidewalk is big enough\" Ms. Lieu Yasuda said. \"We're not afraid. We're not cowering. We're being safe.\" \nAfter getting vaccinated Augustine Tsui is again commuting from New Jersey to his law-firm job in Midtown Manhattan but he said he does not know when his life or commute will ever feel normal. After years of taking the bus and train he now drives to work and pays as much as $65 in parking the price of easing his family's worries. His wife Casey Sun stays at home making organic soaps and cosmetics for her online business and said she rarely leaves the house. \nMr. Tsui's office is not far from where an attacker bit off part of an Asian man's finger in mid-May. Mr. Tsui wears a mask to conceal his face as he hustles inside. \n\"Instead of getting anti-Asian comments it's not entirely clear who I am\" he said. \"I can just go about my day.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198305, Requested 6966. Please try again in 1.581s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198305, Requested 6966. Please try again in 1.581s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Palm Desert's mayor and council members denounced the \"hateful acts\" of a group of women who used profanities and mocked the Korean accent of a longtime resident and restaurant owner this week when she asked them to leave at closing time.\n\"Behavior such as this does not represent the values of Palm Desert,\" Mayor Kathleen Kelly said during Thursday's City Council meeting.\nThe incident occurred Monday night at Musashi Japanese Restaurant and Sushi Bar on Highway 111 in Palm Desert, where owner Stella Kim said she had a run-in with three customers, all women, who came in for dinner early in the evening.\nThe women dined inside and remained at the table talking long after their meal had been served, Kim told The Desert Sun on Friday. The restaurant closes at 9:30 p.m. and by 9 p.m., the three women were the only customers remaining, Kim said.\nWhen staff began preparing to close  putting up the chairs, sweeping the floor and cleaning  the women continued to chat among themselves, she said.\nWhen Kim heard them call Uber for a ride, \"I said, 'Ladies, can you wait for Uber outside, please?'\" Kim said.\nThey got up to leave without saying anything, and Kim said she thanked them.\nThen, one of the women came back in to use the restroom and began complaining to Kim about the food saying it was horrible, they had to wait too long and would not be back.\n\"I told her, 'That's fine,'\" Kim said.\nThe woman continued to complain and argue, cursed at Kim and began mocking her Korean accent, Kim said.\n\"I have never had this experience in Palm Desert,\" said Kim, who moved to the Coachella Valley in 1993, opening her restaurant  one of two  in 1996.\n\"I have such good customers and I never felt they looked down on me,\" Kim said.\nShe and her husband raised their children in Palm Desert, and said none of them have experienced racism in the valley.\n\"I'm very happy to have raised my children in this valley,\" she said.\nRacial attacks  verbal and physical  on Asian Americans and Pacific Islanders have been on the rise, largely fueled by people who say they blame people of Asian descent for the COVID-19 pandemic.\nThere have been reports of Asian people being recently attacked in the Los Angeles area and on March 16 eight people, including six Asian women, were killed when a gunman opened fire in three spas during a spree that spanned the Atlanta area. Authorities continue to investigate the suspect's motives.\nThe Indio City Council may soon consider a resolution condemning racism and violence against Asian Americans in the east valley city, as a result of the Georgia shootings.\nKim said the three women, all white, had never before been in Musashi.\nMayor: 'We abhor all hateful acts'\nWhen Kelly heard of the incident, she issued a written statement Thursday which she later read during the City Council meeting.\n\"We abhor all hateful acts. A hateful act toward any of us diminishes us all. Diversity with its varied experiences, perspectives and languages is a vital asset of this community,\" Kelly said.\nThe city in 2020 adopted \"Unite Palm Desert\" as its mantra and guiding principal, Kelly said.\n\"Over the last 12 months, much of our focus under the 'Unite Palm Desert' banner has been about holding up those most affected by the pandemic with essential support,\" Kelly said.\n\"But 'Unite Palm Desert' is about much more than that. It encourages our whole community to embrace the principles of mutual respect expressed in city of Palm Desert Resolution No. 2018-09, in which we affirmed that this city will not tolerate prejudice, racism, bigotry, hatred, bullying and violence towards any groups within our community.\"\nNational Public Health Week is commemorated April 4-10, which Kelly said makes it timely to affirm that much more than physical well-being contributes to public health.\nShe and the City Council on Thursday presented a proclamation \"imploring us all to recommit ourselves to achieving a fully healthy community in which people of all ethnicities, identities and backgrounds consistently enjoy equal respect.\"\nOther council members also commented on the incident involving Kim.\n\"This personally, as an English learner, is something that spoke to me, as somebody who has been told, 'Learn English. Speak English. Go back to your country.' These are things that I have heard, that my family has heard,\" Councilmember Karina Quintanilla said.\nWhen someone speaks to her with an accent, Quintanilla said, \"I hear hope, I hear hard work, I hear determination, and that's what an accent means to me.\"\nMayor Pro Tem Jan Harnik said she has known Kim and her family for a long time.\n\"Their children graduated from Palm Desert High, they've been great supporters of the high school; they've been great community members and it's a great family and they're wonderful people,\" Harnik said. \"I know Stella well and ... it's hard for me to even understand how anybody could say that to the caliber of human being that Stella is.\"\nCouncilmember Sabby Jonathan said he is an immigrant who has been \"the recipient of hate throughout my life. But you know, I always felt that those who hate and do these bad things to other people do not represent our city, the Coachella Valley or our country.\"\nKim said she appreciates the city's support.\n\"I don't want to make a big deal about this ... but this is upsetting,\" she said."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197724, Requested 6707. Please try again in 1.329s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197724, Requested 6707. Please try again in 1.329s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": ". \nThe survey also offered a first national look at how much live instruction students are getting, an important metric given the large number of students still learning from home. About a third received more than five hours a day, about a third were offered three to four hours per day, and about a third got two hours or less.\nThose figures surprised Peggy Carr, associate commissioner at the National Center for Education Statistics. \n\"I knew it was going to be low but not quite that low,\" she said.\nScant live instruction was most common in the Midwest and in towns, and less common in the Northeast and in cities.\nBut Carr said she was encouraged by the number of schools that were open for at least some in-person learning in January. A number of districts have reopened since then, so the current total is likely to be higher. The Education Department plans to report results from the same set of schools once a month. \nThroughout the pandemic, educators have talked about a need to prioritize certain students for in-person instruction. The survey found for fourth grade, 44 percent of schools prioritized students with disabilities, with about 1 in 4 prioritizing English language learners, students with lower grades, students without Internet access at home and students experiencing homelessness. Those percentages were higher across the board for eighth grade.\nNonetheless, the survey found that children with disabilities and those learning English attended in-person school at similar rates to other students.\nThe survey was conducted by the National Center for Education Statistics, part of the federal Institute for Education Sciences. It included 3,500 schools serving fourth-graders and 3,500 schools serving eighth-graders."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193137, Requested 7091. Please try again in 68ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193137, Requested 7091. Please try again in 68ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A new study just released by Stop AAPI Hate reveals that reports of hate from Asian Americans and Pacific Islanders have now reached 10,370. \nThe period covered begins with the start of the pandemic from March 19, 2020 to September 30, 2021. \nA survey released in conjunction with the report found Asian Americans with a high school education are experiencing hate at twice the rate of those with at least some college. \n41 out of 100 of those who halted their education after high school report being victims of anti-Asian hate versus 20% of those who entered college and 13% of those with a BA or higher degree. \n\"It's tragic but not surprising that Asian Americans with lower education levels are experiencing more hate,\" said Cynthia Choi, co-founder of Stop AAPI Hate and co-executive director of Chinese for Affirmative Action. \"Anti-Asian hate is tied to systemic racism against our community. Stopping hate is not about quick fixes like law enforcement but about deeper investment in our communities.\" \nEqually troubling for Asian American parents, 30% report their children have experienced anti-Asian hate in school. 31% of Pacific Islander parents report the same thing about their kids. \n\"The levels of Asian American children experiencing hate in school is devastatingly high,\" said Russell Jeung, Ph.D., co- founder of Stop AAPI Hate and professor of Asian American Studies at San Francisco State University. \"There needs to be an urgent push toward incorporating solutions that promote racial understanding in schools, including through investment in Ethnic Studies.\" \nHowever, the concern is not limited to just children. Adults report being worried about going back to work due to the backlash facing many Asian Americans. \n31% of Asian Americans and 26.4% of Pacific Islanders say they've experienced a hate incident at work during the past \nyear. \n23.5% of Asian Americans and 21.7% of Pacific Islanders say they fear going back to work because of potential hate or discrimination. \n\"When it comes to stopping anti-Asian hate, our elected leaders should be responsive to the Asian American community,\" Manjusha Kulkarni, co-founder of Stop AAPI Hate and executive director of Asian Pacific Policy and Planning Council. \"Locally and nationally, they must make real investments in civil rights, community resources, and education. \nAsAmNews has done numerous reports on efforts to further education about Asian Americans in the schools. \nMore education and public awareness is cited as the number one solution by \n52.8% for AAs and 57.5% for PIs. \nCommunity-based solutions and civil rights rank 2nd and 3rd ahead of more law enforcement."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198956, Requested 6980. Please try again in 1.78s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198956, Requested 6980. Please try again in 1.78s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "SEOULThe killings happened more than 7,000 miles away. But for many South Koreans, the Atlanta-area spa shootings hit close to home. \"The Victims Were Korean Mothers,\" read a headline Sunday from the country's largest newspaper.Of the eight people who died , six were women of Asian descentincluding four who have been identified as ethnic Koreans, ranging in age from 51 years old to 74. One was a South Korean citizen.The rampage in Georgia has reverberated across this nation of 52 million, which in the decades since the Korean War has had a deep and enduring relationship with the U.S. The two are allies and share close cultural ties.It can often seem like every Korean knows someone with relatives or friends living in the U.S. South Korea sends more of its children to study in America than in any other foreign country.Lee Myung-kyu, a 55-year-old office worker, said he knows many South Korean families who have dreamed of immigrating to the U.S., hoping for a better life. His own daughter wants to go to school in America. But Mr. Lee said he now has doubts.\"I keep thinking about whether something like this could happen to her,\" Mr. Lee said.Local police say the white man from Georgia charged with murder in the case said he was driven by what he called a sex addiction. Authorities say they are investigating whether the killings were racially motivated.The attack has sparked fear at the same time police and government officials in New York and other U.S. cities have said hate crimes against Asian-Americans have risen since the start of the Covid-19 pandemic, which first emerged in China.Han Ye-rim, 32, said she has long idealized the U.S. as a diverse society. But staring at a victim list that looks much like herself, Ms. Han wonders how she would actually fare leaving Seoul.\"Learning about the Atlanta incident was a wake-up call to me,\" Ms. Han said. \"I'm realizing that I can be targeted for being different if I leave this country.\"What made the Atlanta rampage especially jarring was how good South Koreans, and Korean-Americans, had been feeling lately about their standing in the U.S.U.S. Secretary of State Antony Blinken and Defense Secretary Lloyd Austin visited Korea on their first foreign trip . Barely a year ago, the South Korean film \"Parasite\" emerged with an unprecedented Best Picture win at the Academy Awards. BTS, the Korean pop band, had recently performed at the Grammys and topped Billboard's album charts .Meanwhile, South Koreans had rushed to the local box office to see the U.S. film \"Minari,\" which depicts a new Korean immigrant family in rural Arkansas and was itself just nominated for several Oscars.\"It's really a weird kind of dichotomy,\" said Abraham Kim, executive director for the Council of Korean Americans, a Washington-based nonprofit group, with celebrations of pop culture on the one hand and what he described as Asians \"being targeted for violence on the other.\"South Korean media has given widespread coverage to the Atlanta shootings. In a Thursday editorial, Kyunghyang Shinmun, a left-leaning newspaper, called American society \"defenseless to racist attacks.\" Another outlet, the right-leaning Segye Ilbo, urged the U.S. to take \"effective measures so that crimes against humanity do not take root.\"On Friday, President Biden, saying that the investigation is still under way, mourned the victims and declared that \"hate can have no safe harbor in America.\"South Korean President Moon Jae-in has called the Atlanta killings shocking, while the country's foreign ministry supported the U.S. government's efforts to stand against hatred and violence. \"Such a crime is unacceptable under any circumstances,\" the foreign ministry said in a Saturday statement.Walking with a friend just blocks from the U.S. Embassy in Seoul, where the American flag continues to fly at half-staff in honor of the shooting victims, Yoon Ji-a recalled living in California during her youth. Her parents had a few brushes with racism, she said. But the events in Atlanta caught her by surprise.\"It's scary,\" said Ms. Yoon, a 20-year-old college student.There are about 1.8 million Korean-Americans, according to the U.S. figures. The biggest Korean populations are in the metropolitan areas of Los Angeles, New York and Washington, D.C., according to Pew Research Center figures, analyzing U.S. data. Atlanta ranks seventh-largest.Jean Lee has two children living in the U.S., though she hadn't learned of the Atlanta-area shootings until local media began broadcasting coverage of the weekend protests and vigils across nearly two dozen American cities. Now the 48-year-old fears her children could be targeted.\"A lot of hate speech surfaced when people began calling the coronavirus the 'Wuhan virus' and it's unfortunate that this issue came to light because of the shootings,\" Ms. Lee said. \"It feels late for Asians who have been experiencing discrimination for so long.\"Jenna Lee, a 25-year-old online shopping-mall owner, said she lived in Atlanta for two years as a teenager. In recent days, she said, she watched \"Minari,\" with its tale of struggling immigrants, and it prompted her to wonder whether Asian-Americans would be forever foreign and forever invisible.\"Asians are more than just people trying to assimilate into American society,\" Ms. Lee said. And in her view, she said, \"the shootings show how vulnerable we are to discrimination.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195777, Requested 6997. Please try again in 832ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195777, Requested 6997. Please try again in 832ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "From Unsplash by Fernando AguilaWow, you speak English so good, called out the man, sitting casually on the ground in front of a DC Metro station. \n\"What?\" I blurted out in surprise. He had interrupted my conversation with my girlfriends as we were walking into the station. After I turned to look at him, he asked, \"Hey, can you spare any change?\" \nImmediately, I could feel the anger rising up in me as a wave of heat. After a long summer of being harassed by more than a dozen men for being a young Asian American woman who dared to take public transit, eat at restaurants, or walk down the street, I had had enough. I exploded at him. \nIt's been about 25 years, so I can't recall the exact words I used. I know I raised my voice. I know I told him that I was born here, in the United States, and that my English was better than his. I undoubtedly told him to go f- off before I angrily stomped away to catch the Metro. My friends were caught off guard by my reaction and I remember their surprised faces. Perhaps the man sincerely thought he was offering me a compliment. But by that point of the summer, I was DONE with racist (and often sexualized) street harassment. \nI had not lived outside of California, and in particular, outside of major metro areas like the San Francisco Bay Area and Los Angeles (with highly visible communities of Asian Americans and Pacific Islanders), since I was a toddler. So when I moved to Washington, DC at 19 to pursue a policy internship for the summer before my junior year at UCLA, I was not prepared to be an oddity. \nWhen my fellow UCLA roommates, also women of Asian descent, and I rode the Metro, went to restaurants, or simply walked down the street on our way to the July 4th celebration on the Mall, we were repeatedly harassed on the basis of both our race and gender identity. And this wasn't limited to inside the Beltway. We experienced harassment on weekend trips to New York City and Boston, too. \nSome of what we experienced falls under the definition of microaggressions, which Dr. Derald Wing Sue, a professor of counseling psychology at Columbia University, defines as the \"everyday slights, indignities, put-downs and insults that members of marginalized groups experience in their day-to-day interactions with individuals who are often unaware that they have engaged in an offensive or demeaning way.\" But many of the incidents were intentional acts of racist and sexist behavior. \nWe never reported any of the harassment to the police, nor should we have since I don't think any of the offensive conduct would be legally considered a crime. Yet just because such offensive speech is legal doesn't mean it isn't cause for concern. Hate speech can lead to physical attacks. Thankfully, my friends and I were never spit on, punched, pushed to the ground, or physically assaulted (unlike other Asian American and Pacific Islanders who have been over the past year, as documented in the Stop AAPI Hate report). \nThe summer I moved to Washington, DC, I was overwhelmed by the sheer quantity of aggressive and degrading verbal assaults, as men leaned out of car windows, approached us on the street, or came up to us in crowded restaurants. \nThese men would crow, \"I've never f - ed a Chinese girl, I wonder what it's like!\" or leer in our faces, shouting, \"Ching Chong, Ching Chong!\" \nI can't quite recall all the insults hurled in our faces, but I wouldn't be surprised if some of the men yelled out, \"Me so horny!\" or \"Me love you long time!\" This was the 90s after all, and 2 Live Crew and Sir Mix-a-lot's hit songs were still fresh in many minds. \nThere usually wasn't enough time to say anything in response other than \"F you!\" when drivers called out their insults, as they drove by too quickly. But the few times someone walked up to us and made these comments, I recall being too surprised and scared to say anything. Usually, walking away was my first response. \nOn a less aggressive note, there were also the repeated questions asking us where we were from. \"California!\" was never sufficient. People kept trying to figure out \"what\" we were. I started to brace myself for the question every time we got into a taxi, as the male driver would inevitably ask and then follow up with, \"No, really, WHERE are you from?\" \nIf I tried to reply, \"Well, I was born in New York City,\" then the questioner would inevitably ask, \"Well, where are your PARENTS from?\" Sigh. I would have to reluctantly admit that my immigrant parents are from Taiwan. If I felt like the questioner was genuinely interested or would recognize the difference, I would add that their families were refugees from China who fled to Taiwan after the Communists took over. \nSometimes, the comments were more innocent observations about our novelty. One Friday night, we went to TGI Friday's for dinner and dared to talk in boisterous voices, laughing as we enjoyed our meal. We were not noticeably louder than any other group of four young people hanging out with friends after a long week of work. \nNonetheless, our waiter was stunned. He literally said to us, \"Wow, you're not like the Asians around here. You don't walk around staring at the ground.\" He then called over other waiters from the rest of the restaurant to come gawk at us. \nTwo waiters actually collided because they were too distracted while staring at us. Our confidence and casual ease were unfamiliar to them. I know they were not being mean-spirited, but I felt like an exotic"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193140, Requested 7013. Please try again in 45ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193140, Requested 7013. Please try again in 45ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "LAST CALL WITH SAUL\nReach out to an Asian friend, or make one\nI have lived most my adult life in Asian neighborhoods. I moved to Mainland China shortly after college for a variety of reasons- a big one was that my privileged childhood brought me to many countries, but comparatively to so little of Asia. I landed in a \"small\" city in the Hunan province, three hours by train outside of Guilin, and my first six months had its challenges. I taught English to hundreds of noisy high schoolers, and my apartment was at most 30 steps away from my students' classrooms.\nThe city I lived in, Yongzhou, almost never saw foreigners - eager students often came by unannounced with photographers to take pictures with us. I learned to wear a hoodie when going to the grocery store. And without speaking Mandarin, I grew to rely on a community around me to keep me safe and to get me where I needed to go. Often that meant getting on the backs of motorcycles without ever knowing if I was understood.\nThe next six months in Shanghai, I lived a much more cosmopolitan life and thought about ways to re-enter journalism. I'd been filing my first freelance stories for a newspaper I encountered while in Hunan, the Guangzhou\nMorning Post, and then I flaked on the deadline to take the GRE for American journalism programs. I began looking for more permanent writing jobs in China, which is how I discovered a career portal on the University of Hong Kong website. So I applied, took an entrance exam and interviewed. The founder of the journalism center and dean at the time, Ying Chan, came to Shanghai to talk to the program's finalists. It felt more like a sales pitch-a good one, mind you-but I was still unsure what I would do. My mom called not long after I got in to say she and dad supported me and encouraged that I go, so I did.\nI consider Hong Kong an extremely diverse city, and I could have picked a place surrounded by Europeans and Americans to live. But even then, some classmates and I found a neighborhood that was relatively less expensive and near school. Sai Ying Pun was very Chinese populated and also on a steep hill- I'd often get to and from public transit by taking escalators up and down local fish markets. I liked the neighborhood so much that I remained after graduating, finding a one-bedroom apartment near the top of the hill and commuting downtown for work.\nI love Hong Kong, and for a long time, I called it a second home until\nI realized I couldn't afford to go back. I miss it, and likewise, I miss train trips to Guangzhou, Shenzhen, Chengdu, Harbin and Beijing. That life offered such extreme variation in experience and culture, something I often took for granted and struggled to adjust to. Ultimately, though, a piece of me missed home too much, and I returned after three years abroad.\nA family friend offered me a room in San Francisco. She lived on 18th and Noriega, and immediately I felt at home with a Chinese dessert shop a block away. I moved a couple times in the next year, first down the street to 35th Avenue, then over to \"mid\" Sunset near 18th and Lincoln, where I have lived in the same apartment the last 10 years. I get my hair cut at \"Salon de Hong Kong\" (2100 Irving St.), and my favorite sandwiches in The City come from an Asian- owned \"Uncle Benny's Donut and Bagel.\" (2049 Irving St.)\nI mentioned my journey the other day to my hairdresser, and she told me that after living in Sunset 28 years, \"I don't feel like I live in America.\" I feel the same many days, I told her, but also that I had considered moving elsewhere in light of the pandemic, and because rents finally began falling. Still, it was hard to consider leaving somewhere that I'd become so comfortable living in, I said. \"Maybe this is home for you,\" she told me. And it is.\nWesterners are often fascinated and surprised by my journey to San Francisco- it isn't every day a Northern California suburban bumpkin moves halfway around the world just because. More often though, the comment is more incredulity: \"Wow, China, what was that like?\" followed by a stream of racially distasteful commentary about Chinese people's supposed lack of manners or hygiene. These are broad-based remarks that many people fully believe about a country they have often never visited, and who may not have any close Asian friends.\nChina in particular is almost comical to me in the way people routinely just-and let's not mince words-bash it. Certain friends imitate Chinese accents, while others talk about how cheap they are, and for at least several years if not more, there's this weird attack on Chinese- and overseas-made products for not being locally made or high quality. I say \"weird\" because many people around me know what Foxconn is and that their phones and other devices come from a factory in Shenzhen. And likewise, almost everyone I know has an Amazon Prime account. I realize the other word is \"hypocritical.\"\nI could go on, and it's hard to avoid at least mentioning the gay community and its persistent so-called \"preferences\" that exclude some minorities-and especially Asian men-when they date and hook up. I could write 10,000 words on so many topics said just now, but suffice to say on this one: I think preferences are real. I also think you can leave race out when expressing them, and you can thoughtfully consider what influenced you to have those preferences in the first place.\n\"Stop Asian Hate\" is a movement we're talking about now because six Asian women were gunned down in an Atlanta spa. The circumstances are"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196163, Requested 7139. Please try again in 990ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196163, Requested 7139. Please try again in 990ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "ISLAMABAD, March 17 -- Eight people, six of whom were Asian women, were killed in shootings at three\ndifferent spas in the US state of Georgia on Tuesday, with a 21-yearold man in custody on suspicion of staging\nall three attacks, police said.\nThe shootings came with many Asian-Americans already on edge following a recent spike in hate crimes against\nthe community and triggered immediate fears that Asian-run businesses may have been deliberately targeted.\nFour of the victims were killed at Young's Asian Massage near Acworth, a suburb of Georgia's capital city\nAtlanta, the Atlanta Journal- Constitution reported.\nCaptain Jay Baker of the Cherokee County sheriff's office told the paper the victims were two Asian women, a\nwhite woman, and a white man, while a Hispanic man was wounded.\nThe Atlanta police department separately confirmed that four women were found dead at two business\nestablishments in northeast Atlanta, identified as the Gold Massage Spa and Aroma Therapy spa.\nPolice told the Atlanta Journal- Constitution that all four Atlanta victims were Asian women.\nSouth Korea's Yonhap News Agency reported the country's foreign ministry had confirmed that four of the\nvictims were of Korean descent.\nAuthorities have identified Robert Aaron Long as a suspect in all three shootings.\nBased on the pattern of surveillance video from the shooting scenes, Atlanta police spokesman Sergeant John\nChafee told AFP: \"It is extremely likely our suspect is the same as Cherokee County's, who is in custody.\n\"We are working closely with them to confirm with certainty our cases are related,\" he added.\nLong was taken into custody after a \"brief pursuit\" about 240 kilometres from Atlanta, according to a statement\nby the Georgia Department of Safety on Facebook.\nDescribing the scene in northeast Atlanta, the city police department said: \"Upon arrival, officers located three\nfemales deceased inside the location from apparent gunshot wounds.\" While on the scene, officers were advised\nof shots fired across the street, where they found a fourth female victim.\nAdriana Mejia, niece of one of the victims, said the family was \"devastated\" after her uncle was shot and that\nthey were praying for his recovery.\n\"We never know when we're at the wrong place at the wrong time because this was so all of a sudden,\" she\nsaid.\nThe Federal Bureau of Investigation was assisting in the investigation, a spokesman told AFP.\nMarginalised minorities The shootings come as reports of attacks against Asian-Americans, primarily elders,\nhave spiked in recent months - fueled during the Covid-19 pandemic, activists believe, by talk of the \"Chinese\nvirus\" by former President Donald Trump and others.\nNews of the shootings came just hours after the release of a report by the advocacy group, Stop AAPI Hate,\nsuggested a marked increase in hate crimes against Asian-Americans - with women disproportionately affected."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194846, Requested 7597. Please try again in 732ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194846, Requested 7597. Please try again in 732ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "\"I love downtown. It's a beautiful place to walk around,\" said Kay Kim, a longtime resident of Savannah, \"but now I'm scared to go.\"\nIn all 38 years of living in Savannah, Georgia, Kim, a Korean American, has not felt this kind of hypervigilance until now. With the rise of Anti-Asian violence across the U.S. and, most recently, the shooting deaths of eight people in the Atlanta area, six of whom were Asian women, she says she and others can't be too cautious.\n\"Even in Savannah you never know,\" said Kim. After hearing of the shooting incident Kim texted her children, \"be careful wherever you go, it's a scary world now.\"\nBut hate and discrimination have always lurked in the shadows and, throughout specific moments in history, bubbled up to the surface. Lawmakers in the past week have echoed the same sentiment, reminding people that this is nothing new.\n\"It's senseless,\" said Kim -- a word she falls back on repeatedly.\nFor many in the Asian community, making sense of this violence and the motives behind each incident feels futile. The rise in violence against Asians has generally been attributed to racist rhetoric surrounding the pandemic, but not all crimes have been labeled as hate crimes.\nThat includes the shooting deaths in Atlanta by the suspect Robert Aaron Long, a 21-year-old white man who told police his sex addiction compelled him to attack two spas that he frequented. Police have not yet determined that Long \"specifically targeted\" his victims, but experts say it is hard to separate the crime from race.\nAuthorities say the suspect opened fire at Young's Asian Massage in Acworth, Georgia on Tuesday evening, killing four people and injuring a fifth, before driving 30 miles into Atlanta and killing four more people at two businesses, Gold Spa and Aromatherapy Spa. Long was arrested about 150 miles south of Atlanta. Police said he was heading to Florida and intended to carry out more shootings at spas there.\nOne thing is certain: Many Asian Americans are left with a feeling of vulnerability, a feeling that increases each time someone of Asian descent is assaulted on the streets or killed.\nEach story is a harbinger of what could come.\nKim, who attends a local Asian church in Savannah, says her whole congregation is scared. In conversation, she oscillates between feeling anxiety about the recent violent events, but also faith that the world is still beautiful.\n\"It doesn't have to be this way,\" she says. \"It's a beautiful America, a blessed country. We shouldn't ruin it with these kinds of acts.\"\n\\ There's a Clash\n\"You do know what it's like to be a misfit,\" says an Asian American woman and Savannah business owner whose concern for her safety is so great that she did not want to be identified for this story.\nShe was born in and grew up in the U.S. and has always recognized a disconnect between her ethnic identity and her American one. She recalls moments from childhood when no one would want to sit next to her or the other Asian kids on the school bus.\n\"It happened every day,\" she reminisces, \"And you hear that all the time: 'You don't belong. You're from another country.'\"\nIt's been decades since that time in her life and she now boasts of a multicultural family full of love and chatter. Between her family and friends, there's a representative from every country, she jokes. \"It would always be just like, the world was in the living room,\" she says.\nBut even so, there are moments when she feels hostility from this world again.\nShe rattles off all the broadcast news channels she watches. She remembers seeing the coverage on Vicha Ratanapakdee in late January, a Thai man who was shoved to the ground in San Francisco and later died from his injuries.\n\"It's kind of a little bit of that all over again,\" she says, \"there's a clash within myself and I see the clash outside in the world now.\"\n\\ Endless Patience\nAn Nguyen, a retired respiratory therapist and Vietnam War veteran, settled in Savannah in the mid 70s. He now runs the Cha Ct Tuong Temple in Garden City, which sits on acres of green space. It's a stark difference from their two-bedroom cottage 20 years ago and Nguyen is proud of how far he has come.\nAs Nguyen lists off decades of accomplishments, it's clear he's motivated by wearing several hats in the community, working with church leaders in Garden City and mobilizing his neighbors to fill out the 2020 census. He then starts on another list of all the places he's lived and visited: San Antonio, Dallas, Washington D.C., San Francisco. But when asked why he ended up in Savannah, Nguyen says it's because it reminds him of his distant home.\n\"The weather is almost like Vietnam,\" he says. \"The river, the water, the ocean.\"\nBut most of all, Nguyen says it's because of the people.\n\"When I came here, I don't have anything after the war. And they helped me...hold my hand, guide me on what to do,\" he said.\nNguyen believes he has more friends than enemies. In the midst of the country's current strife, he urges unity.\n\"We are in the United States of America,\" he said. \"We need to live together, work together.\" It's why he and so many other immigrants come to this country in the first place, he said. That same hope, even in the face of violent racism, holds together their vision of what this country should be, but isn't.\n\"Every day I think I make people smile,\" says Nguyen.\nWhen people accuse him of bringing COVID-19 to America, he says he corrects them with a smile too. \"I'm Vietnamese. My life's work was working in healthcare."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193220, Requested 7470. Please try again in 207ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193220, Requested 7470. Please try again in 207ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "As the COVID-19 pandemic spread across the state and the country last year, the number of anti-Asian bias incidents \nwhich includes shunning, racial slurs and physical attacks  went up by nearly 75% within the past year, data released by\nthe New Jersey State Police show.\nCOVID-19's origin in Wuhan, China, has caused a significant backlash against Asian Americans across the United States for\nthe past year and New Jersey has seen its fair share of discrimination toward Asian Americans, said U.S. Rep. Andy Kim, an\nAsian American Democrat representing New Jersey's 3rd Congressional District.\nExperts say the COVID-19 pandemic did not start this new trend of discrimination toward the Asian American community,\nbut rather gave people an \"excuse\" to target them and highlights the long history of bigotry against them.\n\"I believe those statistics are being under-reported and that discrimination against Asian Americans in our country existed\nfar before COVID-19 and it will last after COVID-19,\" Kim said.\nAnti-Asian bias reports, which involve Asian or Pacific Islander individuals, jumped 74%, from 39 in 2019 to 68 in 2020\nacross the state, according to preliminary data compiled from the New Jersey State Police. However, these statistics are\nbased off what is only reported to police, as experts and officials believe there are more cases that go underreported.\n\"I am terrified,\" said Nina Gao, president of the Asian American Alliance of South Jersey and a Shanghai native who lives in\nCherry Hill. \"I think a lot of my friends feel the same way, you know, is this safe anymore? Yes, a lot of these attacks you see\nhappening in the city, but if it doesn't stop, is it going to happen tomorrow or a year down the road in the suburbs?\"\nHere is a breakdown of reported bias incidents against Asian Americans by county across New Jersey:\nBergen: 14\nBurlington: 1\nCamden: 2\nEssex: 1\nGloucester: 1\nHudson: 4\nMercer: 7\nMiddlesex: 14\nMonmouth: 10\nMorris: 1\nPassaic: 2\nSomerset: 5\nSussex: 1\nUnion: 3\nWarren: 1\nAtlantic, Cape May, Cumberland, Hunterdon, Ocean, and Salem counties did not see any incidents of bias against Asian\nAmericans over the past year, according to New Jersey State Police. The report did not specify type of incident by county.\nAsian Americans reside in all 21 counties and in every part of the state. There are approximately 960,000 Asian American\nresidents, or 10% of the total New Jersey population.\nThe number of reported incidents seem to track by population: In Middlesex County, about 25% of the total county\npopulation is Asian American as of 2019, according to the U.S. Census Bureau. Bergen has an Asian population of 16%,\nwhile Hudson, Somerset, Mercer and Monmouth have populations of more than 10%.\nAcross New Jersey, bias incidents among all groups  not just Asian American and Pacific Islanders  involving aggravated\nassault rose by 18% from 2019 to 2020. Incidents involving a tactic of intimidation rose by 7.5%, while terroristic threats\nskyrocketed by more than 103% from 32 reports in 2019 to 65 made in 2020.\nThe State Police report also detailed an increase in incidents targeting Black, Latino and LGBTQ communities, showing that\nincidents of harassment and discrimination rose for the second straight year. In 2020, New Jersey recorded 1,441 bias\nincidents, which was the highest total ever recorded and a 45% increase from 2019's total.\nBut nothing stood out more than incidents involving harassment, as New Jersey saw an increase of more than 72% in this\ncategory  421 in 2019 and then up to 726 incidents in 2020. Additionally, \"other\" forms of bias incidents, which includes\ndesecration of venerated objects, significantly rose by more than 278%, from 34 to 129 this past year.\nFor all race groups across New Jersey, simple assaults fell by about 4%, while bias incidents involving destruction, damage or\nvandalism to one's property fell by 3.5%, according to the annual report.\nStop Asian American Pacific Islander Hate (Stop AAPI Hate), a nonprofit organization in California, recorded 3,795 incidents\nof anti-Asian discrimination across the United States  a 26% increase from the prior year's 2,808 incidents. Some 42% of\nthe incidents were reported by Chinese Americans.\nNew York, New Jersey and Pennsylvania were all in the top 10 for most reported incidents, the group's data showed.\nBetween March 19, 2020, through Feb. 28 of this year, shows that more than 500 incidents took place so far.\nBelow is a breakdown of the top 10 states with highest number of bias incidents against Asian Americans over the past year,\naccording to Stop AAPI Hate's report:\nCalifornia: 1,691\nNew York: 517\nWashington: 158\nTexas: 103\nPennsylvania: 97\nMassachusetts: 96\nIllinois: 92\nFlorida: 59\nNew Jersey: 59\nMaryland: 51\nThe most common types of discrimination involved harassment and shunning, which made up 68% and 20.5% of the group's\nreports.\nPhysical assault was the third-most common category, making up 11% of the total incidents across the country. A quarter of\nthe incidents"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197756, Requested 7745. Please try again in 1.65s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197756, Requested 7745. Please try again in 1.65s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "WASHINGTON  One in 4 Americans including nearly half of Asian Americans in recent weeks have seen someone blame Asian people for the coronavirus epidemic a new USA TODAY/Ipsos Poll finds.\nThe nationwide survey was taken Thursday and Friday in the wake of last week's mass shooting in Georgia that killed eight people six of them women of Asian descent. Reports across the country of physical assaults and verbal abuse against Asian Americans have jumped during the yearlong pandemic.\n\"My friend went to the supermarket and got bullied\" said Pong Rattanakosum 45 a health care worker from Los Angeles and an American of Thai descent who was polled. When he heard about the shooting in Georgia \"I felt like anger and also anxious\" he said in a follow-up interview.\nWhile a 57% majority of Americans describe the coronavirus pandemic as a natural disaster, 43% say they believe a particular organization or people is responsible. In response to an open-ended question, most in that group specified China, comprising nearly 1 in 4 of the total survey. \nThe online poll of 1195 adults has a credibility interval of plus or minus 3.2 percentage points.\n\"China released it so the buck has to stop there\" said Joanne Von Prisk 78 a retired customer service manager from Sun City, Arizona who is white. \"I don't know that it was accidental or it was intentional. It should have been contained and dealt with there.\"\nThere were sharp partisan differences in perceptions.\nPartisan racial differences\nRepublicans were more than twice as likely as Democrats to hold a specific group or organization responsible for the pandemic. And Democrats were almost twice as likely as Republicans to report having witnessed Asian people being blamed for the pandemic in the past month.\nAmong the 25% of those surveyed who said they had seen someone blaming Asian people for the pandemic race and ethnicity were a divide as well: 18% of white Americans, 34% of Hispanic Americans, 40% of Black Americans and 46% of Asian Americans said they had witnessed such incidents.\n\"A year into the coronavirus pandemic and Americans continue to blame China for the outbreak particularly Republicans\" said Cliff Young, president of Ipsos.\nAn academic study published in the American Journal of Public Health last week showed the effect of then-President Donald Trump labeling the pandemic the \"Chinese virus\" during its early weeks. When he first used the hashtag #chinesevirus on Twitter in March 2020 the number of people using it exploded and they were much more likely to use explicitly anti-Asian hashtags than those who used #covid19 in tweets.\n\"The previous administration you know they would openly say 'Chinese virus'\" said Shirin Bhasin 44 of Milwaukee an Indian American who works in human resources. She was among those surveyed. \"So they have sort of instilled in people that it's to do with China it's to do with the Chinese people.\"\nReports of violence have exposed bias against Asian Americans that has long existed but not always been recognized she said. \"It has brought more awareness and people who were just living under the rock thinking that there was no racism  some of my colleagues their eyes have opened up.\"\nBut Bobby Colvin 74 a mango grower from Pahokee Florida who is white accuses the news media of hyping reports of attacks on Asian Americans. \"I haven't seen it\" he said. \"I don't believe it at all.\"\nA nonprofit group called Stop AAPI Hate reported nearly 3800 incidents of anti-Asian hate in about a year since the pandemic emerged including verbal harassment, physical assaults and civil rights violations. \"President Trump's insistence on the term 'Chinese virus' clearly stigmatized Asians as disease-carriers\" founder Russell Jeung a professor of Asian American studies at San Francisco State University told USA TODAY. \"Such words directed fear and anger towards Asian Americans and has led to the violence and deaths that we're grieving over.\"\nChanges from April 2020 to now\nIn an Ipsos poll last April one-third of those surveyed reported having seen Asians being blamed for the pandemic 7 points higher than in the new survey. But the change in attitudes over the past year about coming into close contact with someone of Asian descent is complicated. The number who express fear about being near someone of Asian ancestry is relatively unchanged at 21%. But the percentage concerned about being close to an Asian American who isn't wearing a mask or other protective gear rose 8 points to a 54% majority. There was no such significant shift in views in general about being close to people not wearing protective gear."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193628, Requested 7158. Please try again in 235ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193628, Requested 7158. Please try again in 235ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Main Street Patrol volunteers in Flushing Queens on March 26 2021. Alarmed and outraged by a surge in racist violence citizens band together to make city streets safer. (Stephanie Keith/The New York Times)NEW YORK - One recent Saturday afternoon Teresa Ting passed out purple medical masks - the same color as the 7 train that connects Flushing Queens to Manhattan - among a group of 14 volunteers gathered on the steps of the massive Flushing post office on Main Street. A spreadsheet on Ting's phone detailed the day's watch routes in the neighborhood. One team would monitor Main Street another would take Prince Street and 37th Avenue. A third group would patrol the blocks between Union Street and College Point Boulevard on Roosevelt Avenue where a man shoved an Asian American woman to the ground outside a bakery in February. The attack galvanized Ting to form Main Street Patrol a volunteer safety group that casts a watchful eye over downtown Flushing equipped with little more than safe intervention strategies and a walkie-talkie app on their phones. Ting an actress and Queens native works with a rotating cast of volunteers from different parts of the city and walks of life all driven by outrage at unprovoked assaults against Asian Americans and inaction by bystanders. \"This past year has taken a toll\" Ting said. \"Something needed to be done. As it happens that Saturday was her 30th birthday her second celebration in a pandemic but her first in her new role safeguarding the community. Asian Americans are grappling with fear rage and anxiety brought on by attacks against people of Asian descent particularly older men and women. In an especially egregious episode a little more than a week ago a 65-year-old Filipino woman was brutally assaulted in broad daylight outside a luxury apartment building in midtown Manhattan. Two lobby staff members who witnessed the assault but failed to intervene have been fired. After each case Asian Americans express shock and pain but point to a long history of bigotry that has been magnified during the pandemic. Advocates also attribute the current climate to former President Donald Trump and his use of damning and inaccurate phrases like \"the China virus.\" The New York Police Department has received 35 reports of anti-Asian hate crimes already this year compared with 28 in all of 2020 and only three in 2019. Officials say the data fails to paint a full picture as many incidents go unreported or are not categorized as hate crimes. As the attacks mount Asian Americans and their allies are hoping to make their streets safer. In addition to Main Street Patrol whose volunteers take weekend shifts in downtown Flushing at least three other similar groups have formed during the pandemic. A series of assaults against women in East Williamsburg Brooklyn in January led to SafeWalks NYC which offers escorts at train stations in various neighborhoods including Manhattan's Chinatown. And in February last year Karlin Chan a community activist formed the Chinatown Block Watch whose participants keep watch over an area that is the birthplace of Chinese culture in New York City. Chan who is in his mid-60s is aware of the risks he faces as he roves the neighborhood. \"With my beautiful crop of gray hair there is a target on my back\" he said. But the urgency of the moment attracted Chan who experienced racism as a child growing up in Chinatown in the early 1960s. He leads volunteers along Mott Street and Grand Street as well as the blocks under the Manhattan Bridge to check in with the neighborhood's residents and business owners. Civilian patrols have a long and mixed history in New York City. Most notable is the Guardian Angels founded in the late 1970s when economic blight helped fuel a wave of violent crime across the city. Known for their red berets and bullish presence the Guardian Angels emerged as a crime-fighting patrol on the subways acquiring a kind of hero status while simultaneously drawing the ire of politicians and transit officials for their vigilante justice. After a prolonged period of low crime in New York City the Guardian Angels have reemerged with the sharp increase that has coincided with the pandemic and they have spent time on the ground in Chinatown this past year. The group's founder Curtis Sliwa announced in March that he was running as a Republican in the city's 2021 mayoral race. But Main Street is quick to stress that it focuses on intervention rather than deterrence.\"We want to remain a little more covert so we blend in more and we can actually run into catching more situations rather than suppressing them\" Ting said. The one incident the group has run into got hairy after a patroller started recording two young men who were overheard making racist remarks and threatening to harm an older man. The men became agitated and knocked the patroller's phone out of his hand eventually fleeing. The group later revised some of its protocols stressing that volunteers should record episodes only if they can do so from a safe distance. While civilian patrols do not have official approval of the Police Department they work in concert with officers by being an extra set of eyes and ears\" said Capt. Paul J. Zangrilli in Chinatown's Fifth Precinct. Earlier this year for example when teams of pickpockets were targeting older women Chan's Chinatown Block Watch provided the police with information that ultimately led to the apprehension of both groups. \"They have added to our success in their heightened vigilance and heightened awareness out in the community\" Zangrilli said. As Main Street Patrol was winding down its Saturday tour in Flushing another group was assembling in Confucius Plaza in Manhattan. Ricky Yang stood behind a folding table with a sign that read \"Protect Chinatown\" the group he established in February as a chaperone service which got off to a slow start. Now the group was setting out on foot patrols in part to raise awareness about its services and to monitor the streets. As volunteers mobilized one older man of Asian descent stopped to ask a question not about safety"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193045, Requested 8028. Please try again in 321ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193045, Requested 8028. Please try again in 321ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Texas: 103\nPennsylvania: 97\nMassachusetts: 96\nIllinois: 92\nFlorida: 59\nNew Jersey: 59\nMaryland: 51\nThe most common types of discrimination involved harassment and shunning, which made up 68% and 20.5% of the group's\nreports.\nPhysical assault was the third-most common category, making up 11% of the total incidents across the country. A quarter of\nthe incidents took place on public streets, while a third of the incidents occurred at businesses.\n\"I think the (COVID-19) pandemic and, the discrimination that we've seen, has been pouring gasoline on the fire over the\nlast year,\" Kim said. \"It's disturbing and I truly also believe that these numbers do not reflect the totality of what is going\non.\"\nCOVID-19's impact\nAsian history experts say the issue of harassment is nothing new for the Asian community, as a majority have dealt with\ndiscrimination for decades. But there have been several high-profile incidents, such as the recent shootings in Atlanta-area\nspas on March 16, in which six of the eight victims were Asian women, that have played a large part in highlighting the\nissue.\nLast March, a New York City man attacked a 65-year-old Asian American woman in broad daylight and hurled anti-Asian\ninsults at her as she walked down a midtown Manhattan street, according to New York City police.\nThe woman was later hospitalized with serious injuries after the attacker punched, kicked and stomped on her as staff and\nbystanders did not intervene the incident. NYPD called the incident \"a hate crime assault,\" which later prompted a large\noutcry amongst the Asian community for justice and change.\nEarlier this month, Jonathan Russo, 27, was charged with multiple counts of assault and harassment, both as a hate crime,\nin connection with pushing a 64-year-old Asian American woman on a sidewalk and knocking her down in New York,\nassaulting a 32-year-old Asian American woman by grabbing her hair, and shoving a 77-year-old Asian American man to the\nground.\nRichard Chen, 40, a web producer and a board member for the Asian American Action Fund, said anti-Asian discrimination is\nnothing new across the tristate area, saying that even though he would often see people staring at him when the pandemic\nfirst started, he believes many others have had it far worse in years past.\n\"Discrimination toward (Asians) has always been there, especially in New York,\" said Chen, a resident from Branchburg in\nSomerset County. \"Sure, we usually get those typical stares because of where the virus started, but those who have been\nharassed or assaulted because of it is completely unacceptable and is likely going to continue to be honest.\"\nJennifer Ho, a professor of Asian American studies at University Colorado of Boulder and the director of Humanities and Arts,\nsaid this topic is the story of the moment, and that the pandemic has only given people another reason to attack this\ncommunity.\nHo emphasized the recent violent attacks at Asian American seniors and women throughout the first few months of this year,\nspecifically videos that have surfaced across the mainstream media, has likely led to the increase of reports recorded by the\nStop AAPI Hate nonprofit organization.\n\"I want to say that with the violence we're seeing now and with all the racism and the videos that came out last year against\nAfrican Americans, the reports are increasing because of the national attention it is receiving,\" Ho said. \"Prior to this year,\nwhen was the last time you honestly saw this much attention on racism against Asian Americans?\"\nAdditionally, the report released by the AAPI anti-hate organization also showed that Asian women made up 68% of the total\nincidents, compared with men, who made up 29% of all incidents recorded.\nIn the Stop AAPI Hate report, victims of anti-Asian harassment sent in their stories to the organization, which included one\nFilipino American woman reporting that while in a Washington, D.C., metro station with her boyfriend, an unknown man\nshouted at her \"Chinese b----,\" then proceeded to cough at the couple and physically threaten them.\nIt didn't begin with the pandemic\nIn 1871 in Los Angeles, around 500 white and Hispanic people murdered 19 Chinese residents in the so-called Chinese\nmassacre  a buildup of a growing anti-Asian culture that played a part in the Chinese Exclusion Act of 1882, which banned\nthe immigration of Chinese laborers. It was the nation's first set of immigration laws to ban individuals based on race.\nJapan's bombing on Pearl Harbor in 1941 resulted in the internment of most Japanese Americans across the West Coast to\nbarracks and camps in remote areas during World War II, with the federal government deeming them a possibly disloyal\nsecurity risk. Over 127,000 Japanese Americans were placed in camps in the United States during the war and over 1,800\ndied in the camps.\nJason Oliver-Chang, a professor at the University of Connecticut whose research focuses on Asian American studies and\nAmerican immigration history, said Asian Americans were used as a racial wedge after World War II and the Vietnam War.\n\"Japanese Americans were used as a racial wedge in the '40s and '50s, as with an American allied victory in World War II,\nthere was then a need to claim victory at home as well,\" Oliver-Chang said. \"People at this time were arguing to fight\nfascism abroad and fascism at home and in some respects, people looked to the recovery of Japanese Americans from the\nincarceration experience during World War II, as evidence of the United States overcoming the color line.\"\nOliver-Chang said even"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197673, Requested 7028. Please try again in 1.41s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197673, Requested 7028. Please try again in 1.41s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "The recent horrendous fatal shooting of eight people - six of them of Asian descent - was yet another senseless killing by an American man who hated women and, in particular, Asian women. This horrific crime was stretched to nearly one hour at three different Atlanta adult parlours, resulting in targeted hatred toward mostly Asian \nwomen. \nThe audacious Cherokee County Sheriff's Office's Capt. Jay Baker's synopsis of the killer's actions was: \"He was pretty much fed up and kind of at the end of his rope. Yesterday was a really bad day for him, and this is what he did.\" The loss of eight innocent lives relegated to this evil criminal carnage to \"a bad day\" is perturbing. \nAs painful as it was to hear these comments from the sheriff, internet sleuths and journalists found Baker's Facebook posts promoting shirts that called COVID-19 an \"IMPORTED VIRUS FROM CHY-NA.\" \nWith the rhetoric coming from the U.S. during this COVID era, there has been an increase in Asian hate incidents in Canada. According to Project 1907, a grassroots group made up of Asian women that collects data on incidents of racism, hate, and violence experienced by the Asian diaspora in Canada: \nCanada has a higher number of anti-Asian racism reports per Asian capita than the United States; \nWomen are impacted the most, reporting six per cent of all incidents; \nIn B.C., women are even more disproportionately impacted, accounting for nearly 70 per cent of all reported incidents; \nVerbal abuse and harassment are widespread, occurring in 65 per cent of all reported incidents; \nNearly 30 per cent of reported incidents are assault, including targeted coughing, spitting and physical attacks and violence. \nAccording to a report released by the Vancouver Police Department, there has been a 717 per cent increase in anti-Asian hate crimes from 2019 to 2020. On April 3, Fight COVID Racism reported 980 incidents of anti-Asian hate crimes across Canada during the pandemic. \nAccording to 2016 Statistics Canada, approximately 1,400 people of Asian descent live in Peterborough, and during this COVID pandemic, they have been feeling the effects of anti-Asian racism. Their racist incidents have been more subtle, a microaggression, \"feeling of being watched\" or \"getting a look,\" the feeling of \"what are you doing here,\" or \"you don't belong here.\" \nDuring this COVID-19 era, members of an Asian family who have lived in Peterborough for a few years were celebrating their child's birthday party. For the first time since living in Peterborough, a stranger yelled, \"Go back to China.\" \nNot all Asians are from China, and they are not a monolithic ethnic group. \nThe diversity among Asians is vast and varied like any other ethnic group. The ignorance of people, allied with the philosophy of Trumpism, has given rise to the anti-Asian racism. Their narrative may also include references to the Chinese Communist political system and Chinese oligarchy. These detrimental opinions are red herrings and should have no place within the context of anti-Asian racism. \nInstead, we need to stop this cycle of hate toward those who are different from ourselves. Stop the labelling of others, eliminate the negative stereotypes; stop the microaggressions! We need to come together in solidarity and break the exclusivity barriers to create a supportive, welcoming, caring society - one where we are all treated with dignity, respect, and equity as human beings. \nWe need to move beyond our statements, Black Lives Matter, Stop Asian Hate; we need to take a stand and be bold. These actions can range from checking in with your Asian friends on their well-being. Are they feeling safe in our community? Listening to other racialized groups, hearing their narratives, educating yourselves, being bold and taking a stand against hate. \nEven small acts of kindness give us hope, energy, and the opportunity for conversations. As Peterboroughians, reach out and make this community a place that treats all citizens regardless of a person's demographic: race, gender, religion, gender orientation, ethnic group, social-economic status, or ability/disability as one with equity. \nHere's how you can make a difference. \nVisit Community Race Relations Committee of Peterborough at racerelationspeterborough.org. \nReport hate crimes in Peterborough: phone 705-876-1122 or online at peterborough police.com. \nDonate: \nThe Canadian Anti-Racism Education and Research Society Stop is the best place to learn how to stop racism. Visit stopracism.ca. \nCanadian Council of Churches Anti-Racism Network: councilofchurches.ca. \nThe Canadian Race Relations Foundation (CRRF): crrf-fcrr.ca/en/about/join-our-team. \nEducate: \nProject 1907, Racism Incident Reporting Centre's Elimin8hate reporting centre collects data on incidents of racism, hate and violence experienced by the Asian diaspora in Canada. See project1907.org. \nFight COVID racism: \nThis platform aims to allow individuals to share their experiences of racism and will allow us to track and record instances of anti-Asian racism during COVID-19. You can also find resources and support that you can access. Visit covidracism.ca. \nCharmaine Magumbe is the chair of the Community Race Relations Committee. Learn more at \nracerelationspeterborough.org. \nCAPTION: \nAn anti-Asian-hate rally at Nathan Phillip Square drew hundreds outside of Toronto City Hall on March 28. People of Asian descent living in Peterborough have been feeling the effects of anti-Asian racism during the pandemic, Charmaine Magumbe writes.Steve Russell Toronto Star file photo \nCREDIT: Charmaine Magumbe Special to"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194535, Requested 7468. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194535, Requested 7468. Please try again in 600ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "It's a matter of human nature, I suppose, to choose to place blame on someone or something for an agonizing, catastrophic event. We may not always have all the correct information, and we may not even be accurate as to what was behind it, but I guess that really doesn't matter, as long as someone pays. \nAll right, so you're angry, join the club! I totally believe 100% of all Americans are angry regarding how the pandemic affected them for one reason or another. You have been sick, your family and friends have been sick, you were laid off, your business went under, or regrettably someone you loved died. All good reasons to be angry. \nSo who pays for your pain? Our nation is kind of at that point right now, where many people have picked out someone to vilify for COVID-19. And believe me, there are many targets to choose from. Of course, there's Donald Trump, who was president when the pandemic began. Then we can add former Vice President Mike Pence, who was Trump's pandemic point man. I'm quite sure most Americans have had enough of Dr. Fauci, who can't help changing his mind on a daily basis. Of course, President Biden's mask mandate was frustrating, and his COVID-19 relief package price was so astronomical, it may have actually crippled the restart of America's recovery. \nNeedless to say, the daily updates and recommendations by the Centers for Disease Control and Prevention were more confusing than Dr. Fauci's daily diatribe. And if you're a Minnesotan, Gov. Walz's lockdown, mandates and restart program would have more than tested the patience of Job. \nOr maybe we can blame the Chinese. After all, isn't China where the virus came from? But what if we don't know which Asian people we come in contact with are actually Chinese? After all, don't all Asian people look alike? Shouldn't all of them be punished for this indiscriminate virus? \nRegrettably, that's exactly what's happening across the world and in our own backyards. Asian people have become targets of opportunity for many angry people in America. NBC News reported 3,800 hate incidents between March 19, 2020, and Feb. 28, 2021. That number is up from 2,600 incidents reported in that time period a \nyear ago. \nAn Asian American Pacific Islander advocacy organization reported 68% of the assaults happened to women, while 29% of victims were men, and 3% listed themselves as transgender or nonbinary. \nThese attacks include being spat upon, verbally abused, pushed, shoved, punched, or thrown to the ground, or refusing to give them service in a store. However, some people have taken it to a sadistic level. The BBC reports a Chinese woman was slapped in the face before being set on fire. An elderly Thai man was killed when he was violently thrown to the ground. \nSorrily this is not the first time Asian people have been singled out for oppression and violence in the United States. After the attack on Pearl Harbor in 1941, the government systematically rounded up people of Japanese descent, placing 120,000 of them in confinement camps for nearly four years, fearing they would attack America from within. \nWhat everyone needs to realize is that every Asian American in our country has also suffered through the ravages of COVID-19 just as we all have. If anyone can actually attempt to justify these abhorrent attacks, you truly need to look into your soul and figure out who you really are. \nGerry Feld writes about issues from a conservative perspective and is a published novelist."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193206, Requested 7866. Please try again in 321ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193206, Requested 7866. Please try again in 321ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "The Kenney administration turned Philadelphia into a national joke when it put a bunch of self-described college kids in charge of our first mass COVID-19 vaccination site. \nBut that was just the warmup act. \nCouncilperson \nMark Squilla and some 50 Italian American groups have provided a new punchline by suing Mayor Jim Kenney's administration in federal court, accusing it of discriminating against his Italian American constituents and asking the court to declare Italian Americans a protected class. \nMakes sense, given the rise of hate crimes directed at the group lately. Oh wait, that would be Asian Americans who have been increasingly and violently targeted since the pandemic began. As recently as Tuesday, a 64-yearold man of Asian descent was assaulted in the city's Chinatown neighborhood. This after a 27-year-old Asian woman, on Sunday, was hospitalized after a man walked up to her on a Chinatown sidewalk, hit her in the face, and walked away. \nBut who has time for pesky facts when there are Big Lies to reinforce about Columbus being a civil rights activist! Squilla insists some historians believe Columbus was kind to Native populations he encountered in the Caribbean. But some historians would be wrong, and all actual civil rights activists are rolling in their graves. To agree with Squilla & Co. is to ignore documented atrocities of looting, murder, and slavery. (I promise there are better Italians to pledge your undying love and devotion. Google them.) \nIf this were a few days earlier, I would have bet money it was a lame April Fools' joke. \nBut nope, they unfurled one ridiculous grievance after another, including that should they win, they want the city that would be you, my fellow taxpayers attorneys' fees for the plaintiffs. \nSo, let's see what we'd be paying for, shall we? \nto pay \nFor starters, they want Columbus Day to be reinstated after Kenney had the audacity to acknowledge the institutional racism and marginalization of Black, Indigenous, and other communities of color by renaming the city's holiday Indigenous Peoples' Day, and (adding insult to injury) recognizing the Juneteenth holiday commemorating the emancipation of enslaved people in the United States. \nBut where does that leave the poor genocidal colonizer? \nEspecially after, as evidence of trumped-up discrimination, they cited the city attempting to remove the Christopher Columbus statue from Marconi Plaza when it became ground zero for armed groups accused of assailing protesters and passersby. The lawsuit also brought up the unjust demotion of former First District Capt. Louis Campione after allegations that his officers defended white vigilantes during protests there last spring. There are some damning videos that sure seem to support this, but who are you gonna believe: them or your lying eyes? \nAnd then there's the city's perennial racist lightning rod: the statue of the late mayor, police commissioner, and resident police brutality fan boy Frank Rizzo, proponent of racially divisive language and policies his followers consistently insist have been misunderstood, twisted by woke mobs who hate Italians. (I mean, the guy had Black bodyguards, for crying out loud!) \nThere's a lot of that \"you got it all wrong ...\" mentality going around these days: Consider the Capitol insurrectionists scrambling to apologize now that they are realizing there might be actual consequences for unleashing the deadly carnage on the country in the name of Donald Trump's Big Lie of a stolen election. \nIt would be easy to write off so much of the madness, maybe enjoy a few chuckles at the latest insanity. But remember what happened last time we laughed off a clown? One of the more disturbing parts in the suit against the city is the allegation that it discriminated against Italian Americans by not including South Philadelphia's 19148 zip code in its list of prioritized neighborhoods for coronavirus vaccine distribution. It's one thing to pledge allegiance to white rage by worshiping at the foot of some problematic statues, but slinging this kind of misinformation isn't just irresponsible, it's dangerous. \nThe city has been prioritizing vaccine outreach to Black and brown residents because they are disproportionately getting sick and dying from COVID-19. That is a fact. Also a fact: 19148 is one of the most vaccinated parts of the city. But again, who cares about (real) systemic racism. \n \nThe truly pathetic thing about Squilla & Co.'s lawsuit is that Kenney and his administration do need to be held to account for a vaccine screw-up the partnership with Philly Fighting COVID and its huckster CEO, who not only had zero qualms botching the rollout of desperately needed vaccine, but also felt emboldened to mock and question Ala Stanford, a physician who founded and runs the Black Doctors COVID-19 Consortium, a nonprofit that offers testing and vaccination in underserved neighborhoods. \nBut then who needs public service when there's political theater. hubinas@inquirer.com \n215-854-5943 \nNotesFromHel"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195626, Requested 7587. Please try again in 963ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195626, Requested 7587. Please try again in 963ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "As the COVID-19 pandemic spread across the state and the country last year, the number of anti-Asian bias incidents \nwhich includes shunning, racial slurs and physical attacks  went up by nearly 75% within the past year, data released by\nthe New Jersey State Police show.\nCOVID-19's origin in Wuhan, China, has caused a significant backlash against Asian Americans across the United States for\nthe past year and New Jersey has seen its fair share of discrimination toward Asian Americans, said U.S. Rep. Andy Kim,\nan Asian American Democrat representing New Jersey's 3rd Congressional District.\nExperts say the COVID-19 pandemic did not start this new trend of discrimination toward the Asian American community,\nbut rather gave people an \"excuse\" to target them and highlights the long history of bigotry against them.\n\"I believe those statistics are being under-reported and that discrimination against Asian Americans in our country existed\nfar before COVID-19 and it will last after COVID-19,\" Kim said.\nAnti-Asian bias reports, which involve Asian or Pacific Islander individuals, jumped 74%, from 39 in 2019 to 68 in 2020\nacross the state, according to preliminary data compiled from the New Jersey State Police. However, these statistics are\nbased off what is only reported to police, as experts and officials believe there are more cases that go underreported.\n\"I am terrified,\" said Nina Gao, president of the Asian American Alliance of South Jersey and a Shanghai native who lives in\nCherry Hill. \"I think a lot of my friends feel the same way, you know, is this safe anymore? Yes, a lot of these attacks you\nsee happening in the city, but if it doesn't stop, is it going to happen tomorrow or a year down the road in the suburbs?\"\nHere is a breakdown of reported bias incidents against Asian Americans by county across New Jersey:\nBergen: 14\nBurlington: 1\nCamden: 2\nEssex: 1\nGloucester: 1\nHudson: 4\nMercer: 7\nMiddlesex: 14\nMonmouth: 10\nMorris: 1\nPassaic: 2\nSomerset: 5\nSussex: 1\nUnion: 3\nWarren: 1\nAtlantic, Cape May, Cumberland, Hunterdon, Ocean, and Salem counties did not see any incidents of bias against Asian\nAmericans over the past year, according to New Jersey State Police. The report did not specify type of incident by county.\nAsian Americans reside in all 21 counties and in every part of the state. There are approximately 960,000 Asian American\nresidents, or 10% of the total New Jersey population.\nThe number of reported incidents seem to track by population: In Middlesex County, about 25% of the total county\npopulation is Asian American as of 2019, according to the U.S. Census Bureau. Bergen has an Asian population of 16%,\nwhile Hudson, Somerset, Mercer and Monmouth have populations of more than 10%.\nAcross New Jersey, bias incidents among all groups  not just Asian American and Pacific Islanders  involving aggravated\nassault rose by 18% from 2019 to 2020. Incidents involving a tactic of intimidation rose by 7.5%, while terroristic threats\nskyrocketed by more than 103% from 32 reports in 2019 to 65 made in 2020.\nThe State Police report also detailed an increase in incidents targeting Black, Latino and LGBTQ communities, showing that\nincidents of harassment and discrimination rose for the second straight year. In 2020, New Jersey recorded 1,441 bias\nincidents, which was the highest total ever recorded and a 45% increase from 2019's total.\nBut nothing stood out more than incidents involving harassment, as New Jersey saw an increase of more than 72% in this\ncategory  421 in 2019 and then up to 726 incidents in 2020. Additionally, \"other\" forms of bias incidents, which includes\ndesecration of venerated objects, significantly rose by more than 278%, from 34 to 129 this past year.\nFor all race groups across New Jersey, simple assaults fell by about 4%, while bias incidents involving destruction, damage\nor vandalism to one's property fell by 3.5%, according to the annual report.\nStop Asian American Pacific Islander Hate (Stop AAPI Hate), a nonprofit organization in California, recorded 3,795 incidents\nof anti-Asian discrimination across the United States  a 26% increase from the prior year's 2,808 incidents. Some 42% of\nthe incidents were reported by Chinese Americans.\nNew York, New Jersey and Pennsylvania were all in the top 10 for most reported incidents, the group's data showed.\nBetween March 19, 2020, through Feb. 28 of this year, shows that more than 500 incidents took place so far.\nBelow is a breakdown of the top 10 states with highest number of bias incidents against Asian Americans over the past\nyear, according to Stop AAPI Hate's report:\nCalifornia: 1,691\nNew York: 517\nWashington: 158\nTexas: 103\nPennsylvania: 97\nMassachusetts: 96\nIllinois: 92\nFlorida: 59\nNew Jersey: 59\nMaryland: 51\nThe most common types of discrimination involved harassment and shunning, which made up 68% and 20.5% of the\ngroup's reports.\nPhysical assault was the third-most common category, making up 11% of the total incidents across the country. A quarter\nof the incidents took place on public"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198441, Requested 7845. Please try again in 1.885s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198441, Requested 7845. Please try again in 1.885s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "wedge in the '40s and '50s, as with an American allied victory in World War II,\nthere was then a need to claim victory at home as well,\" Oliver-Chang said. \"People at this time were arguing to fight\nfascism abroad and fascism at home and in some respects, people looked to the recovery of Japanese Americans from the\nincarceration experience during World War II, as evidence of the United States overcoming the color line.\"\nOliver-Chang said even though the Civil Rights Movement in the 1960s played a major step in overcoming the racial\nbarriers in the United States, Asian American voices still did not receive as much attention.\n\"I think it helps to explain some of the absence of Asian American voices during these time periods because Asian\nAmericans were drawing from the harsh treatment and atrocities happening in Japan, Vietnam and in Southeast Asia, to\ninform others of their own treatment in the United States,\" Oliver-Chang said.\nCongressman Kim, the son of Korean immigrants, said he has also had his fair share of experiences with racism and\ndiscrimination, but is more concerned about how much longer this issue will continue.\n\"There are a number of instances that really stick out, but I think what Asian Americans often feel is just this perpetual and\nconstant and rhythmic erosion of belonging that takes its toll,\" Kim said. \"I think it worries me and others, not just about\nhow we get through this moment and are we safe, but where are we heading as a country in this direction.\"\nWhat's being done\nExperts and advocates agree the rise in Asian American attacks in 2020 became more prevalent after President Donald\nTrump began to routinely refer to the COVID-19 pandemic as the \"China virus.\"\nIn January, President Joe Biden issued an executive order condemning the recent attacks against Asian Americans  and\nwithout naming them, criticized former federal officials who repeatedly referred to COVID-19 as the \"China virus\" or the\n\"Kung Flu.\" Many believe he was criticizing Trump and members of his administration.\nThe order calls for better data collection about hateful incidents, along with mandating federal agencies to fight \"racism,\nxenophobia, and intolerance\" directed at Asian Americans and Pacific Islanders.\n\"The federal government must recognize that it has played a role in furthering these xenophobic sentiments through the\nactions of political leaders, including references to the COVID-19 pandemic by the geographic location of its origin,\" Biden\nsaid in his order. \"Such statements have stoked unfounded fears and perpetuated stigma about Asian Americans and\nPacific Islanders and have contributed to increasing rates of bullying, harassment and hate crimes against AAPI persons.\"\nExperts and advocates agree addressing the root cause of the harassment and violence requires more education, more\nawareness of racism  not just focusing on one community but for all  and building political power for all minorities.Last March, a roundtable was hosted by New Jersey Attorney General Gurbir Grewal, Gov. Phil Murphy, Kim and other high-\nranking officials to discuss bias and hate crimes amid the shooting in Atlanta and attacks surfacing across the country.The Asian American Alliance in South Jersey will be hosting several marches and rallies in the coming weeks to raise\nawareness, share stories and help others understand the violence and discrimination is not just happening in the cities, but\nalso in the suburbs.\nIn North Jersey, hundreds of Asian Americans recently took to populated streets to spread awareness of hate against\nAsians, shedding cultural norms. Similar to the marches and rallies held by other activists this past year, protestors were\nseen carrying posters and signs that read \"Stop Asian Hate\" and \"Hate is a Virus.\"\nLast Thursday, the Senate passed with overwhelming bipartisan support an anti-hate crime bill, which is known as the\nCOVID-19 Hate Crimes Act, to address the drastic increase of violence and discrimination directed at Asian Americans\nduring the COVID-19 pandemic.\n\"We cannot let this slip by, we have to seize the moment when people are paying attention to make the kinds of changes\nthat will outlast the public attention,\" Oliver-Chang said.\n\"This is the moment when we need to build political power across Asian American communities, so that our communities\nknow what the states are for our invisibility and if they don't speak up or report on attacks, we risk having this history\nrepeated on us.\"\nHere are a few tips you can do to help fight discrimination against Asian Americans:\nReport any hate crimes to your local police or send tips to the FBI.\nSubmit tips to Stop AAPI's Hate's reporting center at www.stopaapihate.org/reportingincident, which has tracked\nthousands of incidents across the United States.\nYou can also go to Asian American Advancing Justice site at www.advancingjustice-aajc.org, where you have the\nopportunity to share your experience.\nReporter Mary Chao contributed to this report.\nJoshua Chung is the 9-5 breaking news and weather reporter. A lifelong Jersey Shore resident, he is a recent graduate of\nMichigan State University. Contact him at jchung@gannettnj.com, 917-703-9373 or on Twitter @Joshchunggg\nMore on mobile\nGet instant news updates on your phone or tablet. Download the free APP.com \"app\" for videos, photos and more about\nlocal crimes. Search \"Asbury Park Press\" in the app store."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197062, Requested 7656. Please try again in 1.415s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197062, Requested 7656. Please try again in 1.415s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "TEHRAN \n- \nAn American human rights activist says the Joe Biden administration is not capable of leading the U.S., describing his policies as dumsily too ideological which enrages the majority of the people. \n\"The Biden administration is neither capable of handling domestic or international issues, and his policies and those around him are clumsily too ideological on enraging the majority of the people with prevaricating promises, puerile policies, ineffective half-measures,\" Randy Short tells the Tehran Times. \nAmid hot discussion about the trial of Derek Chauvin, the officer who knelt on George Floyd's neck, many observers point to challenges that may threaten the future of the country like deep-rooted division in American society. \nDuring his election campaigns, President Joe Biden pledged to restore American leadership via reversing Trump's policies. However, some pundits are skeptical of Biden's competence to treat American society's chronic wounds. \n\"Biden is incapable of leading the United States,\" Short believes. \nFollowing is the text of the interview: \nQ: How do you see the rise of racist movements in the U.S., especially against Asian-Americans? \nA: I do not see a rise in racist movements in the United States. These groups have always been part and parcel of American life. Scores of millions of Americans primarily white or those that consider themselves to be near-white have had armed hate groups for decades. Regarding Asian Americans, there has been violence and discrimination against various Asian American ethnic groups; however, there are three things that must be kept in mind: (1) millions of Asians are embraced and assimilated as white and they are often very racist against all non-white people; (2) most Asians strive to be as white as possible which means adopting anti-Black and other racist behaviors and practices in addition to their own hatreds of people not in their clan/ethnic group, and (3) too often Asians seeking to be invisible and alien to Blacks or other non-white people or they are involved in illegal activity or are not lawfully in the United States and do not wish to risk being arrested, jailed, or deported. Therefore, for decades few Asians complain to the authorities about incidents of racial intolerance. \nAcross the United States, crime rates are soaring, and Asians are experiencing what other Americans are experiencing-a spike in robberies and violent crime. Another issue at hand that is not being addressed Asian businesses have been giving a commanding role in the economic life of Black and Brown communities, and they often function as an element of racist white power and economic exploitation. \nRacism in the media worldwide ignores the murders, assaults, and beatings that Asian Americans mete out to Black people, and as a result, the reporting has made the situation appear as if Black people are simply hurting Asians. This is neither true nor possible. Blacks are highly segregated from Asians all over America. Second, whites kill and assault Asians with more deadly results than anything that could be said of Blacks. Asians are about 7% of the U.S. population and are 1.4% of the reported victims of hate crimes. Blacks are 12.5% of the population and are victims of 60% of the hate crimes. When is an Asian an Asian or an Arab or white? \nIgnorant American whites in particular already have low or no tolerance for non-white people, in general, are easily angered by people different from themselves. People who hate others for no reason at all are predisposed to hate others if a plausible reason or cause is provided. Furthermore, many Americans cannot tell the difference between one group of Far Eastern Asians from another. \nIt must be remembered the United States has fought many wars and insurgencies in the Far East and killed millions of Asians either directly or via proxy allies in Indonesia, Vietnam, Japan, Korea, China, the Philippines, Cambodia, and Laos. Many people grew up hearing the derogatory language used to describe Asians as \"chinks\", \"Japs\", \"nips\", \"slants\", \"gooks\", \"sand Niggers\", \"dune coons\", \"Chinaman\", \"camel jockeys\", \"dots\", \"VCs\", and many other names. These are most common among white Americans, but these words and stereotypes have been around for 150 years. There have been anti-Asian riots and massacres in America-all done by whites whom Asians favor over more tolerant Black and Brown Americans. The Civil Rights that Asians enjoy were fought for and earned by Blacks whom they hate, and these same rights were denied to them by whites who they love. The Martinican psychiatrist Frantz Fanon described colonized people who worshipped those that oppressed them as having \"dark skin white mask\". A major solution I would like to suggest to solving the Asian American crisis is for them to stop being either passive or active supporters of white racism. \nAsians have either had a free ride in American life leaving the heavy lifting of the racial cross to the Blacks they loath so much or actively helped make life harder for people who have not bombed them in Asia nor oppressed them in America. The Asian Americans' Stockholm syndrome days are over and they must sober up to the reality that the way that they have conducted their affairs in America, Africa, and Latin America have become toxic. Money spent trying to look white or alter their \nappearance or enter into unhappy marriages to whiten their children have not made them fully accepted as white. Their refusal to stand for justice has a price they are paying today. \nQ: Do you think the Biden administration can address the challenges threatening the U.S.? \nA: No, Biden is incapable of leading the United States. His honeymoon with the press and the left will end before Labor Day this year. Biden is a singular example of how the hatred of Blacks can help a mediocre non-original thinker American politician have a career in high office. He is white,"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195419, Requested 6906. Please try again in 697ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195419, Requested 6906. Please try again in 697ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Albert Harris-Russell was pumping gas at a station across from the Orange County Jail when a stranger confronted him and yelled \"F- the Chinese.\"\nIt was mid-July, and it had become routine to hear former President Donald Trump and other Republican leaders refer to the coronavirus pandemic as the \"China virus\" or \"Kung flu\" even after the phrases had been decried as racist.\nThe man continued with his xenophobic tirade, also referring to COVID-19 as the \"China virus\" until Harris-Russell cut him off, telling the stranger he was Vietnamese, not Chinese. At the same time, he said the man noticed the National Guard logo on Harris-Russell's license plate. The man apologized and walked away.\n\"I can't imagine if I was Chinese what could have happened to me,\" he said. \"He could have gotten physical.\"\nHarris-Russell shared the story publicly for the first time after a series of shootings at three Atlanta spas left eight people dead, including six Asian-American women. Though the shooting has not been officially deemed a hate crime, it heightened the awareness of racially-motivated violence against Asian Americans.\nStop AAPI Hate, a coalition that tracks different forms of hate against Asian Americans and Pacific Islanders in the United \nStates reported nearly 3,800 incidents nationwide since the start of the pandemic.\nThough local law enforcement agencies say there has not been a spike in violence against Asian Americans, the data from Stop AAPI Hate show that the vast majority of incidents recorded were verbal attacks, deliberate avoidance and online harassment, actions unlikely to be reported to police.\nSince the Atlanta attack, Central Florida residents - descendants of China, Vietnam, Korea, Laos, the Philippines and Taiwan - have opened up about harassment that has largely not escalated to physical violence but made them feel less safe in recent months.\n'I don't feel as safe'\nTai Hai Le said a stranger yelled racial slurs at him both in Orlando near the beginning of the pandemic and while he was traveling through a Texas airport.\nMinkyung Chung, a mental health counselor who recently moved from Central Florida to Atlanta, said her family questioned if it was safe to leave home with masks - before Orange County made masks mandatory - for fear of harassment.\nDustin Sundara said he hasn't been attacked personally, but as a long-time gun owner, he bought a smaller one to conceal carry more often.\n\"If you look Asian in general you can be seen as a target,\" Sundara said. \"Personally, I don't feel as safe. A lot of us are technically American. We work here. We pay taxes. Logically and rationally it doesn't make sense because we are all American.\"\nAdrian Lee, a University of Central Florida junior and president of the Asian Pacific American Coalition, described an incident this year when another UCF student said he was approached late at night and asked if he was Vietnamese and if he had coronavirus.\n\"It's easy to brush away,\" said Lee of the incident that might seem minor if it isn't taken into context with the violence that has erupted against Asian Americans across the country.\n\"You really have to look at these incidents as a pattern in the way the pandemic has been racialized,\" she said.\nLee helped plan a vigil for 6:30 p.m. Tuesday on the University of Central Florida campus to honor the victims.\nA legacy of anti-Asian discrimination\nInvestigators say accused shooter Robert Aaron Long, who is white, told police a sex addiction, not race, motivated the killings.\nStill, Ryan Buyco, an Asian-American studies professor at Colorado College said the Atlanta incident is part of a centuries-long legacy of anti-Asian discrimination, sexism and sexualization.\n\"These attacks towards Asians and Asian Americans are new for a lot of us...but we have to understand they aren't new, and they are part of a longer history of anti-Asian racism in the United States,\" he said.\n\"It's not just the shooter in Atlanta, but also the murder of Vincent Chin,\" Buyco said, referencing the killing of a Chinese-American man who was beaten to death in Michigan in 1982.\n\"It goes back to the incarceration of Japanese Americans and to the [Chinese] exclusion laws [of the late 1800s],\" he added.\nThe Page Act of 1875, specifically barred Chinese women from entering the country, because of the assumption that they were prostitutes and would \"infect\" the morality of white men, said Buyco.\n\"The shooting in Atlanta speaks to the sexist and racist history towards Asian women in particular, given that the suspect blamed his crime on his sexual addiction,\" said Buyco. \"Seeing these women not as people, but as sources of temptation, draws from long-standing stereotypes against Asian women as hypersexualized and subservient to men.\"\nFor Le, who was twice verbally harassed since the start of the pandemic, he wants better treatment and more empathy for other Asian Americans.\n\"I hope that others can be mindful of the hardships and struggles that many Asian Americans face because of the boundaries systematically set in the United States that work against people of color,\" he said."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197569, Requested 8172. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197569, Requested 8172. Please try again in 1.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": ", his nonsensical immigration policies and tolerance for human trafficking, and his deviant sexual obsession with offending the religious and moral sentiments of scores of millions of Americans promoting transgenderism and genital mutilation of minors preclude his being able to unite America in anything but bloodshed and chaos. \nQ: Given the U.S. performance under the Trump presidency, do you think the country can present a successful model of governance to the world? \nI have no faith in the present administration of President Joseph R. Biden or his Vice President Kamala Harris Emhoff because both are corrupt and insincere blindly devoted to the bankers, militarists, and the globalists in general. The complete and shameful failure of the Biden-Harris removes unwanted invading American forces from Afghanistan, Iraq, Syria, and ends the wars in West Papua, Libya, Yemen, and the low-intensity conflicts on the African continent (Mozambique, Congo, Niger, Mali, and Nigeria). The Biden-Harris administration is destabilizing Central America by encouraging the flow of drugs, human sexual trafficking of children and post-pubescent youth, and continuing trade policies that destroy the economic base of primarily agrarian societies. Biden, aiming to mask his corruption having used his former office as Vice President to facilitate influence peddling to illegally enrich himself and his immediate family, has feigned being tough on the Peoples' Republic of China which has given untold millions of dollars to the President's family. Further, Biden has hypocritically denounced the Chinese mass incarceration of its restive Uyghur minority but has defined his political career repressing Black Americans for a half-century. \nBiden is the architect of the most draconian laws on the earth that racially targeted millions of Black Americans to be gulag and warehoused in prisons that renewed constitutionally sanction slavery for Black Americans. Biden's disdain for China's human rights challenges appears to be the projections of a pathological liar, careerist racist, an unhinged corrupt sociopath berefts of his mental faculties and common-sense morality. Biden's bizarre obsession with homosexuality has prompted him to threaten loyal ally nations to forgo their religious beliefs and age-old creeds to appease his quack-science belief in transgenderism and non-binary sexuality. \nIn a world fighting to recover its economic moorings, Biden appears to be more preoccupied with imposing debased and immoral degeneracy and psychotic realpolitik than pioneering a genuine international policy that is attuned to the political realities of today. Biden's deliberate and premeditated attack on Syria and the continuance of the counterproductive and failed sanctions regime against the Islamic Republic of Iran manifest the maladroit and moribund foreign policy thrust of his administration. Whereas Biden has gained some international respect for returning to the United States to various treaties relating to global warming and trade, his decisions insult the highly popular \"America First Agenda\" of former President Donald J. Trump. Furthermore, the notion of Biden's providing a successful model to the world is sure to be tested as the restive and angry American public is being held captive to his ineffective and fraudulent COVID-19 \"pandemic\" program that is ignoring the serious social issues faced by 330 million Americans. \nThe Biden administration is neither capable of handling domestic or international issues, and his policies and those around him are clumsily too ideological on enraging the majority of the people with prevaricating promises, puerile policies, ineffective half-measures (i.e. student loan forgiveness waffling), continuance policies aimed to harm Black Americans, amnesty and/or at-will immigration, gun-seizure, tolerance of the assaults on First Amendment Freedoms (speech and assembly), and attacks on religious freedoms and culturally insensitive policy thrust (i.e. the transsexual agenda) threaten to incite mass social disturbances that will far exceed the scale and scope of the George Floyd riots. Biden will be no more able to be a respected global leader than was President Lyndon B. Johnson was during the \"Long Hot Summer\" racial riots of 1963 to 1968 nor George H.W. Bush, Sr. during the Rodney King Riots that swept America in 1992. Biden's health and his leadership are failing, and the whole world knows it, and the world is becoming more multipolar because of failed, corrupt, indecent, and ersatz leadership demonstrated by a succession of American presidents that are tragicomic versus genuine powerful men with vision and purpose. \nQ: What are the main rulers in America? Capitalists or lobbyists? \nA: The controlling interests that rule the United States of America is an inbred coalition of old established Anglo-European Banking families that can trace their lineage to royal or noble blood and those who are nouveau riche who have intermarried with the older oligarchical families. The new rich are primarily farrago of ethnys that all aspire to be either honorary White Anglo Saxon Protestant or are Roman Catholic or of the Jewish faith. All the commanding heights of production, important cultural, social, philanthropic, the military and religious institutions, the reins of government, and the purse strings of the public chest are controlled by what famed Sociologist C. Wright Mills called the \"Power Elite\" comprised in the late 20th century the 400 families that controlled America. \nThe wealth of the richest 1% of Americans constitutes an oligarchy that is allied with their over class peers who rule the other nations of the world. Elite people are the true power that dominates American life not the ballot box or the officials elected by voters. The lobbies' think tanks, research-advocacy institutions, and other entities all serve international finance capital, the conglomerated mass media, the high-tech industry, the agri-industrial complex, military-industrial complex, the medical- pharmaceutical complex, and hydrocarbon and energy sector. The United States has a hypercritical relationship with reality because it profess"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198929, Requested 7838. Please try again in 2.03s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198929, Requested 7838. Please try again in 2.03s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A troubling rise in anti-Asian incidents reported to Minnesota community groups started a month before COVID-19 appeared in the state last year. \nAs the pandemic worsened, so did reports to groups like the St. Paul-based Coalition of Asian American Leaders. In one call to the group, a couple said an angry fellow grocery shopper blamed them for the coronavirus and shoved the husband in the parking lot. \n\"This time it was shoving. What if next time someone wants to run them over with their car?\" said Bo Thao-Urabe, the group's executive director. \nMinnesota's Asian American and Pacific Islander communities are joining state and federal Democratic lawmakers in urging new legislation to improve hate-crime reporting and police training in the state. They say there's renewed urgency after the recent deadly shootings of six Asian American women and two others in Atlanta, in addition to ongoing harassment here and elsewhere that threatens to outlast the pandemic. \n\"We must condemn it universally,\" U.S. Rep. Ilhan Omar, D-Minn., said at a news conference on Wednesday in support of federal and state legislative proposals. \"We must remember that our destinies are tied together: An attack on one of us is an attack on all of us.\" \nState Rep. Frank Hornstein, DFL-Minneapolis, is again sponsoring legislation to let community groups file hate crime reports and to update police training guidelines. The bill would make graffiti and other acts of property damage eligible to be counted as bias crimes. \nHate crime data in the U.S. has been historically uneven: The FBI produces an annual tally each year, but those numbers come from voluntary reporting from police nationwide. Not all states participate, and while Minnesota police agencies are required to submit data on \"bias-motivated incidents\" to the state Bureau of Criminal Apprehension (BCA), many of the state's largest police departments routinely report having investigated no cases each year. \n\"So many thousands of hate crimes are not reported, and when they go underreported they are not discussed and the perpetrators are not held accountable,\" Hornstein said. \nWhat data does exist points to an upward trend of anti-Asian hate crimes around the country. A recent study by the Center for the Study of Hate and Extremism at California State University, San Bernardino, found such incidents climbed nearly 150% in 16 of the country's largest cities last year, even while hate crimes dropped overall. \nThe BCA has not released data on hate crimes in 2020 but last reported a slight increase in overall bias incidents from 2018 to 2019. \nLast week, Minnesota Attorney General Keith Ellison convened the first of multiple virtual roundtable discussions on stopping AAPI hate. Local Department of Justice leaders and elected officials including Ramsey County Attorney John Choi participated. \n\"This is not only a conversation, this is a call to action,\" said Ellison, who announced a task force. \"We are going to keep the pressure on. We are going to keep working hard and we are going to protect the community.\" \nSurge in incidents \nU.S. Attorney General Merrick Garland has cited the surge in incidents against Asian Americans in ordering a 30-day review of how the Justice Department can best curb hate crimes. \n\"These sensational events are a rallying call for action, but it is up to us to do the rest of the work,\" said Rep. Samantha Vang, DFL-Brooklyn Center, who is co-sponsoring the House bill with Hornstein. \nOmar is also co-sponsoring a resolution to call on all public officials to denounce anti-Asian racism related to COVID-19 and asking federal officials \"to expeditiously investigate and document all credible reports of hate crimes and incidents and threats against the Asian-American community and prosecute perpetrators.\" \nDemocrats on Wednesday repeated calls for the Republican-led Minnesota Senate to hold a hearing on Hornstein and Vang's proposal, which passed a House committee last week on a 10-8 party-line vote. \nTheir bill's chances of success are slim in the Senate, where a GOP spokeswoman said Wednesday that Sen. Warren Limmer, the Maple Grove Republican who leads the Judiciary Committee, will focus only on passing a budget for the remainder of the 2021 \nsession. \nRachel Aplikowski, the GOP Senate spokeswoman, said law enforcement groups have expressed concerns that adding additional reforms on top of new training requirements passed last year would be difficult to implement in a timely manner. \nGender identity issue \nOther Republican state lawmakers have cited the House bill's protections for gender identity in their objections. \n\"I believe in science. I believe if you have an XY chromosome you're a male, if you have a YY chromosome you're a female,\" said State Rep. Eric Lucero, R-Dayton. \"And the language here is going to put Minnesotans in the awkward position of being science deniers and having to choose science over somebody's confusion.\" \nLucero unsuccessfully tried to add an amendment making officers immune from claims that they did not properly investigate bias crimes based on gender identity. \n\"The bottom line of civil rights and human rights laws in the state is that everybody is protected, no exceptions,\" Hornstein said."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194670, Requested 6949. Please try again in 485ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194670, Requested 6949. Please try again in 485ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "valued. We can resist. Take up space. Make noise,\" tweeted novelist Min Jin Lee.\n\"Instead of centering a White Supremacist who had a 'sex addiction' (which btw you can also be a racist terrorist simultaneously), remember the real victims yesterday and their families,\" Los Angeles comedian Kristina Wong said in a tweet.\nAnd Rep. Ted Lieu's tweets got angrier as the day went on.\nAt 8:27 a.m., the Torrance Democrat targeted Trump and some elected officials for creating a hostile environment with hateful anti-Asian rhetoric. \"If you are one of those officials,\" he wrote, \"please stop.\"\nAt 9:55 a.m., he scoffed at reports that \"sex addiction\" rather than racism motivated the shooting.\nTwo and a half hours later, he sounded like he'd had enough, after a spokesman for the Sheriff's Department in Cherokee County, Ga., said the gunman went on a rampage because \"yesterday was a really bad day for him.\"\n\"All of us have experienced bad days,\" Lieu tweeted. \"But we don't go to three Asian businesses and shoot up Asian employees.\"\nThe former prosecutor said in an interview that denying a racial component to the shootings \"does not comport with the facts as we know them right now.\"\n\"We've seen an increasing amount of violence toward Asian Americans, beginning with more verbal harassment,\" Lieu said. \"Then it escalated. You had an Asian American family in Texas last year that was stabbed, because the perpetrator thought they had spread the virus ... Then you saw elderly Asian Americans targeted. For many of us, it was not surprising to see multiple Asian victims of the crime.\"\nAnd then there's history, a reminder that what's past is prologue, from the 19th century \"yellow peril\" scapegoating and Chinese Exclusion Act to the incarceration of Japanese Americans by the U.S. government during World War II.\nIn 1982, a 27-year-old Chinese American man named Vincent Chin was beaten to death with a baseball bat by two unemployed Detroit autoworkers who thought he was Japanese.\nIt was a time when the U.S. auto industry was in decline. The attackers blamed Chin.\n\"When Americans feared Japan's rise in the 1980s, that caused an increase in hate crimes,\" Lieu said. \"And now, with the pandemic, you see a rise in hate crimes against Asian Americans. ... When America felt threatened in the past, sometimes this country would target Asians.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198114, Requested 7983. Please try again in 1.829s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198114, Requested 7983. Please try again in 1.829s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK - One recent Saturday afternoon Teresa Ting passed out purple medical masks - the same color as the 7 train that connects Flushing Queens to Manhattan - among a group of 14 volunteers gathered on the steps of the massive Flushing post office on Main Street. A spreadsheet on Ting's phone detailed the day's watch routes in the neighborhood. One team would monitor Main Street another would take Prince Street and 37th Avenue. \nA third group would patrol the blocks between Union Street and College Point Boulevard on Roosevelt Avenue where a man shoved an Asian American woman to the ground outside a bakery in February. \nThe attack galvanized Ting to form Main Street Patrol a volunteer safety group that casts a watchful eye over downtown Flushing equipped with little more than safe intervention strategies and a walkie-talkie app on their phones. \nTing an actress and Queens native works with a rotating cast of volunteers from different parts of the city and walks of life all driven by outrage at unprovoked assaults against Asian Americans and inaction by bystanders. \n\"This past year has taken a toll\" Ting said. \"Something needed to be done. As it happens that Saturday was her 30th birthday her second celebration in a pandemic but her first in her new role safeguarding the community. \nAsian Americans are grappling with fear rage and anxiety brought on by attacks against people of Asian descent particularly older men and women. In an especially egregious episode a little more than a week ago a 65-year-old Filipino woman was brutally assaulted in broad daylight outside a luxury apartment building in midtown Manhattan. Two lobby staff members who witnessed the assault but failed to intervene have been fired. \nAfter each case Asian Americans express shock and pain but point to a long history of bigotry that has been magnified during the pandemic. Advocates also attribute the current climate to former President Donald Trump and his use of damning and inaccurate phrases like \"the China virus.\" \nThe New York Police Department has received 35 reports of anti-Asian hate crimes already this year compared with 28 in all of 2020 and only three in 2019. Officials say the data fails to paint a full picture as many incidents go unreported or are not categorized as hate crimes. \nAs the attacks mount Asian Americans and their allies are hoping to make their streets safer. \nIn addition to Main Street Patrol whose volunteers take weekend shifts in downtown Flushing at least three other similar groups have formed during the pandemic. A series of assaults against women in East Williamsburg Brooklyn in January led to SafeWalks NYC which offers escorts at train stations in various neighborhoods including Manhattan's Chinatown. \nAnd in February last year Karlin Chan a community activist formed the Chinatown Block Watch whose participants keep watch over an area that is the birthplace of Chinese culture in New York City. Chan who is in his mid-60s is aware of the risks he faces as he roves the neighborhood. \n\"With my beautiful crop of gray hair there is a target on my back\" he said. But the urgency of the moment attracted Chan who experienced racism as a child growing up in Chinatown in the early 1960s. He leads volunteers along Mott Street and Grand Street as well as the blocks under the Manhattan Bridge to check in with the neighborhood's residents and business owners. \nCivilian patrols have a long and mixed history in New York City. Most notable is the Guardian Angels founded in the late 1970s when economic blight helped fuel a wave of violent crime across the city. Known for their red berets and bullish presence the Guardian Angels emerged as a crime-fighting patrol on the subways acquiring a kind of hero status while simultaneously drawing the ire of politicians and transit officials for their vigilante justice. \nAfter a prolonged period of low crime in New York City the Guardian Angels have reemerged with the sharp increase that has coincided with the pandemic and they have spent time on the ground in Chinatown this past year. The group's founder Curtis Sliwa announced in March that he was running as a Republican in the city's 2021 mayoral race. \nBut Main Street is quick to stress that it focuses on intervention rather than deterrence. \n\"We want to remain a little more covert so we blend in more and we can actually run into catching more situations rather than suppressing them\" Ting said. \nThe one incident the group has run into got hairy after a patroller started recording two young men who were overheard making racist remarks and threatening to harm an older man. The men became agitated and knocked the patroller's phone out of his hand eventually fleeing. \nThe group later revised some of its protocols stressing that volunteers should record episodes only if they can do so from a safe distance. \nWhile civilian patrols do not have official approval of the Police Department they work in concert with officers by being an extra set of eyes and ears\" said Capt. Paul J. Zangrilli in Chinatown's Fifth Precinct. Earlier this year for example when teams of pickpockets were targeting older women Chan's Chinatown Block Watch provided the police with information that ultimately led to the apprehension of both groups. \n\"They have added to our success in their heightened vigilance and heightened awareness out in the community\" Zangrilli said. \nAs Main Street Patrol was winding down its Saturday tour in Flushing another group was assembling in Confucius Plaza in Manhattan. Ricky Yang stood behind a folding table with a sign that read \"Protect Chinatown\" the group he established in \nFebruary as a chaperone service which got off to a slow start. Now the group was setting out on foot patrols in part to raise awareness about its services and to monitor the streets. As volunteers mobilized one older man of Asian descent stopped to ask a question not about safety measures but about the COVID-19 vaccine. \nBefore sending teams out Yang 27 emphasized hypervigilance. Like Main"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194966, Requested 7497. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194966, Requested 7497. Please try again in 738ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "The nation is mourning another mass shooting, the second in less than a week. But even as we grieve the 10 lives taken in Boulder, Colorado, I am still mourning those killed in the Atlanta area on March 16. Welcome to the \"This is America\" newsletter, centered on race, identity, and how they shape our lives. Im Eve Chen, a senior video producer with the USA TODAY Network and co-chair of Gannett's Asian American employee resource group.Ask most Asian Americans and we can tell you about the times we were picked on growing up. The times kids yelled \"ching chong\" and pulled at the corners of their eyes. The times classmates made fun of our names or the unfamiliar foods we ate or the way our parents spoke. The times we were asked if we were related to literally any other Asian American at school.As adults, were asked where were from, no, where were really from. Were told we speak English well even if weve lived here our whole lives. Were fetishized as women or desexualized as men. Were blamed for COVID-19, as witnessed by one in four Americans according to a USA TODAY/Ipsos poll conducted last week.Of course most people dont mean to be racist, unconsciously centering themselves and keeping others outside. But a lifetime of microaggressions, coupled with actual violence against Asian Americans throughout the pandemic, creates cumulative trauma, which reached a tipping point with the Atlanta-area spa shootings.But first, race and justice news we're watching this week:\n- Asian Americans report the biggest increase in serious incidents of online hate and harassment during the COVID pandemic.\n- There's been a rise in anti-Asian attacks. Here's how to be an ally to the community.\n- Jury selected in the trial of former police officer Derek Chauvin charged with murder in the death of George Floyd.\n- Supreme Court wrestles with complex questions of tribal power arising from a late-night traffic stop.Too close to home\nThe shootings hit especially close for those of us who call the Atlanta area home. My old newsroom is just blocks away from two of the spas where women who looked like me were gunned down.Whatever the stated motive or charges, my local and larger Asian American community felt attacked again.Every day for months, Asian Americans around the country have been beaten, pushed to the ground, spat on, and indeed killed. Stop AAPI Hate reported nearly 3,800 anti-Asian incidents over the past year, mostly targeting women. Asian Americans also reported the biggest increase in serious online hate and harassment incidents, according to an Anti-Defamation League survey shared exclusively with USA TODAY, with 17% reporting sexual harassment, stalking, physical threats, and more.Weve had an entire pandemics worth of fear and frustrations unloaded on us. The bullies never went away.Weve been here before\nIt's scary, but I refuse to live in fear in my own country.The morning after three local Asian American businesses were attacked, I went out to support three others: a restaurant, a food stall, and a grocery store. A few days later, I supported three more. After a year of enduring the dual threats of COVID-19 and racism, I wanted local business owners and workers who reminded me of my own parents years ago to know now is not the time to close their doors.From one of the largest lynchings in U.S. history, the Chinese Massacre of 1871, to the WWII internment of 120,000 Japanese Americans, to the 1982 murder of Vincent Chin, a Chinese American killed by two Detroit autoworkers who blamed industry struggles on Japan, we've faced so many hardships since Asians first arrived on U.S. shores in the 19th century.I was reminded of our resilience last week when I stopped by one of my favorite Asian American shopping centers in Duluth, Atlanta's Koreatown. Seeing the flag fly at half-staff in front of the H Mart, I remembered: We are grieving, but we are still standing and strong.Stand with us. Reach out to Asian American friends and colleagues without burdening them to share or educate you. Do the work to learn about Asian American experiences and history by reading up or watching relevant documentaries. Amplify Asian American voices and concerns and call out anti-Asian racism. Hollaback! and Asian Americans Advancing Justice are offering free bystander training sessions through April. #StopAsianHate"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197494, Requested 7704. Please try again in 1.559s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197494, Requested 7704. Please try again in 1.559s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Bigotry and brutality targeting Asian-Americans has spiked over the past year as racist rhetoric linked to the coronavirus pandemic soared. \nVerbal harassment, shunning, spitting and violent physical assault were among the 3,795 incidents reported by victims nationwide since last March, according to Stop AAPI Hate. \nAnti-Asian hate crimes reported to the NYPD skyrocketed 833% last year compared with 2019, with three anti-Asian crimes reported in 2019 and 28 in 2020. The latest anti-Asian numbers, as of this April 11, saw 54 anti-Asian hate crimes reported to the NYDP, compared with 12 by that date in 2020 - an increase of 350%. \nLast month, Brandon Elliot, 38, was hit with an assault as a hate crime charge after he was caught on video stomping on Vilma Kari, 65, on a Hell's Kitchen sidewalk. \"F-k you,\" he allegedly told the Filipino woman. \"You don't belong here.\" Video released by police ends with a doorman who witnessed the horrific incident closing his building door and the attacker walking away. \nAcross America's 16 largest cities, reported cases of anti-Asian hate and discrimination surged 145%, while overall hate crimes fell 6%, the Center for the Study of Hate and Extremism reported. \nFormer President Donald Trump added fuel to the fire by calling COVID-19 the \"Chinese Virus\" and \"Kung Flu.\" \nIn a dozen recent interviews with prominent members of the Asian-American and Pacific Islander community in fields of film, TV, theater, fashion and music, heartache over the rising tide was undeniable, as was recognition the U.S. has a long history of minimizing anti-Asian discrimination. \nComedian Margaret Cho said last month's slaughter of eight people, including six Asian women, at three Asian spas in metro Atlanta was a \"devastating\" inflection point. \nHere, Cho and the other famous figures share their personal experiences with discrimination and their reactions to the recent spate of bigoted terror. \nDavid Henry Hwang \nPlaywright \n\"A few years ago, I was stabbed in the neck by an unknown assailant around 9 p.m. on my block in Brooklyn. The attacker severed my vertebral artery and I lost one-third of my blood. I managed to walk a few blocks to Brooklyn Hospital, where doctors there saved my life. The NYPD did not label my attack a hate crime, but New York state Assembly member Ron Kim (D-Queens) called a press conference to denounce anti-Asian hate and violence. \n\"The current wave of AAPI attacks is nothing new. Because Asians have always been stereotyped as perpetual foreigners, our well-being has always been a function of America's relationship with Asian nations. The big lie of American racism is that if you do everything right and play by the rules, you will be accepted in this country. But the U.S. throughout its history has consistently betrayed this 'model minority' myth, used by the right wing to drive a wedge between AAPIs and our natural Black, Indigenous and people of color allies. We are painfully reminded once again that white supremacy can never be defeated by accommodation, only through organized opposition and progressive activism.\" \nMargaret Cho \nActress and comedian \n\"For me as an Asian-American artist, comedian, actor, it's really more about noninclusion. It's really about being very invisible within Hollywood, within the media, and it's hard to kind of understand how to even talk about invisibility because it's, like, invisible. It's very weird when you're just excluded from the conversation and you're trying to be in an industry that doesn't see you. It's the most insidious kind of racism because it's this feeling of nonexistence. It combines erasure with \na kind of gaslighting that we don't matter or we don't make any impression. And the truth is, we've made a huge contribution to American life. We've been here since 1849, yet still somehow we are seen as 'other,' or not part of America, so this has to change. \n\"I was a resident of Atlanta for seven years. I know exactly all of those spas and the neighborhoods and the communities. They're very familiar to me. The fact that law enforcement was really bending over backwards to try to humanize the murderer was really disturbing. And not categorizing this as a hate crime is part of the problem. \n\"You don't ask somebody who's committed a hate crime whether or not it's a hate crime. It's not something that's up to their opinion.... It is a hate crime when you're targeting Asian women who you say make you feel a certain way so you're \n--- \ngoing to kill them. Why would he specifically seek out Asian spas, Asian-owned spas, Asian-run spas in three different locations in order to kill Asian women if race isn't a factor? That to me is clearly a hate crime against Asians and Asian women in particular. It has nothing to do with sex addiction. Lots of people are saying these are sex workers. I was a sex worker. I know what sex work is. This to me seems more likeskin care and aestheticians. The women were mostly 50, 60, 70. This was not a typical idea of what people think of as sex workers or Asian sex workers. The framing of this as a crime by a sex addict who wanted to kill sex workers, it's not exactly that. There's a lot of projection happening onto this crime that has to do with the inherent racism that exists in society around Asian \nwomen. \n\"I'm grateful to the hashtags #StopAAPIHate and #StopAsianHate. All of these movements are really important, and they really connect us in way that"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192551, Requested 7977. Please try again in 158ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192551, Requested 7977. Please try again in 158ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Phil Yu was not surprised that a white gunman killed eight people, six of them Asian women, at Atlanta-area spas advertising their \"Asian\" or \"international\" staff.\nThe worst thought he had on Wednesday, the day after the massacre, was just that, he said -- he had seen it coming.\n\"Being Asian in this country, no matter how long you've been here or how you got here, often feels like a constant negotiation of feeling unwelcome,\" said Yu, a blogger and commentator in Southern California who posts as Angry Asian Man.\nMany Asian Americans are bristling with pain and fury, seeing the killings as a culmination of a steady drumbeat of racist attacks, with some people blaming them for the coronavirus pandemic because of its origins in China.\nLaw enforcement officials said that Robert Aaron Long, a 21-year-old from Woodstock, Ga., was motivated by \"sex addiction,\" that there probably wasn't a racial component to the attacks and that Tuesday was \"a bad day for him.\"\nLong was charged Wednesday with eight counts of murder and one count of aggravated assault.\nBut many Asian Americans saw it differently. The killing of so many Asian women, at businesses known to employ Asian workers, was a racial targeting by its very circumstances, they said.\nThey feared for their parents, their grandparents, their friends, their children, themselves. They lamented that hard-won immigrant dreams had been tarnished by hate. Some Asian American women shared their experiences of being dehumanized and fetishized by white men.\nAfter all, the killings happened the same week the advocacy group Stop AAPI Hate reported that thousands of Asian Americans and Pacific Islanders have faced racist verbal and physical assaults since the pandemic began more than a year ago.\nAlso this week, former President Trump again called COVID-19 \"the China virus\" on national television.\nRep. Judy Chu (D-Monterey Park) is set to testify at a congressional hearing Thursday on the rise in hate crimes against people of Asian descent.\nChu, the first Chinese American woman elected to Congress, chairs the Congressional Asian Pacific American Caucus.\nShe laid the eight deaths directly at Trump's feet, as did other Asian Americans.\n\"President Trump clearly stoked the flames of xenophobia against AAPIs with his rhetoric,\" Chu said Wednesday on Capitol Hill during the Democratic Caucus' weekly press conference, by calling the virus \"kung flu\" and the \"Wuhan virus,\" in addition to the \"China virus.\"\n\"And what we saw yesterday is the result of that,\" she said.\nAt least four of the women killed were of Korean descent, and the attacks have been closely covered in Korean-language media.\nAt the Korean supermarket H Mart in Garden Grove, some shoppers debated the suspect's motives.\nSome wondered why there hasn't been more focus on the shooter's \"racial targets.\"\nWhen asked for their thoughts, several older shoppers turned away.\n\"We don't want to be targets, too,\" one said.\nAt neighboring businesses, people dashed in and out for quick haircuts and steamed tofu lunches.\nElizabeth Choi was choosing a birthday cake for her uncle at the Paris Baguette bakery.\nThe occasional mention of anti-Asian hate incidents within her inner circle seemed \"just like casual talk, until we heard about what the guy did to the spa workers,\" said Choi, who is Korean American.\n\"Why would people want to harm poor workers? Something more aggressive seems to be happening -- yes, we're scared now,\" she said through an interpreter. \"Really scared. Terrified.\"\nThe Brea homemaker, 51, said her daughter advised her not to go to \"too many areas outside of Asian areas.\"\n\"When they're looking for a Chinese face\" to blame the pandemic on, \"any Asian face\" will do, Choi's daughter told her.\nAt the beginning of 2020, when news of the coronavirus was circulating in Asia and Europe but not yet in the United States, Christine Liwag Dixon braced herself for what was to come.\nDixon is mixed-race Filipina and white, a 31-year-old writer who lives in New York City. Family members and friends had shared some of the racist comments about Asians they had endured even in the very early days of the pandemic.\n\"I was afraid to leave the house,\" she said.\nWhen she finally ventured out once last year for a neighborhood walk, she wondered: \"What if they say something? What if they throw something at me?\"\nAs Dixon watched the news of the Atlanta attacks and learned about the increase in anti-Asian hate crimes, she cried.\nAll those times, she realized, she wasn't overreacting. When officials said the shootings weren't racially motivated, she laughed in disbelief.\n\"I think that's a load of b--\" she said.\nDixon and other Asian Americans are frustrated that there was not more attention on the issue.\n\"I have felt almost gaslit by the lack of coverage,\" she said. \"I've been sitting here all day wondering, 'Why now? Why today?' We've been talking about this for a year. Why does it take these Asian women being slaughtered for people to suddenly pay attention?\"\nSocial media was charged with such outrage.\n\"In less than 48 hours, we had a historic Asian Oscar moment with multiple firsts in 93 years--then a mass shooting targeting 3 Asian-owned businesses. This is how terrorism works -- you're not allowed to feel safe, accepted, or valued. We can resist. Take up space. Make noise,\" tweeted novelist Min Jin Lee.\n\"Instead of centering a White Supremacist who had a 'sex addiction' (which btw you can also be a racist terrorist simultaneously), remember the real victims yesterday and their families,\" Los Angeles comedian Kristina Wong said in a tweet.\nAnd Rep. Ted Lieu's tweets got angrier as the day went on.\nAt 8:27 a.m., the Torrance Democrat targeted Trump and"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196154, Requested 7296. Please try again in 1.035s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196154, Requested 7296. Please try again in 1.035s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "BEIJING, March 25 (Xinhua) -- The videos and images of a 76-year-old Asian woman pitifully crying with a bruised eye after being assaulted by a white man in San Francisco have gone viral on social media and shocked the whole world.The elderly woman who bravely beat off the attacker was yet the latest victim of a surging tide of anti-Asian violence and hate crimes in the United States, which has traumatized millions of Asian Americans and plunged them into growing fear. Ealier this month, six Asian Americans were shot dead in cold blood by a young white gunman in Atlanta, Georgia.When people across the world are expressing sympathy for the victims as well as their indignation against those perpetrators, they are also wondering why the United States, a country known as a nation of immigrants, is struggling with waves of violence and xenophobia?With a short history densely stained by a racist tradition, the country built by immigrants is unsurprisingly infamous for racial discrimination. From the state-sanctioned killings of native Indians and the extreme abusive policies against African slaves and Chinese laborers, to the Chinese Exclusion Act and the Immigration Act of 1924, the United States has a deplorable track record in maltreating its minority groups.As time passes, such a racist tradition has turned more extreme and violent. When political, economic and social problems arise, ethnic minorities have always been made the scapegoats. That is also the case in the COVID-19 pandemic, which has so far infected over 30 million people in the world's most developed country.To gloss over the country's incompetence in fighting the pandemic and shift the blame, unscrupulous Washington politicians and media outlets have been explicitly manipulating racist rhetoric and sentiments by linking the deadly pathogen to specific ethnic groups, leading to increasing violence and hate against Asian Americans.According to Stop AAPI Hate, a California-based nonprofit social organization tracking incidents of violence against Asian Americans and Pacific Islanders, it received nearly 3,800 reports of attack or abuse against people of Asian descent between March 2020 to February 2021.Erika Lee, a historian and the author of \"America for Americans,\" described the acts of racism and violence facing Asian American and Pacific Islander communities \"a systemic national tragedy.\"Behind the tragedy is the U.S. society's deep-seated and long-standing white supremacy, a belief that white people constitute a superior race and should therefore dominate the society. This extremist bigotry has gained fresh momentum in recent years from the intensified polarization of American politics.In 2020, distribution of white supremacist propaganda in the United States increased nearly twofold from a year ago, with 5,125 incidents of racist, anti-Semitic and other hateful messages being reported, according to the Anti-Defamation League, an anti-hate organization headquartered in New York.Regrettably, successive U.S. administrations have been tolerant and insensitive to these problems, letting the systemic national tragedy deprive ethnic minorities of their basic rights in wealth distribution, employment, healthcare, education and political participation.In fact, devastating racism is only the tip of the iceberg of America's mess in human rights protection. China's State Council Information Office on Wednesday issued the Report on Human Rights Violations in the United States in 2020, depicting the whole picture of the country's chaotic human rights situation.Besides comprehensive, systematic and continuous racism, the report also disclosed Washington's incompetent pandemic containment, the American democracy disorder, the continuous social unrest, the growing polarization between the rich and the poor as well as the U.S. trampling on international rules that has resulted in worldwide humanitarian disasters.With mounting pressure from nationwide protests against anti-Asian hate, the U.S. government is likely to take some perfunctory measures. But one thing is clear: without sweeping reforms and an earnest attitude, Uncle Sam can hardly clean up its mess in human rights protection, let alone act as a beacon."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198952, Requested 7842. Please try again in 2.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198952, Requested 7842. Please try again in 2.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Nation Confronts Recent Rise in Attacks Against Asian Americans and Pacific IslandersFor some Americans, the shocking attacks and murders over a week ago by a white man in Atlanta that took the lives of eight people, six of them Asian women, provide further credence for more stringent gun control laws in the U.S. despite objections from those who point to the Second Amendment.But even before the March 16 shootings at three massage parlors in the Atlanta area, many who identify as Asian Americans and Pacific Islanders [AAPI] had already become aware of and concerned about an increase in \"hate crimes\" in which they or others from their community had been victims.Some assert that the rise in attacks, whether physical or verbal, began to escalate after former President Trump took office. Not only did Trump refer to COVID-19 as the \"Chinese virus\" or the \"Wuhan flu\" and support anti-Asian hashtags but the Biden Administration says his \"damaging rhetoric\" both fueled anti-Asian discrimination and \"elevated threats\" to the community.Meanwhile, retail corridors around the D.C. region have been promised greater police protection and surveillance in efforts by law enforcement to keep those who own or frequent the local restaurants, hair and beauty outlets and nail salons free from attacks. Many of these businesses are owned or operated by members of the AAPI community.At the House of China, a Chinese restaurant in Riverdale, Md., when one female clerk was asked to share her views about the recent shootings in Atlanta, she ignored this reporter and continued to take food orders. However, she did briefly say, \"I don't speak English.\"But another person at the restaurant, unwilling to share their name, said that in recent months, he's experienced frequent harassment by local police leading him to wonder, \"What should I do?\"But rather than lay the blame at Trump's feet, consider the data compiled by national coalitions like Stop AAPI Hate, an organization which addresses anti-Asian discrimination among its goals as cited in a report March 23 in The Washington Post.According to the organization, 3,795 self-reported incidents were filed by AAPIs between March 19, 2020 and Feb. 28, 2021 in the U.S. The incidents, which ranged from name-calling and verbal assaults to physical altercations like being punched, kicked or spit upon, have inevitably been underreported, experts say. Those familiar with the AAPI community point to cultural mores which influence their members to often remain silent rather than speaking out about such violations.\"Unfortunately, the reality is Asian Americans are now scared to go outside,\" said John Yang, president and executive director of Asian Americans Advancing Justice, a D.C.-based nonprofit. \"You have community members who are literally thinking about whether they should appear in public at all.\"But one local politician said he refuses to be silent including Maryland Gov. Larry Hogan who met with local Asian business owners, along with his wife, Yumi, an immigrant from Korea, in Howard County on Monday, March 22.The Hogans, along with Howard County Executive Calvin Ball, toured Asian-owned businesses that are part of \"Korean Way\" - a state-designated five-mile stretch along Route 40 that serves as home for about 166 Korean businesses.One day earlier, Hogan said on CNN that he has witnessed the effects of discrimination against Asian Americans escalate during the coronavirus pandemic. He referred to the recent murders in Atlanta as \"outrageous\" and \"unacceptable.\"In the District, Ben de Guzman, Director of the Mayor's Office on Asian and Pacific Islander Affairs, issued a statement strongly condemning the March 16 shootings \"and the senseless murder of eight people, including six Asian-American women.\"And in social media posts immediately after the attacks and then later during a press briefing, D.C. Mayor Muriel Bowser reaffirmed the city's commitment to opposing violence against the AAPI community throughout the District.\"Our office is connecting with District residents, business owners, city agencies, law enforcement, organizations, our government, and our colleagues around the country towards a common cause,\" she said. \"We know that the attacks against Asian Americans and Pacific Islanders are a national phenomenon. Here in the nation's capital, we are committed to addressing anti-Asian violence, racism and xenophobia.\"My wife, my three daughters and my grandkids are all Asian, and they - they have felt some discrimination personally,\" he said. \"We feel it personally with my daughter, who is sometimes afraid to come to visit us, with people who had best friends that were being harassed at the grocery store, or being called names and people yelling about the China virus, even though they're from Korea and born in America.\"WI Editor D. Kevin McNeir contributed to this report.\nSidebarOne day earlier, Hogan said on CNN that he has witnessed the effects of discrimination against Asian Americans escalate during the coronavirus pandemic."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194261, Requested 7978. Please try again in 671ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194261, Requested 7978. Please try again in 671ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK - The surveillance video captures a brutal scene: A woman is thrown down a flight of stairs and smacks into the subway platform violently enough to fracture a bone in her face. It was May 28 and the woman in her 60s was among dozens of people attacked during a spate of anti-Asian violence this year. It may not even have been the first such attack by the suspect John Chappell a law enforcement official said. Two months earlier Chappell who had dozens of prior arrests had been suspected of lighting an Asian woman's backpack on fire the official said. He was released just days after his arrest in May.Six months into a series of brutal attacks on people of Asian descent across the city Chappell's case underscores the challenges police and prosecutors have faced in both preventing the violence and punishing those responsible. Many of the attacks are unpredictable and carried out by people in the throes of mental health episodes seemingly at random. Officials say they doubt many of the hate crime charges related to the attacks will stick in court and those arrested are often released quickly. And the Police Department appears to have scaled back its efforts to stop them: An undercover unit intended to prevent anti-Asian attacks has not been active since May after officers faced threats of violence themselves.But the attacks have continued and anxiety and trauma still grip many pockets of the city's Asian communities where the violence feels fresh even as the spotlight on it has dimmed. \"There's still this fear that permeates throughout the community\" said Chung Seto a community leader and political strategist in Chinatown.For many she said the fear feels like a continuation of the darkest days of 2020 when city residents were afraid of going outside because of the coronavirus. Now shop owners in Seto's neighborhood remain concerned about staying open late and elders - including Seto's parents - will not venture outside. \"It's not so much catching COVID\" Seto said. \"There's no vaccine for racism.\"Attacks on Asian Americans have shaken cities around the country: In Los Angeles hate crimes against Asian Americans more than doubled in the past year and in Boston Asian American elders are learning how to defend themselves with canes. For New York the problem endures as the city forges ahead with its reopening and visitors once again wander the streets of Chinatown - and many living in the neighborhood say they feel left behind.But for New York's police stopping the attacks before they happen is particularly difficult - even when the person accused has dozens of prior arrests. And even when arrests are made the defendants are often released pending trial corrections records show. Chappell for example was released just a few days after his arrest despite prosecutors seeking high bail. \"It's nice to know there's a task force. It's nice to go on the bus and there's this messaging of anti-Asian hate crimes\" said Kevin Nadal a professor at John Jay College of Criminal Justice. \"But what does that actually do?\"The challenges continue even as anti-Asian violence keeps rising: As of June 27 reported hate crimes against Asian New Yorkers had increased by 400% compared with the same time frame in 2020 from 21 to 105 according to Police Department statistics. The psychological effects of that violence have scarred entire communities. In South Brooklyn where a community senior center just reopened after closing for the pandemic Don Lee a community organizer said Asian elders have been hesitant to travel to and from its programs. \"There are people who are excited to come back but we know many of the seniors don't feel safe to come out\" Lee said. \"The fear's still very real.\"Lee said he knew firsthand that some victims of harassment and hate crimes were no longer reporting the incidents to police because they believed nothing meaningful would be done with their case. \"What is the point right? What is the point?\" Lee said. \"I don't think it's the police. I think it's the system.\"Law enforcement officials and experts note that it can be difficult to prosecute cases as hate crimes which require proving the defendant's intent was based on the victim's race or ethnicity. In previous years many suspects might have been arrested on assault or harassment charges without a hate crime designation. \"The public is seeing this rash of attacks on Asian Americans and it is possible that there is a trend happening because of racial animus,\" said Alissa Heydari, a former assistant district attorney in New York City who now helps direct the Institute for Innovation in Prosecution at John Jay College of Criminal Justice. \"But to prove it in court when the criminal standard is beyond a reasonable doubt it is really hard to show that a victim was picked in large part because of their ethnicity or gender.\"The attacks many of which have been recorded on video and shared widely shocked the conscience of the city. Groups of volunteers now patrol the streets of Chinatown hoping to deter potential attacks. Many Asian New Yorkers say they no longer leave home without pepper spray or established buddy systems. In March the Police Department cobbled together a volunteer group of Asian American officers who work during their time off hoping to stop attacks if they see them happening including a pilot program where undercover officers wandered streets where anti-Asian violence had taken place and was thought to likely reoccur.The plainclothes officers were meant to both lure potential offenders into confrontation and intervene if they saw anti-Asian harassment occurring. But the undercover strategy left officers in tenuous positions and some were nearly attacked according to a law enforcement official familiar with the matter. In one instance an undercover officer who is of Asian descent was approached by a man on a train platform in Queens. The man waved his hand and hat in the officer's face and said \"That's why you peoples are getting beat up\" according to a police report. He was charged with aggravated harassment as a hate crime in April. Another officer was approached by a man in midtown Manhattan who shouted anti-Asian slurs at him and said \"Go back"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195876, Requested 7779. Please try again in 1.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195876, Requested 7779. Please try again in 1.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "On my walks through pre-pandemic Shanghai in late September 2019, around every corner I stumbled upon everyday scenes of older people's integration into the fabric of the city and community life.\nPeople with full heads of gray hair rode bikes and sat in front of storefronts, chatting with neighbors and eating meals from large metal bowls. An old woman whizzed by in a motorized wheelchair alongside scooters in the street. At Fuxing Park, grandpas tended to grandchildren and older men bounced spinning disks in a game of diabolo.\nAfter three days of walking around Shanghai's teeming streets, I rested my weary feet at Jing'an Sculpture Park. An old man glanced at me writing in my notebook and struck up a conversation in English. He'd studied English in the United States and wanted to know how old I thought he was. 70, I guessed. He threw his head back and smiled. 85. He lived nearby and came to the park every day.\nAsian elders in America are vulnerable\nSuch postcards echo similar scenes in Asian neighborhoods across the United States. They remind me of older men smoking, chatting and sipping warm beverages in a plaza near San Francisco's Japantown and a trio of elders gathered for morning calisthenics in North Beach's Washington Square. They spark sweet memories of the elders I saw before the pandemic who danced at Portsmouth Square in Chinatown, reminding me that I've not danced with my hula class in a year. Many Asian American and Pacific Islander elders in my halau modeled aging with joy, smiling as they danced through aches and pains and weathered the loss of relatives and friends.\nPublic outings carry extra danger for Asian people, with a spate of recent attacks targeted against Asian elders that have resulted in racial trauma, injury, and death. In the Bay Area, viral videos showed the senseless killing of an 84-year-old Thai man named Vicha Ratanapakdee, thrown violently to the ground in San Francisco. Days later, another video surfaced of a 91-year-old man also shoved in Oakland's Chinatown. Other violence against Asian elders has involved robbery, such as the ambush of a 67-year-old man in San Francisco laundromat and 75-year-old Pak Ho robbed and killed in Oakland during his morning walk last week.\nVolunteer escorts have sprung up to assist Asian elders, along with other DIY community safety efforts such as hiring private armed security guards and providing shopkeepers with air horns. But they aren't a cure-all, and public safety depends on sustained government support and commitment.\nThese attacks haven't occurred in a vacuum but have intersected with rising violence and economic insecurity. Desolate cities and empty retail spaces from businesses that haven't survived the COVID pandemic mean fewer \"eyes on the street,\" increasing danger for everyone, but especially Asian elders who may be perceived as vulnerable. Decades of structural racism and community disinvestment have exacerbated the pandemic's acute economic devastation and increasing gun violence across the country.\nRussell Jeung, San Francisco State University professor and co-founder of Stop AAPI Hate, has pointed to community disinvestment in areas with Asian residents, \"We live in multiracial neighborhoods with high crime rates and in these neighborhoods everybody is attacked, everybody is vulnerable...on a day to day basis in these high crime neighborhoods, pretty much every racial group is a target. We need more opportunities and more resources in these under-resourced communities like mine.\"\n'Bullets flying by your house'\nReimagining community safety doesn't mean flooding streets with police officers. It does mean investing in jobs, educational opportunities, health care and affordable housing that make healthy communities, and programs to promote public safety such as non-police community ambassador programs and violence reduction efforts targeted towards young people.\nIn Oakland, pandemic closures of schools and community spaces have hampered community violence prevention programs. Recently, for the first time in a year I saw teenagers doing football drills on a gleaming new field at Fremont High School, benefiting the school's mostly Black and brown student body. I felt safer witnessing this small but important step forward.\n\"Even if you're not directly impacted, having bullets flying by your house is traumatic,\" says Loren Taylor, a city council member representing East Oakland. Last month I experienced this trauma during a quiet Sunday afternoon interrupted by the firecracker sound of spraying bullets across the street from my Oakland home. No one was harmed, but as I watched police officers place at least 10 yellow evidence markers near bullet casings, I worried about the Chinese grandmother who often sits with her grandson on the porch of their intergenerational home two doors down. Fortunately, she was safe.\nRacism and violence are twin public health crises that require a response as urgent as our investment in COVID vaccine development. Living in a fortress is not a solution, nor should we expect anyone to restrict their activities and movement due to fear and racism. In this tense time we mustn't lose sight of public acts of interracial solidarity, such as rallies against anti-Asian violence. We have a collective interest in safe public spaces and communities free from racial harm.\nWhen vandals destroyed treasured cherry blossom trees outside San Francisco's Japanese Cultural Center in January, I reflected on how much we have already lost to hate. The community raised funds for replacements, but unless we root out the seeds of hate, we will be replanting trees and mourning lives needlessly lost for generations to come."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194244, Requested 6850. Please try again in 328ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194244, Requested 6850. Please try again in 328ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "uen, he is scared to leave his house. In frail health, he knows he cannot defend himself. \n\"If people beat me, I can die right away,\" said Nguyen. \nHe tells his wife to come straight home from work and not run unnecessary errands. He also advises her to wear a mask and a hat. \"Don't let people recognize you as Vietnamese,\" he said. \nWhen Nguyen learned about the beating of an 83-year-old Vietnamese immigrant in San Francisco that took place a day after the Atlanta shootings, he realized he knew the victim, Ngoc Pham, who was his supervisor on the police force in Vietnam. Nguyen still had Pham's number and reached out to check on him after an image of his badly bruised face was splayed over the Internet. \nPham told him he is going to be OK, but Nguyen remains shaken. \"Oh my God, maybe me later, next time,\" he said. \nPresident Biden, through an executive order in January, has taken aim at anti-Asian hate and has directed the Department of Justice to make it one of its highest priorities. Last fall the US attorney's office in Massachusetts created a civil rights task force, and much of its current focus is on preventing hate crimes against the Asian American community, according to a spokeswoman. \nPolice departments in Boston and Lowell say that since the Atlanta killings, they have stepped up patrols, from Chinatown to Fields Corner, from Cambodia Town to the Buddhist temples in Lowell. They've also made sure bilingual officers are working those beats. \n\"We understand a lot of these crimes may go unreported,\" said Boston police Deputy Superintendent James Chin. \nChin said Boston police have been working closely with Asian American groups to encourage reporting of incidents of any kind. The department is also translating its brochure on civil rights into multiple languages, including Chinese and Vietnamese. \nOne of the biggest challenges is that undocumented immigrants fear deportation if they deal with the police. \n\"We don't care about their status,\" said Chin. \"Don't be afraid to report.\" \nChin grew up in Chinatown, and his parents are in their 80s. For him, this wave of anti-Asian sentiment \"hits home a little doser.\" \n\"We are not like New York or San Francisco,\" said Chin, but \"everyone is on edge. ... We want to be vigilant.\" \nThe Center for the Study of Hate and Extremism at California State University-San Bernardinohas been tracking decades of hate crime data. These acts tend to be vastly underreported, especially among Asian Americans, said the center's director, Brian Levin. \n\"This data does not reflect by any means the cumulative impact of actual hate crimes directed against Asian Americans,\" said Levin, a professor of criminal justice. \"What it is effective at is showing the trends, locations, targeting, and timing.\" \nLevin has found that Boston, New York, and Los Angeles are often a bellwether for hate crime for the rest of the country. What is troubling about the current climate of anti-Asian bias crimes - beyond an overall increase is that the cases have been more violent. \n \n \nOne way to protect Asian Americans is strengthening hate crimes laws. Representative Grace Meng, a New York Democrat, has introduced legislation calling for greater federal oversight of COVID-19-related hate crimes and requiring the Justice Department to provide Congress with regular updates about bias incidents. In Massachusetts, Long Nguyen's daughter, state representative Tram Nguyen, is co-sponsoring a bill that would bolster the Commonwealth's hate crime statute. \nWhile some Asian American groups don't agree that legislative fixes are the best way to fight bias, Nguyen, along with Attorney General Maura Healey, sees updating the law as a way to increase accountability in the criminal justice system. \n\"It is important to name hate crimes for what they are. They are meant to terrorize communities,\" said Nguyen, a Democrat from Andover. \"This bill is not going to end hate and violence. It will allow justice to be served.\" \nShen, the woman who was shoved to the ground in Medford, still does not feel comfortable reporting the assault to the police. \nLieutenant Detective Paul Covino of the Medford police said he understands how some victims are wary of law enforcement, and so encourages Shen's friends and support network to report the incident on her behalf. \n\"We're here to help her, not to hurt her, or make her look bad,\" said Covino. \"If we have no one reporting to us there are Asian hate crimes going on, we assume there are none going on.\" \nAsian Americans are nearly 10 percent of Medford's 57,000 residents. Like Lowell and Quincy, Medford police have not received reports of anti-Asian hate crime over the past year. \nAre we to believe Massachusetts is largely insulated from anti-Asian hate? Or are our society and systems so broken we have lost our way? No community should live in fear, and it's time we fix this."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193853, Requested 7946. Please try again in 539ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193853, Requested 7946. Please try again in 539ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A few weeks ago, Y. W. Shen, a petite Chinese American woman in her 70s, was on her daily walk in Medford when out of nowhere she was shoved to the ground by a stranger who muttered, \"Chink bitch,\" before sprinting off. \nThe incident took place in broad daylight near a busy playground, but no one offered to help her as she sat on the curb, clutching a tissue to her bleeding head, Shen said. Doctors at the emergency room later told her she likely suffered a concussion. \nRecalling the attack later, Shen said she felt like the 65-year-old Filipino woman in New York City whose brutal beating just one day before hers was caught on video as security guards in a nearby building watched and locked the doors. \n\"People usually jump to help someone if they have fallen down,\" said Shen. \"It's as if we are not seen as really belonging to American society, so it feels OK to treat us differently.\" \nShen had a similar experience a year ago in March, at the beginning of the pandemic. She was crossing at an intersection when two white men in a SUV drove right up to her, spitting and screaming: \"Virus, bitch!\" \nThat time, Shen managed to angrily yell back a profanity. \nAs infuriating as the incident was, Shen said what made her even more mad was this: The racist assault took place in front of a bus stop in Medford where a dozen people were waiting. \n\"Nobody did anything,\" Shen recalled. (To protect Shen's identity, the Globe is using her Chinese name.) \nShen's experiences were never captured in police logs because she, like many Asian Americans, are reluctant to report a hate crime. For some, it is a language barrier, while others fear retaliation, or, as in the case of Shen, they simply don't trust law enforcement. \nFor these reasons, the incidents that are reported plainly represent a fraction of the actual number. An organization called Stop AAPI Hate has tried to quantify the real number by collecting reports from the Asian American community, and identified 3,795 incidents across the country over roughly the first year of the pandemic. \nBut here in Massachusetts, police departments in Lowell and Quincy, cities with sizable Asian American populations, say they do not have a single report of a hate crime against Asian Americans over the past year. Boston and other major cities, however, have seen a sharp rise in anti-Asian hate crime since 2019, even as the overall numbers of hate crimes have declined. \nBoston recorded 14 anti-Asian incidents - primarily verbal assaults in 2020, up from eight in 2019. Already in 2021, the city has eight reported cases, according to the Boston Police Department. That compares to 16 reports of hate crimes against Black people and one against a Hispanic person this year. \nAnti-Asian hate has been surging as some people, looking for a scapegoat or blinded by bigotry, blame COVID-19 on China. Elderly Asian Americans, in particular, feel under attack after a spate of incidents in New York and Califomia were caught on video and went viral. One 84-year-old Thai immigrant died in January after being barreled to the ground outside his San Francisco home. \nLocal Asian American groups say they are unaware of similar assaults against seniors. But the March 16 mass shooting in the Atlanta area that killed eight-including six Asian American women is heightening fears of potential violence spreading here. \n \nAll of which raises the question: What kind of society are we when senior citizens are picked off for blood sport? For many Asian Americans, myself included, this crosses a line. We are a culture that fiercely values those of the older generation who made sacrifices to allow their children to have a better life in America. To attack them is to attack our very essence. To have to tell our parents and grandparents to watch their backs is heartbreaking. I am certain if white seniors had been the victims, this country would have declared a national emergency by now. \nHow J.H. Tang, 72, lives her life is the new normal for many older Asian Americans. The South End resident avoids going out alone, and, when she steps outside, she takes a good look around. \n\"I never really felt this kind of fear,\" Tang said in Cantonese, speaking through a translator. \"I feel like the Chinese are especially being targeted. We are treated like enemies, like we all have the virus.\" \nHer son recently bought her pepper spray, but the grandmother of five wasn't sure what to do with it. \n\"I am afraid of using it,\" Tang said. \"What if they have a weapon? I don't want to escalate the situation.\" \nVivian Tseng, a 69-year-old retired corporate lawyer in Concord, had a completely different response after the Atlanta murders: She decided to learn how to shoot a gun. Like many Asian Americans, Tseng has been feeling invisible and helpless after a year of growing anti-Asian sentiment. \n\"It's driven by rage and a sense of aloneness,\" Tseng said. \"If I don't do something for myself, nobody is going to save me.\" \nLong Nguyen, the father of state Representative Tram Nguyen, was an undercover police officer in Vietnam. But now, at age 67 and living in Methuen, he is scared to leave his house. In frail health, he knows he cannot defend himself. \n\"If people beat me, I can die right away,\" said Nguyen. \nHe tells his wife to come straight home from work and not run unnecessary errands. He also advises her to wear a mask and a hat. \"Don't let people recognize you as Vietnamese, he said. \nWhen Nguyen learned about the beating of a 83-year-old Vietnamese immigrant in San Francisco"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194482, Requested 8059. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194482, Requested 8059. Please try again in 762ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Felicity Tao received more media requests in the last few days than she has in the last year. The requests all looked the same, and they would be familiar to any minority community dealing with tragedy.\nThe reporters she talked to almost seemed to follow a script  a script in search of pain. And when they couldn't find that, or her pain wasn't painful enough, they moved on.\nTao was being asked these questions because eight people had been killed in a series of shootings in and around Atlanta on Tuesday night. Six of the victims were women of Asian descent.\nIt was the kind of shocking violence that grabbed the nation's attention. Finally, America could see the kind of Asian racism that is often hidden away or underreported, even as former President Donald Trump seemed to revel in calling COVID-19 the \"Chinese virus.\"\nBut even with eight people dead, there was debate about whether the attack was a hate crime. About whether racism was the primary motive. About whether America was finally ready to stop Asian hate.\nTao was a journalist before immigrating to America, and she knows the truth is more complex than that. The truth is Tao was in San Francisco for her 20th wedding anniversary a few years ago when a woman kicked her luggage and told her to go back to her country.\nShe hasn't experienced overt racism like that in Cincinnati. But the truth is she often feels invisible here, which takes its own heavy toll. Because there is no Chinatown here, and stories of prejudice against Asian people often go untold.\nTao is a communications director for the Greater Cincinnati Chinese School in Butler County, and she founded the Greater Cincinnati Chinese Cultural Exchange Association. She is an advocate who knows Asian people are underrepresented in Cincinnati media, including The Enquirer.\nShe knows this because she is the one sending press releases that often go unanswered. And after the devastating attacks in Atlanta, the intense spotlight on Asian Americans only reinforces the lack of attention they are normally paid.\n\"We don't exist,\" Tao said.\n'People are scared'\nAcross the street from a sports bar in Silverton, the sign hangs in all capital letters: WANGLAW. In the window below, there is a poster: \"English is spoken here.\"\nStanding in the lobby looking out at the window, Charleston Wang says the poster helps keep white nationalists away. He laughs, but it's hard to tell if he's joking.\nAfter watching the news conferences about what happened on Tuesday, Wang looked up the definition of sex addiction. He did this because he felt confused and offended and sad as he watched a sheriff's spokesperson in Georgia answer questions about a possible motive for the attacks.\nThe suspect had denied any racial motivation, police said, and blamed the violence on his sexual addiction.\n\"Yesterday was a really bad day for him,\" the white police official said about the white suspect. \"And this is what he did.\"\nWang recites the deputy's name, Capt. Jay Baker, along with other details of the shootings almost 500 miles away. He slaps his hands on the maroon leather couch in his office.\n\"People are scared,\" he said.\nWang didn't know these women, but he has clients like them. Wang is an immigration lawyer who has represented dozens of spa workers. He's represented them after they've been sued for allegedly causing injury. He's represented them after they've fled their country. He's represented them trying to get married. And he's represented them after police stings.\nIn most cases, Wang said, they're vulnerable targets.\nHe throws his hands around, hitting the couch again. He's stressed. He's mad. He's a lot of things.\nHe is 65, and his hair is colored orange. On one side of the couch is a bowl of fake fruit, grapes and strawberries. On the other side is a large American flag.\nWang was born in Taiwan and lived in Malaysia before he came to America when he was 16. When he graduated from college, and his hair reached beyond his shoulders, his diploma credited Cheng-Kung Wang. When he passed the bar exam a few years later, he added another name to the certificate, which is framed in his office.\nCharleston.\nHe picked the name out of a hat.\nThis week, when Wang looked up the definition of sex addiction, he found research saying it is not an established psychiatric diagnosis. He found the American Psychiatric Association removed sex addiction from its guidebook of mental disorders in 2012.\nAnd as he watched the discussion surrounding the Atlanta killings, he couldn't help but think the deaths were being minimized. He couldn't help but feel like the victims were being blamed.\nIt hurt him, even though Wang doesn't feel persecuted in Cincinnati. Because the man with orange hair says he often feels invisible too.\n'Where are you really from?'\nFelicity Tao came to America when she was 25.\nShe left China with her husband, Mingyi Weng, to attend grad school. Her university was in a cornfield somewhere in Illinois. He was in Cincinnati. She always planned to return to China, and she did. She thought her husband would join her a few years later.\nBut he didn't want to leave. He loved Cincinnati. Compared to where they lived, Cincinnati was quiet.\nHe convinced her to come back. To leave their parents behind and start over. Because he felt this was the best place to raise their children.\nThere are times now, especially last year, when her husband felt like they made the wrong decision. Today, they both realize the American ideals they once believed in are more dreams than realities. The America they fantasized about, the one they saw in Hollywood movies, doesn't exist.\nThis is an America where, in Cincinnati, owners of a Chinese restaurant say their homes and cars have been egged. Where the restaurant receives calls daily telling them to \"go back to China.\" Where staff"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195159, Requested 7894. Please try again in 915ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195159, Requested 7894. Please try again in 915ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK  Police on Wednesday said a man suspected of brutally assaulting an Asian American woman in broad daylight was arrested after surveillance video of the attack drew widespread condemnation.\nIn a court appearance late Wednesday Brandon Elliot 38 of New York City was remanded into custody. His next court date is set for Monday. He was charged with two counts of assault in the second degree as a hate crime and one count of attempted assault in the first degree as a hate crime the Manhattan District Attorney's Office said.\nThe Legal Aid Society which is representing Elliot said in a statement: \"We strongly urge the public to reserve judgment until all the facts are presented in court.\"\nManhattan District Attorney Cyrus Vance described the incident as a \"brutal attack.\"\n\"There is no place for these atrocious acts of violence in New York\" Vance said in a statement.\nThe attack occurred before noon Monday outside a luxury apartment building in Midtown Manhattan. The suspect hurled anti-Asian sentiments and assaulted the 65-year-old woman New York Police Department said.\n'We need to make the most of this moment': Asian American activists are demanding equal civil rights better education in schools after Asian hate attacks\nSurveillance video from inside the building captured images of a man kicking and stomping the woman repeatedly. In a short version of the video staff do not immediately appear to intervene but a longer video shared with local media shows building staff aiding the woman after the suspect flees.\nThe woman was hospitalized with serious injuries but released Tuesday. Vance's office said that the woman was diagnosed with diagnosed with a fractured pelvis forehead contusions and contusions across her body.\nThe woman's daughter speaking with The New York Times identified her as Vilma Kari and said her mother emigrated from the Philippines. The Times did not identify the daughter.\nIn a GoFundMe Elizabeth Kari identifies herself as Vilma Kari's daughter and said her mother has been \"humbled by the outpouring of messages and support from not only our friends and family but from the kind souls all over the world. I am happy to share she is safe and in good spirits.\" The GoFundMe has raised more than $76000 as of Thursday afternoon.\nOn the GoFundMe page Kari wrote that someone across the street who has not been identified shouted at her mother's assailant to get his attention and draw him away. \"This gesture of action is what we need in our world right now. I hope one day my mom and I can thank you personally\" she wrote.\nPolice said the woman was walking to church when she was attacked. The man shouted anti-Asian slurs at her and told her \"You dont belong here\" police said.\nLet me be clear: this brave woman belongs here. Asian-American New Yorkers belong here. Everyone belongs here. Attacks against Asian-American New Yorkers are attacks against all New Yorkers and my Office will continue to stand against hate in all its forms\" Vance said in a statement.\nElliot was living in a hotel nearby that serves as a homeless shelter police said. Elliot was convicted of stabbing his mother to death in the Bronx in 2002 when he was 19 online court records show. He was released from prison in 2019 and is on lifetime parole.\nFor the life of me I dont understand why we are releasing or pushing people out of prison  not to give them second chances but to put them into homeless facilities or shelters or in this case a hotel  and expect good outcomes Police Commissioner Dermot Shea said at a news conference Wednesday. We need real opportunities. We need real safety nets.\nOfficials in New York City and beyond swiftly condemned the attack and highlighted how it was another case in a growing number of incidents of anti-Asian hate and discrimination amid the COVID-19 pandemic.\nMore than 3795 incidents were reported to Stop AAPI Hate from March 19 2020 to Feb. 28 2021 only a fraction of the number of hate incidents that actually occur the advocacy group said. The organization tracks incidents of hate violence harassment and discrimination against Asian Americans and Pacific Islanders in the U.S.\nMayor Bill de Blasio called the incident absolutely disgusting and outrageous and said it was absolutely unacceptable that witnesses did not intervene.\nIn a statement the head of the union representing the building workers disputed the allegation that workers did not act.\nOur union is working to get further details for a more complete account and urges the public to avoid a rush to judgment while the facts are determined union president Kyle Bragg said.\nThe union initially said the workers immediately called for help but on Wednesday said they waited until the attacker left to aid the woman and flag a police car.\nA video released to PIX11 shows staff waiting until the attacker appears to leave the area before going outside to assist the woman. The TV station reported the workers waited because they believed the suspect had a knife.\nDetective Michael Rodriguez said Wednesday that patrol officers came upon the victim after she was assaulted.\nThe management company of the apartment building where the attack occurred said Monday that it had suspended the staff members pending an investigation.\n\"The Brodsky Organization condemns all forms of discrimination racism xenophobia and violence against the Asian American community\" the company said.\nThe victim could easily have been my mother said Andrew Yang a mayoral candidate and the son of Taiwanese immigrants. Yang also called bystanders' apparent inaction \"exactly the opposite of what we need here in New York City.\"\nGov. Andrew Cuomo also weighed in calling the attack \"horrifying and repugnant.\"\nPolice sought the public's help in the case and have said they were increasing patrols in predominantly Asian communities amid the spike in incidents. In New York City there have been 33 hate crimes with an Asian victim as of Sunday compared with 11 as of this time last year the NYPD said.\nIn Washington on Tuesday Attorney General Merrick Garland announced a 30-day review to assess the"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198273, Requested 6911. Please try again in 1.555s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198273, Requested 6911. Please try again in 1.555s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Hundreds of people huddled with signs reading \"#StopAsianHate\" on Saturday afternoon steps from Chinatown Square in Chicago's latest protest against anti-Asian racism after the shooting spree across Atlanta-area spas this month that left eight dead, six of them women of Asian descent. \nThe 2 p.m. rally was led by the Chinatown Security Foundation and the Coalition for a Better Chinese American Community, with more than 65 other Asian American organizations in attendance, according to an event summary.\nIt follows other protests over the past week that also drew hundreds in Chicago to speak out against the recent surge of violence and racism targeting Asian Americans, including a Friday night vigil at Horner Park.\nJulie Moy, an 18-year-old from McKinley Park, had brought her 15-year-old cousin to the outdoor mall in the plaza that morning when they noticed a sign advertising the rally plastered on a restaurant. They walked over to the tables set up on the other side of the \"Chinese in America\" mural and grabbed some posters.\n\"We decided to join because there aren't many rallies against Asian hate, and it just needs to be more of a thing,\" Moy said.\nHer younger cousin Jason added, \"My side of the family is actually from Chinatown, so I feel like being Chinese is deeply rooted in who I am. I feel that it's important to show our support, especially since what happened to the people in Georgia.\"\nThe March 16 massacre in Georgia deepened an old wound for the Asian American community already grappling with a year of fear following the coronavirus pandemic, which many felt caused them to be scapegoated. In recent months, there has also been a wave of reported violence against Asian Americans across the U.S., including an alleged attack last weekend in Chicago that remains under police investigation.\nThe rally began with an acknowledgment of three Asian American men who were killed in Chicago last year. They were Weizhong Xiong, 38, and Huayi Bian, 37, who both were fatally shot in a Chinatown parking lot in February 2020, and Shuai Guan, 33, who was killed in a shooting in Bridgeport last December. Their deaths came during a year of skyrocketing crime in the city.\nChris Huang, president of the Chinatown Security Foundation, discussed plans to add more security cameras to the neighborhood but said the United States's deep-rooted prejudice against Asians cannot be solved by that alone.\n\"This attitude of Asians being looked down on, taken advantage of, treated unfairly have been here since my grandfather came to work on the railroad in the early 1900s,\" Huang said.\nGrace Chan McKibben, executive director of CBCAC, said Saturday's protest was part of a \"growing movement of Asian Americans demanding that our voices be heard.\" The Georgia shootings have only \"heightened the nation's awareness of the racism and sexism that our community has been all too familiar with,\" she added.\nAARP Illinois executive council member Nancy Chen led the crowd in a chant of \"enough is enough\" after sharing her personal fears as older Asian Americans face heightened vulnerability in the recent violence.\n\"I'm a 76-year-old grandmother,\" Chen said. \"I know exactly how our elderly folks in our community feel. ... We should not have to be worried about walking on the street in our neighborhood and worry about being attacked and killed.\"\nNationwide, anti-Asian hate crime reports surged by 149% last year, according to the Center for the Study of Hate and Extremism at California State University's San Bernardino campus. That is despite a 7% drop overall in 2020 hate crime reports.\nAnd a national coalition of Asian American organizations collected 3,795 reports on anti-Asian racism from March 2020 to February, finding almost 70% of them came from Asian women.\nThe organizers of the rally presented five demands: increase Chinatown safety; respond to anti-Asian hate crimes with timeliness and cultural sensitivity; create a website to report such attacks; pass an Illinois bill to mandate teaching Asian American history in public schools; and fund Asian American community organizations.\nIllinois Department of Human Services Secretary Grace Hou, Cook County State's Attorney Kim Foxx and Cook County Board President Toni Preckwinkle attended, as well as multiple lawmakers from Illinois' congressional delegation and statehouse.\nFoxx, who in her remarks dinged former President Donald Trump for what she said was espousing \"white supremacy\" during the COVID-19 pandemic, noted her office's role in prosecuting suspects in the three homicides in Chinatown and Bridgeport.\n\"Though the pandemic has slowed down our court systems, it has not slowed down our quest for justice,\" Foxx said. \"Those cases are still pending. We will seek justice and accountability.\"\nMoy and Jason, the two cousins from McKinley Park and Chinatown who joined the rally, said the rally surprised them with the large crowds spilling onto the second floor of the mall and the racial diversity among its attendees.\n\"I wasn't really expecting this,\" Jason said. \"I feel like very proud and happy to see such a great turnout for the rally.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197814, Requested 6499. Please try again in 1.293s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197814, Requested 6499. Please try again in 1.293s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Black church leaders to discuss the common issues they face and how to combine forces.\nThe Christian message is one that brings people who are divided together Chang said. Thats the whole message of reconciliation. But in the U.S. because our churches were established on top of a segregated society shaped by white supremacy they've never found ways to meaningfully interact with each other.\nIn calling for systemic change and organizing around public hate crimes and deaths Asian American leaders said theyre taking cues from the Movement for Black Lives coalition as well as from Muslim and Asian activists who dealt with Islamophobia after 9/11 and previous generations of community leaders who sought justice for Chin after his killing.\nUnderstanding how other communities have achieved political power is always important said Mielke of the Asian Pacific American Institute for Congressional Studies.\nWe dont do this alone\" she said. \"When we talk about what is needed its community-based and whatever work were doing to prevent this from happening to the Asian American community were also doing to prevent it from happening to any community.\nGeorgia state senator Michelle Au who introduced bills prompted by the Atlanta killings to mandate gun safety and language-specific social services said there's an urgent need for action.\n\"This is the best time to take this energy and attention and turn it into something good\" she said."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197416, Requested 8172. Please try again in 1.676s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197416, Requested 8172. Please try again in 1.676s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Growing up U.S. Rep. Grace Meng remembers the slurs and name-calling she and her fellow Asian Americans occasionally endured on the playgrounds of New York.\nIt was just something we grew up with said Meng who is now in her 40s. We were taught to mind our own business not to rock the boat. But whats changed for my generation  even before the tragedy in Atlanta  is that people like me were starting to see people who look like their fathers and mothers and grandfathers getting beaten up. That really struck a nerve.\nAcross the nation such attacks part of a rising wave of anti-Asian incidents over the past year have shocked many Asian Americans. The March 16 slaying of eight people at three Atlanta spas six of them Asian women has further sparked both a sense of heightened activism from within the Asian American community and broad-based support from beyond.\nThe moment seems rich with opportunity. Whats to be done with this solidarity? For Asian American community leaders and activists the answers range from creating better ballot access and greater political representation expanding Asian American history instruction in schools and emboldening activist participation from untapped groups such as youth and the greater religious community. \nIts really important and meaningful that we have had such widespread support from all over the country Meng said. As an Asian American born and raised here I have never felt that in my entire life. We need to make the most of this moment.\nAnti-Asian sentiment has grown significantly since the start of the COVID-19 pandemic with many in the community citing the disparaging rhetoric of the Trump administration as a factor. San Francisco-based Stop AAPI Hate which tracks discrimination and xenophobia against Asian Americans and Pacific Islanders tallied nearly 3800 such incidents from March 2020 through February 2021.\nMore recently results of an annual survey conducted by the Anti-Defamation League showed that Asian Americans had suffered the largest spike in severe incidents of hate and harassment online.\nThroughout the United States and in Canada this weekend #StopAsianHate marches were scheduled as a response to such sentiments in places like Princeton New Jersey; Buffalo New York; Portland Maine; and Calgary Alberta.\nThe activism extends beyond the streets. Illinois Sen. Tammy Duckworth threatened Tuesday to vote against white nominees to President Joe Bidens administration until more Asian Americans were appointed to high-ranking roles then withdrew that threat after she received assurances the White House would do better. While Vice President Kamala Harris is of Indian descent there are no Cabinet secretaries of Asian American or Pacific Islander descent in Biden's administration despite the president's pledge to reflect the nation's diversity.\nThis weekend U.S. Rep. Hank Johnson of Georgia a member of the Congressional Black Caucus was set to host a virtual conversation for the public on anti-Asian discrimination and violence with Meng and U.S. Rep. Judy Chu of California. The event will be broadcast on Zoom and on Johnson's Facebook page.\nOne of the interesting things Ive been hearing is that this is the first time that Asian Americans have being asked to share their stories in their workplaces said Aarti Kohli executive director of Asian Americans Advancing Justice-Asian Law Caucus a national legal advocacy group. And people are often surprised to hear the racism that their colleagues have faced. So I'm seeing a much broader recognition of the racism that has been aimed at our community.\nFrank Wu president of Queens College City University of New York compares the moment to the mood following the 1982 killing in Detroit of Vincent Chin a Chinese American mistaken for Japanese by two struggling auto workers who beat him to death with a baseball bat. The two were eventually fined $3000 and sentenced to probation.\nThe resulting outrage and subsequent sense of solidarity Wu said crossed lines of ethnicity generation language and class and prompted renewed Asian American civil-rights activism. But like all movements it eventually lost momentum.\nSo while many Black Latino and Jewish leaders and colleagues have reached out to him in unprecedented partnership since the Atlanta killings its crucial Wu said to capitalize on that unity while it lasts.\nOut of this tragedy he said there is something I always hoped for but hadnt seen until now: Real bridge-building intentions.\"Stopping Asian hate with data and education\nMaking the most of the current energy was the thinking behind a National Day of Action and Healing a virtual conversation conducted by Chu Friday with fellow legislators activists and victims of anti-Asian attacks.\nWe wanted to give people a tool to share with their co-workers their bosses their neighbors Meng said. Were hopeful it can be a spark for creating long-term partnerships. Thats the immediate next step  to have this continue.\nAmong the long-term solutions Meng said shed like to see is for Americans to better understand each others histories and contributions  with public education being one way to do that.\nThink about what we learned in school about the contributions of Asian Americans to American history she said. Just a paragraph. I think we can make the most of this moment to expand the curriculum were teaching our kids.\nStop AAPI Hate has likewise advocated for ethnic studies curricula as a means to curtail bullying as well as community-based violence protection programs to protect the elderly and the expansion of civil rights protections to end harassment in business.\nI look forward to seeing this movement continue to grow said Russell Jeung the groups co-founder and professor of Asian American Studies at San Francisco State University.\nChu is also among those pushing two hate-related bills for Congressional approval the No Hate Act and the COVID-19 Hate Crimes Act both meant to improve tracking of hate crimes.\nThese are things that should have been improved a long time ago Chu said noting that the FBI relies on individual states to submit their hate-crime data which means that many dont report anything. Eighteen states dont have a mandate and three states dont even have a hate crime statute. We need to have change there on a national basis."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196355, Requested 6901. Please try again in 976ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196355, Requested 6901. Please try again in 976ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "New analysis of anti-Asian hate crime statistics reveals a majority of perpetrators are White, NBC News reports. \nJanelle Wong, a professor of American Studies at the University of Maryland, College Park examined nine sources and four different data states relating \nto anti-Asian hate crimes. She looked at data from Stop AAPI Hate, Pew Research and official law enforcement statistics. \nWong's analysis revealed that more than three-quarters of perpetrators of anti-Asian hate crimes and incidents, before and during the pandemic, were White. \nUnfortunately, news reports and social media often encourage the idea that most anti-Asian violence and discrimination is committed by people of color, NBC News reports. \n\"This is really how crime is framed in the United States - it's framed as the source is Black,\" Wong told NBC News. \nWong says a misreading of a study from the American Journal of Criminal Justice, which examined anti-Asian hate crimes and incidents from 1994-2014, may have also contributed to misplaced blame. The study found that compared to anti-Black and anti-Latino hate crimes, a higher proportion of perpetrators of anti-Asian hate crimes were people of color. \nUltimately, however, the study found that more than three-quarters of the perpetrators were White. \nThe misconceptions perpetuate harmful racial stereotypes, specifically anti-Blackness, NBC News reports. Yet, Wong pointed out that despite the rise in anti-Asian hate crimes, Black people are the most targeted racial group. \nWong added that the misconceptions can also hurt opportunities for racial solidarity. Most anti-Asian attackers are not people of color, however, research suggests that most people of color are concerned about the rise in anti-Asian hate. \nA poll conducted by the Associated Press-NORC Center for Public Affairs Research found that 85 percent of Black Americans and 82 percent of Hispanic Americans were at least somewhat concerned about the violence against Asians had increased during the pandemic. Only 74 percent of White Americans were at least somewhat concerned. \nWithin those groups, 50 percent of Black Americans, 47 percent of Hispanic Americans and 41 percent of White Americans were extremely concerned."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197378, Requested 7763. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197378, Requested 7763. Please try again in 1.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Angry. Hurt. Shocked. \nThose are some of the emotions that have racked Asian Americans in North Jersey after witnessing unprovoked race-based attacks as Asians have been scapegoated for the COVID-19 pandemic that originated in Wuhan, China. \nBut Asian Americans are harnessing that emotion and using it to fuel a mass movement designed to stem the tide of hate against Asians. In cities across the United States in recent weeks, tens of thousands of people took to the streets to demonstrate. \nMore than 150 years after Asians first settled in America, Asian Americans are for the first time mobilizing en masse in a civil rights movement. The recent attacks have prompted many to speak out for the first time, shedding cultural norms. \nNorth Jersey is home to some of the largest Asian American communities in the United States. More than 40% of residents in boroughs such as Fort Lee and Leonia identify as Asian, as do nearly 60% of residents in Palisades Park. In recent weeks they took to the streets carrying posters and signs that read \"Stop Asian Hate\" and \"Hate is a Virus.\" \nA new generation of North Jersey Asian Americans is spearheading this movement, having grown up in the Garden State feeling isolated. These people have faced racism, but until now brushed it off instead of calling it out. \nBelow, some of these new leaders share their stories. \nBrian Jon \nBrian Jon could not believe what he heard. \nAs a freshman at Bergen County Technical Schools in 2017, he learned of an incident at a sister school, Bergen County Academies, in which a Spanish language teacher allegedly announced during class that she hated Korean students. Outraged, he started a petition drive and gathered more than 1,500 signatures to successfully oust the teacher from the school. \nThat experience led Jon to further activism. An immigrant who arrived from South Korea at the age of 6 with his mother Ellen Jeon, life has not been easy for the 19-year- old. \n\"The only thing I knew how to say was hello,\" Jon recalled of his arrival in New Jersey. \nHe attended schools in Tenafly, raised by his busy single mom. Before long, other kids introduced him to sports such as wrestling, football and swimming. \nThough he assimilated, Jon remains very aware of his Korean American identity. \n\"It is our job to be aware of anti-Asian bias,\" Jon said. \"Our parents' generation, Asians are often taught to be very mannered, to be respectful of our elders so we don't really raise our voices and we normally keep ourselves very shut. We suppress our feelings.\" \nIn January 2020, Jon launched an organization for like-minded young Asian activists called Asian American Youth Council. There are over 50 student members of the organization advocating for Asian American issues. \n\"It's very crucial for us second generation Asian Americans speak up for our elders,\" he said, noting the spate of hate crimes. \"It is a mandatory thing now.\" \nCecilla Chan and Mia Hur \nWatching news reports about the Atlanta massage spa shootings, Cecilia Chan of Tenafly couldn't sleep. She was convinced it was a case of Asian fetishization, where men fantasize about Asian women in a deviant nature. \n\"I was outraged when media tried to spin the narrative of sexual addiction,\" said Chan, who has been victimized by Asian fetishization stereotypes. \"I was completely outraged. It just hit home on so many levels.\" \nMeanwhile, Tenafly resident Mia Hur, a Realtor, purse designer and mother of a teenage son, was also upset about the shootings and that the suspected gunmen was normalized in the media by explaining he had a sexual addiction. She knew she needed to do something. \nSimultaneously, Chan and Hur reached out to elected officials, who asked them to organize. In three days, the duo put together a rally calling for an end to anti-Asian \nhate. \n\"I felt hurt. I felt outraged. I felt angry,\" Chan said. \nHur echoed Chan's sentiments, explaining the current mood of many Asian Americans. \n\"A lot of people probably don't understand why it's a big deal to the Asian American community,\" Hur said of the Atlanta shootings. \"It just perpetuates stereotypes, gaslighting our culture.\" \nChan and Hur connected over shared experiences and emotions, organizing the first anti-Asian racism rally in North Jersey after the Atlanta killings. It was held in Tenafly on March 21 and drew more than 800 people. More than a half dozen vigils followed throughout North Jersey. \nThe recent spate of attacks against Asian Americans is opening old wounds for Chan and Hur. The nonprofit group Stop AAPI Hate reported 3,795 anti-Asian incidents since the start of the COVID pandemic, with Asians scapegoated for the coronavirus. \nNow 47, Hur arrived in the United States from South Korea when she was 6. Back then, hers was among a handful of Asian families in Tenafly. \n\"It was isolating,\" Hur recalled. \"That really made me aware of my identity.\" \nHer parents were busy working as merchants in New York City, so she had to grow up fast. The community of Tenafly was kind and generous to her in her youth, with families inviting her to places and events. Because of its diversity, she and her husband chose to stay in Tenafly to raise their son, now 13. \nBorn in New York City to Chinese immigrant parents who owned Chinese restaurants, Chan, 44, describes herself as a recovering lawyer who is now a stay-at-home mom to two children. Her family lived in Westchester County, New York before moving to Cresskill, where she sometimes felt boxed in. \nChan and Hur both understand the Asian immigrant experience and that language and culture have"
    }
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196959, Requested 6056. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196959, Requested 6056. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195414, Requested 5900. Please try again in 394ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195414, Requested 5900. Please try again in 394ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192858, Requested 9181. Please try again in 611ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192858, Requested 9181. Please try again in 611ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198187, Requested 4415. Please try again in 780ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198187, Requested 4415. Please try again in 780ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196263, Requested 5377. Please try again in 492ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196263, Requested 5377. Please try again in 492ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198412, Requested 4582. Please try again in 898ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198412, Requested 4582. Please try again in 898ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197051, Requested 5203. Please try again in 676ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197051, Requested 5203. Please try again in 676ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197968, Requested 8172. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197968, Requested 8172. Please try again in 1.842s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195533, Requested 6859. Please try again in 717ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195533, Requested 6859. Please try again in 717ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195419, Requested 4890. Please try again in 92ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195419, Requested 4890. Please try again in 92ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199740, Requested 5097. Please try again in 1.451s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199740, Requested 5097. Please try again in 1.451s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199708, Requested 4346. Please try again in 1.216s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199708, Requested 4346. Please try again in 1.216s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199669, Requested 4888. Please try again in 1.367s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199669, Requested 4888. Please try again in 1.367s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199566, Requested 4675. Please try again in 1.272s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199566, Requested 4675. Please try again in 1.272s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199457, Requested 4812. Please try again in 1.28s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199457, Requested 4812. Please try again in 1.28s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199467, Requested 8172. Please try again in 2.291s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199467, Requested 8172. Please try again in 2.291s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198875, Requested 4394. Please try again in 980ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198875, Requested 4394. Please try again in 980ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198724, Requested 4637. Please try again in 1.008s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198724, Requested 4637. Please try again in 1.008s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198526, Requested 4651. Please try again in 953ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198526, Requested 4651. Please try again in 953ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198066, Requested 4352. Please try again in 725ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198066, Requested 4352. Please try again in 725ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197709, Requested 8167. Please try again in 1.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197709, Requested 8167. Please try again in 1.762s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197726, Requested 5006. Please try again in 819ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197726, Requested 5006. Please try again in 819ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197394, Requested 7327. Please try again in 1.416s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197394, Requested 7327. Please try again in 1.416s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197030, Requested 8172. Please try again in 1.56s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197030, Requested 8172. Please try again in 1.56s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196768, Requested 4492. Please try again in 378ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196768, Requested 4492. Please try again in 378ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196637, Requested 4642. Please try again in 383ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196637, Requested 4642. Please try again in 383ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196236, Requested 4857. Please try again in 327ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196236, Requested 4857. Please try again in 327ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196913, Requested 4838. Please try again in 525ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196913, Requested 4838. Please try again in 525ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196194, Requested 4946. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196194, Requested 4946. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196044, Requested 5192. Please try again in 370ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196044, Requested 5192. Please try again in 370ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199533, Requested 4639. Please try again in 1.251s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199533, Requested 4639. Please try again in 1.251s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199495, Requested 4741. Please try again in 1.27s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199495, Requested 4741. Please try again in 1.27s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198943, Requested 6422. Please try again in 1.609s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198943, Requested 6422. Please try again in 1.609s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196825, Requested 5670. Please try again in 748ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196825, Requested 5670. Please try again in 748ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196541, Requested 5551. Please try again in 627ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196541, Requested 5551. Please try again in 627ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199150, Requested 4960. Please try again in 1.233s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199150, Requested 4960. Please try again in 1.233s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199115, Requested 4672. Please try again in 1.136s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199115, Requested 4672. Please try again in 1.136s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198865, Requested 4529. Please try again in 1.018s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198865, Requested 4529. Please try again in 1.018s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198841, Requested 4682. Please try again in 1.056s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198841, Requested 4682. Please try again in 1.056s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198633, Requested 7119. Please try again in 1.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198633, Requested 7119. Please try again in 1.725s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198340, Requested 4399. Please try again in 821ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198340, Requested 4399. Please try again in 821ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198332, Requested 7017. Please try again in 1.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198332, Requested 7017. Please try again in 1.604s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198278, Requested 4536. Please try again in 844ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198278, Requested 4536. Please try again in 844ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198262, Requested 5006. Please try again in 980ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198262, Requested 5006. Please try again in 980ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198027, Requested 4634. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198027, Requested 4634. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196994, Requested 4634. Please try again in 488ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196994, Requested 4634. Please try again in 488ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195654, Requested 5114. Please try again in 230ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195654, Requested 5114. Please try again in 230ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195121, Requested 6081. Please try again in 360ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195121, Requested 6081. Please try again in 360ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198904, Requested 4454. Please try again in 1.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198904, Requested 4454. Please try again in 1.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198446, Requested 4437. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198446, Requested 4437. Please try again in 864ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198306, Requested 4428. Please try again in 820ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198306, Requested 4428. Please try again in 820ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197924, Requested 4482. Please try again in 721ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197924, Requested 4482. Please try again in 721ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197893, Requested 4625. Please try again in 755ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197893, Requested 4625. Please try again in 755ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197520, Requested 4627. Please try again in 644ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197520, Requested 4627. Please try again in 644ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196723, Requested 4483. Please try again in 361ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196723, Requested 4483. Please try again in 361ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196676, Requested 5437. Please try again in 633ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196676, Requested 5437. Please try again in 633ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196564, Requested 4405. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196564, Requested 4405. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195902, Requested 5191. Please try again in 327ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195902, Requested 5191. Please try again in 327ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195753, Requested 4375. Please try again in 38ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195753, Requested 4375. Please try again in 38ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199320, Requested 4389. Please try again in 1.112s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199320, Requested 4389. Please try again in 1.112s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199236, Requested 4329. Please try again in 1.069s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199236, Requested 4329. Please try again in 1.069s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197930, Requested 4745. Please try again in 802ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197930, Requested 4745. Please try again in 802ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195488, Requested 8172. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195488, Requested 8172. Please try again in 1.098s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196974, Requested 4951. Please try again in 577ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196974, Requested 4951. Please try again in 577ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195938, Requested 5972. Please try again in 573ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195938, Requested 5972. Please try again in 573ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195842, Requested 5718. Please try again in 468ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195842, Requested 5718. Please try again in 468ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196533, Requested 5631. Please try again in 649ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196533, Requested 5631. Please try again in 649ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197433, Requested 4657. Please try again in 627ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197433, Requested 4657. Please try again in 627ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196086, Requested 5299. Please try again in 415ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196086, Requested 5299. Please try again in 415ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195430, Requested 6679. Please try again in 632ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195430, Requested 6679. Please try again in 632ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195203, Requested 5456. Please try again in 197ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195203, Requested 5456. Please try again in 197ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199520, Requested 4791. Please try again in 1.293s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199520, Requested 4791. Please try again in 1.293s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199377, Requested 4338. Please try again in 1.114s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199377, Requested 4338. Please try again in 1.114s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199261, Requested 4604. Please try again in 1.159s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199261, Requested 4604. Please try again in 1.159s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199204, Requested 4671. Please try again in 1.162s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199204, Requested 4671. Please try again in 1.162s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199090, Requested 5150. Please try again in 1.272s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199090, Requested 5150. Please try again in 1.272s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198722, Requested 4800. Please try again in 1.056s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198722, Requested 4800. Please try again in 1.056s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198633, Requested 4832. Please try again in 1.039s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198633, Requested 4832. Please try again in 1.039s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198220, Requested 5521. Please try again in 1.122s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198220, Requested 5521. Please try again in 1.122s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197754, Requested 4692. Please try again in 733ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197754, Requested 4692. Please try again in 733ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194690, Requested 5726. Please try again in 124ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194690, Requested 5726. Please try again in 124ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199276, Requested 4909. Please try again in 1.255s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199276, Requested 4909. Please try again in 1.255s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198795, Requested 5438. Please try again in 1.269s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198795, Requested 5438. Please try again in 1.269s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198413, Requested 4986. Please try again in 1.019s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198413, Requested 4986. Please try again in 1.019s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196306, Requested 5167. Please try again in 441ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196306, Requested 5167. Please try again in 441ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195404, Requested 5594. Please try again in 299ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195404, Requested 5594. Please try again in 299ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195458, Requested 4665. Please try again in 36ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195458, Requested 4665. Please try again in 36ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199133, Requested 5171. Please try again in 1.291s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199133, Requested 5171. Please try again in 1.291s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196448, Requested 4956. Please try again in 421ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196448, Requested 4956. Please try again in 421ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195892, Requested 4659. Please try again in 165ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195892, Requested 4659. Please try again in 165ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195547, Requested 5648. Please try again in 358ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195547, Requested 5648. Please try again in 358ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195612, Requested 7190. Please try again in 840ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195612, Requested 7190. Please try again in 840ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195248, Requested 5134. Please try again in 114ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195248, Requested 5134. Please try again in 114ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199045, Requested 4782. Please try again in 1.148s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199045, Requested 4782. Please try again in 1.148s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198366, Requested 4805. Please try again in 951ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198366, Requested 4805. Please try again in 951ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199316, Requested 4757. Please try again in 1.221s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199316, Requested 4757. Please try again in 1.221s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197118, Requested 4678. Please try again in 538ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197118, Requested 4678. Please try again in 538ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197004, Requested 4394. Please try again in 419ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197004, Requested 4394. Please try again in 419ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197080, Requested 5558. Please try again in 791ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197080, Requested 5558. Please try again in 791ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197460, Requested 4838. Please try again in 689ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197460, Requested 4838. Please try again in 689ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195568, Requested 5012. Please try again in 174ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195568, Requested 5012. Please try again in 174ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195514, Requested 5434. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195514, Requested 5434. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195501, Requested 5059. Please try again in 168ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195501, Requested 5059. Please try again in 168ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196678, Requested 5843. Please try again in 756ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196678, Requested 5843. Please try again in 756ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194388, Requested 6205. Please try again in 177ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194388, Requested 6205. Please try again in 177ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199739, Requested 4499. Please try again in 1.271s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199739, Requested 4499. Please try again in 1.271s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196910, Requested 4717. Please try again in 488ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196910, Requested 4717. Please try again in 488ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198963, Requested 4680. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198963, Requested 4680. Please try again in 1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198882, Requested 4363. Please try again in 973ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198882, Requested 4363. Please try again in 973ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198602, Requested 5072. Please try again in 1.102s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198602, Requested 5072. Please try again in 1.102s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198597, Requested 4958. Please try again in 1.066s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198597, Requested 4958. Please try again in 1.066s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198482, Requested 6999. Please try again in 1.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198482, Requested 6999. Please try again in 1.644s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198094, Requested 6113. Please try again in 1.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198094, Requested 6113. Please try again in 1.262s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197486, Requested 4430. Please try again in 574ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197486, Requested 4430. Please try again in 574ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195769, Requested 8017. Please try again in 1.135s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195769, Requested 8017. Please try again in 1.135s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199107, Requested 6534. Please try again in 1.692s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199107, Requested 6534. Please try again in 1.692s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196799, Requested 5655. Please try again in 736ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196799, Requested 5655. Please try again in 736ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196736, Requested 6490. Please try again in 967ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196736, Requested 6490. Please try again in 967ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196732, Requested 5238. Please try again in 591ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196732, Requested 5238. Please try again in 591ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198669, Requested 10132. Please try again in 2.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198669, Requested 10132. Please try again in 2.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197499, Requested 6136. Please try again in 1.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197499, Requested 6136. Please try again in 1.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197567, Requested 6239. Please try again in 1.141s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197567, Requested 6239. Please try again in 1.141s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199652, Requested 5308. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199652, Requested 5308. Please try again in 1.488s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196372, Requested 5217. Please try again in 476ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196372, Requested 5217. Please try again in 476ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194668, Requested 5512. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194668, Requested 5512. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199469, Requested 5274. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199469, Requested 5274. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198520, Requested 4837. Please try again in 1.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198520, Requested 4837. Please try again in 1.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195126, Requested 4936. Please try again in 18ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195126, Requested 4936. Please try again in 18ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196119, Requested 5084. Please try again in 360ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196119, Requested 5084. Please try again in 360ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199136, Requested 4643. Please try again in 1.133s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199136, Requested 4643. Please try again in 1.133s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199023, Requested 4776. Please try again in 1.139s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199023, Requested 4776. Please try again in 1.139s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197854, Requested 5032. Please try again in 865ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197854, Requested 5032. Please try again in 865ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197809, Requested 4820. Please try again in 788ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197809, Requested 4820. Please try again in 788ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196963, Requested 4423. Please try again in 415ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196963, Requested 4423. Please try again in 415ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196954, Requested 4720. Please try again in 502ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196954, Requested 4720. Please try again in 502ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196772, Requested 4764. Please try again in 460ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196772, Requested 4764. Please try again in 460ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195982, Requested 5126. Please try again in 332ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195982, Requested 5126. Please try again in 332ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195380, Requested 4687. Please try again in 20ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195380, Requested 4687. Please try again in 20ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199423, Requested 4800. Please try again in 1.266s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199423, Requested 4800. Please try again in 1.266s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199409, Requested 4707. Please try again in 1.234s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199409, Requested 4707. Please try again in 1.234s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198509, Requested 4830. Please try again in 1.001s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198509, Requested 4830. Please try again in 1.001s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197586, Requested 5305. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197586, Requested 5305. Please try again in 867ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196946, Requested 4364. Please try again in 393ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196946, Requested 4364. Please try again in 393ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198676, Requested 4885. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198676, Requested 4885. Please try again in 1.068s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196795, Requested 4545. Please try again in 402ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196795, Requested 4545. Please try again in 402ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195505, Requested 4614. Please try again in 35ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195505, Requested 4614. Please try again in 35ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194654, Requested 8087. Please try again in 822ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194654, Requested 8087. Please try again in 822ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196692, Requested 5936. Please try again in 788ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196692, Requested 5936. Please try again in 788ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195484, Requested 8172. Please try again in 1.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195484, Requested 8172. Please try again in 1.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196436, Requested 4783. Please try again in 365ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196436, Requested 4783. Please try again in 365ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193912, Requested 6407. Please try again in 95ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193912, Requested 6407. Please try again in 95ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197917, Requested 4348. Please try again in 679ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197917, Requested 4348. Please try again in 679ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197572, Requested 5050. Please try again in 786ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197572, Requested 5050. Please try again in 786ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197433, Requested 4671. Please try again in 631ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197433, Requested 4671. Please try again in 631ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196688, Requested 4574. Please try again in 378ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196688, Requested 4574. Please try again in 378ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195665, Requested 4801. Please try again in 139ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195665, Requested 4801. Please try again in 139ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195625, Requested 5226. Please try again in 255ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195625, Requested 5226. Please try again in 255ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195616, Requested 5524. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195616, Requested 5524. Please try again in 342ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195578, Requested 4572. Please try again in 45ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195578, Requested 4572. Please try again in 45ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198734, Requested 4579. Please try again in 993ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198734, Requested 4579. Please try again in 993ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198461, Requested 4415. Please try again in 862ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198461, Requested 4415. Please try again in 862ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195543, Requested 4997. Please try again in 162ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195543, Requested 4997. Please try again in 162ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198511, Requested 4575. Please try again in 925ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198511, Requested 4575. Please try again in 925ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195450, Requested 4775. Please try again in 67ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195450, Requested 4775. Please try again in 67ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199539, Requested 5136. Please try again in 1.402s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199539, Requested 5136. Please try again in 1.402s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195489, Requested 6402. Please try again in 567ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195489, Requested 6402. Please try again in 567ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196332, Requested 4672. Please try again in 301ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196332, Requested 4672. Please try again in 301ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196243, Requested 4709. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196243, Requested 4709. Please try again in 285ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195335, Requested 5960. Please try again in 388ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195335, Requested 5960. Please try again in 388ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198172, Requested 4745. Please try again in 875ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198172, Requested 4745. Please try again in 875ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197369, Requested 4947. Please try again in 694ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197369, Requested 4947. Please try again in 694ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195428, Requested 5105. Please try again in 159ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195428, Requested 5105. Please try again in 159ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199051, Requested 4374. Please try again in 1.027s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199051, Requested 4374. Please try again in 1.027s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197383, Requested 4589. Please try again in 591ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197383, Requested 4589. Please try again in 591ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196563, Requested 5282. Please try again in 553ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196563, Requested 5282. Please try again in 553ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196460, Requested 4785. Please try again in 373ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196460, Requested 4785. Please try again in 373ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196369, Requested 8172. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196369, Requested 8172. Please try again in 1.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195712, Requested 4458. Please try again in 51ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195712, Requested 4458. Please try again in 51ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199009, Requested 4357. Please try again in 1.009s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199009, Requested 4357. Please try again in 1.009s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199148, Requested 8172. Please try again in 2.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199148, Requested 8172. Please try again in 2.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199048, Requested 6156. Please try again in 1.561s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199048, Requested 6156. Please try again in 1.561s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198784, Requested 4347. Please try again in 939ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198784, Requested 4347. Please try again in 939ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197616, Requested 4563. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197616, Requested 4563. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196447, Requested 5663. Please try again in 633ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196447, Requested 5663. Please try again in 633ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196494, Requested 4651. Please try again in 343ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196494, Requested 4651. Please try again in 343ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199001, Requested 5418. Please try again in 1.325s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199001, Requested 5418. Please try again in 1.325s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198866, Requested 4847. Please try again in 1.113s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198866, Requested 4847. Please try again in 1.113s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197442, Requested 4960. Please try again in 720ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197442, Requested 4960. Please try again in 720ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197377, Requested 4824. Please try again in 660ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197377, Requested 4824. Please try again in 660ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197196, Requested 5046. Please try again in 672ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197196, Requested 5046. Please try again in 672ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199245, Requested 4986. Please try again in 1.269s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199245, Requested 4986. Please try again in 1.269s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198310, Requested 4946. Please try again in 976ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198310, Requested 4946. Please try again in 976ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198178, Requested 8172. Please try again in 1.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198178, Requested 8172. Please try again in 1.905s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197402, Requested 4572. Please try again in 592ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197402, Requested 4572. Please try again in 592ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195933, Requested 5251. Please try again in 355ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195933, Requested 5251. Please try again in 355ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196340, Requested 4936. Please try again in 382ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196340, Requested 4936. Please try again in 382ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195528, Requested 4829. Please try again in 107ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195528, Requested 4829. Please try again in 107ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193747, Requested 8172. Please try again in 575ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193747, Requested 8172. Please try again in 575ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195375, Requested 8172. Please try again in 1.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195375, Requested 8172. Please try again in 1.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194018, Requested 6883. Please try again in 270ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194018, Requested 6883. Please try again in 270ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: <html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>cloudflare</center>\r\n</body>\r\n</html>\n",
    "source": "<html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>cloudflare</center>\r\n</body>\r\n</html>",
    "details": {
        "input": "\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community's key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        \"title\": <report_title>,\n        \"summary\": <executive_summary>,\n        \"rating\": <impact_severity_rating>,\n        \"rating_explanation\": <rating_explanation>,\n        \"findings\": [\n            {{\n                \"summary\":<insight_1_summary>,\n                \"explanation\": <insight_1_explanation>\n            }},\n            {{\n                \"summary\":<insight_2_summary>,\n                \"explanation\": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n\nFor example:\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    \"title\": \"Verdant Oasis Plaza and Unity March\",\n    \"summary\": \"The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.\",\n    \"rating\": 5.0,\n    \"rating_explanation\": \"The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.\",\n    \"findings\": [\n        {{\n            \"summary\": \"Verdant Oasis Plaza as the central location\",\n            \"explanation\": \"Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza's association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]\"\n        }},\n        {{\n            \"summary\": \"Harmony Assembly's role in the community\",\n            \"explanation\": \"Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]\"\n        }},\n        {{\n            \"summary\": \"Unity March as a significant event\",\n            \"explanation\": \"The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community's dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]\"\n        }},\n        {{\n            \"summary\": \"Role of Tribune Spotlight\",\n            \"explanation\": \"Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]\"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n2020,ASIAN MEDICAL PROFESSIONALS,A collective group of Asian medical professionals who serve as front-liners in the fight against the Coronavirus.,7\n2023,ACCOUNTANTS,\"Professionals who manage financial records and accounts, including Asian Americans.\",1\n2022,ARCHITECTS,\"Professionals involved in the design and planning of buildings and structures, including Asian Americans.\",1\n2019,ASIAN AMERICAN SCIENTIST,An Asian American scientist who discovered the cure for the Ebola virus and is currently working on a potential cure for COVID-19.,1\n2025,ENGINEERS,,1\n2021,SCIENTISTS,\"A collective of professionals, including Asian Americans, engaged in scientific research and development.\",1\n2024,EDUCATORS,\"Individuals involved in teaching and education, including Asian Americans.\",1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,rank\n1630,AMERICA,ASIAN MEDICAL PROFESSIONALS,Asian medical professionals are essential to America's healthcare system and should be welcomed for their contributions,21\n3569,ASIAN MEDICAL PROFESSIONALS,ACCOUNTANTS,\"Asian medical professionals and accountants play vital roles in their communities, supporting healthcare and financial stability\",8\n3568,ASIAN MEDICAL PROFESSIONALS,ARCHITECTS,Asian medical professionals and architects contribute to the well-being of society through their respective fields,8\n3565,ASIAN AMERICAN SCIENTIST,ASIAN MEDICAL PROFESSIONALS,The Asian American scientist is part of the Asian medical professionals who are fighting against the Coronavirus,8\n3566,ASIAN MEDICAL PROFESSIONALS,ENGINEERS,Asian medical professionals and engineers are both part of the skilled workforce contributing to the country's advancement,8\n3567,ASIAN MEDICAL PROFESSIONALS,SCIENTISTS,Asian medical professionals and scientists work together in the healthcare and research sectors,8\n3570,ASIAN MEDICAL PROFESSIONALS,EDUCATORS,Asian medical professionals and educators collaborate to promote health education and awareness,8\n\n\nThe report should include the following sections:\n\n- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        \"title\": <report_title>,\n        \"summary\": <executive_summary>,\n        \"rating\": <impact_severity_rating>,\n        \"rating_explanation\": <rating_explanation>,\n        \"findings\": [\n            {{\n                \"summary\":<insight_1_summary>,\n                \"explanation\": <insight_1_explanation>\n            }},\n            {{\n                \"summary\":<insight_2_summary>,\n                \"explanation\": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)].\"\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add \"+more\" to indicate that there are more.\n\nFor example:\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:"
    }
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: <html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>cloudflare</center>\r\n</body>\r\n</html>\n",
    "source": "<html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body>\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>cloudflare</center>\r\n</body>\r\n</html>",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195620, Requested 4902. Please try again in 156ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195620, Requested 4902. Please try again in 156ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195127, Requested 8172. Please try again in 989ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195127, Requested 8172. Please try again in 989ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199789, Requested 4605. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199789, Requested 4605. Please try again in 1.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199221, Requested 4369. Please try again in 1.077s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199221, Requested 4369. Please try again in 1.077s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199214, Requested 4360. Please try again in 1.072s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199214, Requested 4360. Please try again in 1.072s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198988, Requested 4560. Please try again in 1.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198988, Requested 4560. Please try again in 1.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198492, Requested 4482. Please try again in 892ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198492, Requested 4482. Please try again in 892ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198314, Requested 8172. Please try again in 1.945s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198314, Requested 8172. Please try again in 1.945s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198289, Requested 4764. Please try again in 915ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198289, Requested 4764. Please try again in 915ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198235, Requested 4617. Please try again in 855ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198235, Requested 4617. Please try again in 855ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198233, Requested 8172. Please try again in 1.921s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198233, Requested 8172. Please try again in 1.921s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198084, Requested 4589. Please try again in 801ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198084, Requested 4589. Please try again in 801ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197989, Requested 9777. Please try again in 2.329s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197989, Requested 9777. Please try again in 2.329s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197775, Requested 10426. Please try again in 2.46s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197775, Requested 10426. Please try again in 2.46s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197695, Requested 9974. Please try again in 2.3s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197695, Requested 9974. Please try again in 2.3s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197426, Requested 4905. Please try again in 699ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197426, Requested 4905. Please try again in 699ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197328, Requested 10775. Please try again in 2.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197328, Requested 10775. Please try again in 2.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196813, Requested 7955. Please try again in 1.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196813, Requested 7955. Please try again in 1.43s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198461, Requested 5750. Please try again in 1.263s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198461, Requested 5750. Please try again in 1.263s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192232, Requested 10851. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192232, Requested 10851. Please try again in 924ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196346, Requested 4842. Please try again in 356ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196346, Requested 4842. Please try again in 356ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196326, Requested 5023. Please try again in 404ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196326, Requested 5023. Please try again in 404ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196329, Requested 5418. Please try again in 524ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196329, Requested 5418. Please try again in 524ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196258, Requested 8460. Please try again in 1.415s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196258, Requested 8460. Please try again in 1.415s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199786, Requested 10277. Please try again in 3.018s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199786, Requested 10277. Please try again in 3.018s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199659, Requested 4648. Please try again in 1.292s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199659, Requested 4648. Please try again in 1.292s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199159, Requested 8172. Please try again in 2.199s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199159, Requested 8172. Please try again in 2.199s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198781, Requested 9064. Please try again in 2.353s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198781, Requested 9064. Please try again in 2.353s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197407, Requested 6038. Please try again in 1.033s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197407, Requested 6038. Please try again in 1.033s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197308, Requested 10379. Please try again in 2.306s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197308, Requested 10379. Please try again in 2.306s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197111, Requested 6566. Please try again in 1.103s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197111, Requested 6566. Please try again in 1.103s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196986, Requested 10420. Please try again in 2.221s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196986, Requested 10420. Please try again in 2.221s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196943, Requested 8590. Please try again in 1.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196943, Requested 8590. Please try again in 1.659s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196292, Requested 9325. Please try again in 1.685s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196292, Requested 9325. Please try again in 1.685s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196254, Requested 9670. Please try again in 1.777s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196254, Requested 9670. Please try again in 1.777s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195731, Requested 8807. Please try again in 1.361s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195731, Requested 8807. Please try again in 1.361s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
