{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194700, Requested 8036. Please try again in 820ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194700, Requested 8036. Please try again in 820ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Discrimination against China and Chinese people is nothing new – Sinophobia is a well-documented phenomenon that has occurred for centuries.\nHowever, the miscellaneous ways it has revealed itself during the coronavirus crisis reveals the increasingly complex relationship between China and the rest of the world. Sinophobia stems from historical resentment, fears of economic competition and racism. Fear of all Chinese is also linked to past ethnic trends and, at present, the coronavirus.\nIn places where Asians constitute a minority such as Europe, the United States, and Australia, Sinophobia seems to be incited by the profound stereotypes of the Chinese as dirty and uncivilized. Headlines such as “Yellow peril,” Chinese virus “panda-monium” and\n“Chinese kids, stay home” have appeared in French and Australian newspapers. Some Chinese students were also beaten in the United Kingdom. Meanwhile, some Chinese citizens have not been admitted to hotels and restaurants or shunned by public transport.\nAnd all the while, the web is dotted with mocking memes about the coronavirus.\nThe anti-Chinese rhetoric has also gained a sharper and more xenophobic tone in Asia. One common theme has been worries about\nmainland Chinese overrunning and infecting local populations.\nIn Singapore and Malaysia, hundreds of thousands have signed petitions demanding a total ban on Chinese nationals entering their countries; in response, both countries’ governments have implemented some form of entry ban. In Japan, some have dubbed the Chinese “bio-terrorists,” while conspiracy theories about the Chinese infecting locals, particularly Muslims, have predominated in Indonesia and elsewhere.\nIt is crystal clear that the flourishing prosperity of the Chinese has also culminated in ever-increasing numbers of tourists and students visiting and living in numerous parts of the world, resulting in their increased visibility on the ground. Reports of sporadic\ncrude behavior, combined with their sheer numbers, have given rise to the stereotype of Chinese tourists as boors or Chinese students as ultra-rich.\nOf course, Sinophobia is not ubiquitous, as populations in South America, Africa, and Eastern Europe view China more positively, according to the Pew Center for Research.\n“In recent years, a remarkable amount of anti-China sentiment has originated in the U.S., especially under the Trump administration,” said Professor Barry Sautman, a sociologist at the Hong Kong University of Science and Technology. Like President Donald Trump, for instance, most American media outlets insist on calling the recent coronavirus outbreak as a “Chinese virus.”\n“The U.S. itself has had a long history of Sinophobia, most notably with the 1882 Chinese Exclusion Act, which banned Chinese laborers following immigration that began with the Gold Rush. The current wave coincides, and is perhaps in part due to, the rise in nativism in the U.S., as well as the rest of the world,” said Sautman.\n“Now, China is seen as a challenger to U.S. hegemony, and almost every aspect of what the Chinese government does has been criticized heavily. As a result, lots of people around the world pick up on that, and it builds upon Sinophobia that has been historically embedded, like that in Asia,” he said.\nThe U.S. was the first country to impose a travel ban on Chinese travelers and the first to suggest a partial withdrawal of its embassy staff.\nObviously, the West fears the hideous pathogen of COVID-19, precisely because – short of very extreme measures – we are almost powerless to hinder its entry. The analogy in the Western subconscious is that: COVID-19 and China: both foreign contagions, carried by air, indifferent to borders and boundaries, and indifferent to the bodies they enter, are capable of wreaking havoc on core functions.\nWhatever the historical cause of this relentless phobia, the present trigger is the inexorable rise of China as an economic and military superpower – a power that is increasingly inclined to demand deference and respect. Today, most commentators regard China as both a security threat and an economic enabler. It’s both. That’s why coronavirus is becoming a Western excuse for Sinophobia and China-bashing.\nThe media plays a prominent role in this. The use of fake news and misinformation as instruments to foment hate against China is a reason to fear the Western axis. In contrast, China must act against these attacks with the same energy that is being utilized against the coronavirus just because both are real threats and national security issues.\nAmid the controversy and racism, an editorial published in The Lancet, a prestigious medical journal, on March 8 said: “The Chinese government has saved tens of thousands of lives. Isn’t making sure as many people as possible avoid contracting a dangerous virus the highest form of human rights?” China seems to have succeeded in bringing the outbreak of the COVID-19 epidemic under control, as Wuhan has reported zero infections for the first time. In doing so, it is trying to protect the health of the entire world population.\nWe hence owe a debt of gratitude to all officials and volunteers who are working around the clock to contain the virus and to treat those suffering from it. China is doing its best to help Italy and could be leading the way in developing a vaccine against COVID- 19.\nIt is therefore high time for unity and solidarity for all to combat this plague rather than blaming and generating antagonism against one another because, after all, we’re all in the same boat.\nFacing the monumental challenge formed by COVID-19, cooperation is the only way to protect human rights. The wrong response to a pandemic will not only cause grave damage to people’s lives and livelihoods and countries’ health but also harm the world economy.\nThat’s why the world, more than ever, needs to act together to overcome this common threat."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197022, Requested 7023. Please try again in 1.213s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197022, Requested 7023. Please try again in 1.213s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "I give in,\" she said. She will allow her son to skip a nap or eat with the television on. \"Surely, we can't yell and scream every day, right?\"This \"laissez faire\" parenting style seems more than warranted during this strange, stuck-at-home period. But how can mothers fill up their tanks above empty? Is self-care even possible for mothers during the coronavirus era?May described her self-care during the pandemic as \"feast or famine.\" \"Some weeks, I'm on it,\" she said. \"I exercise, feed myself beautiful food, get some quiet time, and I feel really good. Other weeks I'm literally eating butter on bread in the corner of my kitchen eight times a day. It can feel like I just need to get my basic needs met and everyone is in the way of that.\"When I asked Persaud how she makes time for herself, she answered unequivocally, \"Showers!\" Whitney Sandoval, who lives in Wichita, Kan., with her 5- and 3-year-olds, will drive to pick up her groceries and then hang out in her car in the parking lot. \"I listen to music or a podcast, read or just sit in silence. It's the closest I can get to being alone.\"Eating properly, exercising, showering and getting a little alone time sound like they belong in the \"basic health requirements\" category as opposed to \"self-care for mothers.\"Even if those bare minimum self-care needs are met, mom rage doesn't just disappear. Rage has something to say and, according to Dr. Markham, \"Rage doesn't dissipate until it feels heard.\"Ruth King, an educator, life coach, meditation teacher and the author of many books, including \"Healing Rage: Women Making Inner Peace Possible\" and \"Mindful of Race: Transforming Racism from the Inside Out,\" said: \"Rage sits at the crossroads of personal transformation. Rage is not to be understood as a useless emotion, empty of knowledge. Rather, rage is fierce clarity and untapped fuel -- when we push rage away, we can't learn from it.\"Unfortunately, many mothers are doing just that. It is a challenge to find mothers who will talk on the record for this article. One mom eagerly emailed me about her rage, but then declined being quoted, saying, \"You know, mom shame.\"Bellenbaum of The Motherhood Center said, \"There's so much guilt that we feel toward ourselves, and a kind of inner-disappointment that we have these types of feelings at this intensity, especially toward our children.\"It can be challenging for partners living with those who have mom rage to be able to offer compassion and support, especially during the pandemic, when the emotional bandwidth of all parents is stretched thin.In support groups at The Motherhood Center, Bellenbaum has seen mothers find the nonjudgmental witnesses they need in each other. She said, \"When we connect with other women who are having the same feelings, that sense of community creates an initial and immediate relief.\"I have experienced this relief myself. What the mothers who write to me about their mom rage don't know is that their emails help me feel less alone, too. Since the pandemic began, people have been clapping, singing and howling into the night at a certain hour. Some clap to thank essential workers. Others howl in grief or just to relieve stress. Maybe it's time for mothers to take to the windows and bellow out a collective earsplitting roar."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196783, Requested 7497. Please try again in 1.284s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196783, Requested 7497. Please try again in 1.284s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A University of Cincinnati dean is investigating an email to an engineering student in which an adjunct instructor\nis accused of referring to the novel coronavirus as \"the Chinese virus.\" The email came after the student had to\nmiss a lab session due to being quarantined for possible exposure to the novel coronavirus.\n\"I think that the school should take disciplinary actions against the professor because (his) actions completely\nviolate the school's values,\" 20-year-old Evan Sotzing told The Enquirer Thursday evening.\nUC investigation underway\nUniversity spokeswoman M.B. Reilly said Thursday night that the \"matter has already come to the attention of\nthe dean who is already looking into this on behalf of the student.\"\nSotzing said he has been in contact with UC's Dean of Engineering and Applied Science, John Weidner, who he\nsays is \"looking into it.\"\nWeidner said he referred the matter for review to the university's Office of Equal Opportunity and Access. That\ndepartment handles inquiries regarding discrimination, harassment or retaliation based on disability, race, color,\nreligion, national origin and other identities, according to their website.\n\"These types of xenophobic comments and stigmatizations around location or ethnicity are more than troubling.\nWe can better protect and care for all when we speak about COVID-19 with both accuracy and empathy – something we should all strive for,\" Weidner wrote to The Enquirer.\nEmails lay out incident\nSotzing received the email from John Ucker, who is listed as an adjunct instructor in UC's College of Engineering\nand Applied Science. He did not reply to an email from The Enquirer Thursday evening.\nIn emails forwarded to The Enquirer, Sotzing, a third-year engineering student, wrote to Ucker that he would not\nbe able to attend an in-person lab scheduled for Tuesday because his girlfriend tested positive for the novel\ncoronavirus at UC's University Health Services.\nSotzing was tested the same day, he said, and he received a negative COVID-19 result. However, Sotzing said\nthe university's health officials told him to quarantine for two weeks because of his proximity to a positive\npatient.\nThose tests were taken on Sept. 4, Sotzing said. The lab was scheduled for Tuesday, on Sept. 15, which would\nhave fallen in the two-week quarantine time period.\nSotzing provided photo evidence of his test results in an email to Ucker, and wrote that he had been told not to\nattend any in-person classes until Thursday. Ucker did not reply to Sotzing's email until after the in-person lab\nhad already happened.\n\"For students testing positive for the chinese (sic) virus, I will give no grade,\" the email reads. \"You can read the\ninfo I sent to the class re: the torsion test.\"\n'This kind of language is completely unacceptable'\nUC stated in its Return to Campus Guide that it encourages faculty to be flexible with students, especially in\nregards to attendance policies during the pandemic. Weidner said Ucker has a \"routine policy\" of allowing\nstudents to forego one of the lab tests so that a grade will not be recorded.\nUC has reported 321 COVID-19 cases among students as of Thursday. Hamilton County officials said this week\nthe positive rate for the new coronavirus is spiking in Cincinnati's Corryville, Mount Auburn and CUF (Clifton\nHeights/University Heights/Fairview) neighborhoods around UC.\nSotzing said he is not clear whether Ucker's message meant he was getting a zero grade or if the professor is\nmerely not grading the assignment.\n\"This kind of language is completely unacceptable. And especially from people, like, in power... it has no place in\nthis country and it contributes to Asian xenophobia,\" Sotzing said.\nSotzing posted the professor's email response on Twitter Thursday afternoon.\n\"My girlfriend tested positive for COVID and the University of Cincinnati's Health Department instructed me to\nnot attend my in-person lab. Not only did my professor give me a zero for not going, but this was his response,\"\nthe post reads, with a screenshot of the email. Within three hours, the post had over 6,000 likes and over 2,000\nretweets.\n\"He needs to apologize for the fact that he made that comment,\" Sotzing told The Enquirer Thursday.\nHarassment rising towardAsian Americans\nNews of the investigation came on the same day that the U.S. House of Representatives passed a measure\ncondemning anti-Asian bigotry and discrimination during the COVID-19 pandemic.\nIncreased numbers of Asian Americans have reported harassment and even physical assaults amid political\nrhetoric blaming China for the pandemic from President Donald Trump and others. The House measure, a nonbinding resolution, does not require the Senate to pass it, nor does it require the president's signature The Tangeman University Center at the University of Cincinnati's campus on March 11."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196715, Requested 7980. Please try again in 1.408s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196715, Requested 7980. Please try again in 1.408s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "With coronavirus concerns, stay-at-home orders, financial instability and mounting civil unrest, it is no surprise that mothers are experiencing intensified anger.After I wrote a personal essay on mom rage in 2019, strangers on Twitter declared me an unfit mother. But I expected Twitter-hate. What I did not expect were the many emails I received from mothers around the world, saying they too struggle with mom rage and my story made them feel less alone.After the initial flood of emails, a trickle continued over the next six months. Then Covid-19 happened, and with it, stay-at-home orders. My inbox began lighting up again, illuminating a direct correlation between mom rage and sheltering in place.\"Mom rage\" is the colloquial term for the unrestrained anger many women experience during pregnancy, postpartum and beyond.It is a popular topic these days in a support group for working moms at The Motherhood Center, a clinical treatment facility in Manhattan that offers services for pregnant and postpartum women. Paige Bellenbaum, a group facilitator and the center's founding director, said, \"Mom rage is something we talk about all the time. Social isolation, lack of support, managing high levels of anxiety and stress -- this is the new normal of being a mother, and during the pandemic in particular.\"Anger and rage are waving red flags hinting at feelings below the surface. Mothers who experience rage may be feeling alone, unheard, and unsupported, Bellenbaum said. \"But it's so much more powerful to feel angry and rageful than to touch the vulnerability of what lives behind it.\"Between stay-at-home orders, Covid-19 health concerns, financial instability (or fear of it), police violence against Black people, it is no surprise that mothers are experiencing intensified rage above the surface, and feelings of grief, fear, and loneliness below.\"We're asking all parents, but it's especially moms on the front lines, to try to do 24/7 child care without a break at the same time that they're trying to often hold down a job,\" said Laura Markham, Ph.D., a clinical psychologist, parenting coach and author of \"Peaceful Parent, Happy Kids: How to Stop Yelling and Start Connecting.\" \"So, is there more mom rage?\" she asked. \"How could there not be?\"Mom rage expresses itself in different ways. Anya Persaud, who has a 3-year-old and a newborn and lives in Beacon, N.Y., could pinpoint her fury: \"Raising my voice and walking hard are signs I'm heading from frustration to rage.\"Molly Caro May, who lives in Bozeman, Mont., and is the author of \"Body Full of Stars: Female Rage and My Passage into Motherhood,\" said of her rage, \"I never hurt anyone, but I was out in the forest throwing rocks at trees.\"Moms aren't supposed to yell and stomp and throw rocks, and we aren't supposed to share our rage publicly. I have wondered if I've been able to write openly about mom rage without much reproach because it has become so commonplace in our lexicon, or if it is because I am white.Nefertiti Austin, the author of \"Motherhood So White: A Memoir of Race, Gender, and Parenting in America,\" wasn't familiar with the term \"mom rage,\" but acknowledged that the intense anger is somewhat universal for moms. Of Black mothers, Austin said: \"It's tricky for us, because we are already saddled with 'angry Black woman.' I definitely don't want to be described as having mom rage, because it's not going to play the same if I say I have it, than if a white mom says she has it.\"Austin added that because of racist stereotypes, Black mothers are under more pressure to appear perfect. With police violence against Black people, Austin said, Black mothers may have their \"children on a tighter leash than white parents.\"\"The whole 'kids will be kids' thing? We know that that's not true when it comes to our kids. There isn't a lot of grace for Black children,\" Austin said.That fear and perfectionism can only add fuel to the mom-rage fire.Since viewing the video of George Floyd's death, Persaud said her mood and sleep have suffered. \"I've had overwhelming anxiety and feelings of hopelessness and helplessness.\"Duan said one of the factors affecting her mom rage is \"the trauma of being Asian-American during the pandemic,\" after some, including President Trump, have blamed China for the coronavirus outbreak. The attack has led to a surge in xenophobia against Asian-Americans.It's been a few months since the pandemic began and several weeks since protests against police violence filled our neighborhoods. All the while, mothers continue to work multiple jobs at once (teacher and mom at a minimum), and they're exhausted. May said she vacillates between \"this week, we're going to study cities of the United States\" and \"actually, we're just going to be outside playing with sticks.\"Like May, Duan concedes her bandwidth has been lower since Covid-19. \"I think it goes hand-in-hand with my resignation,\" she said. \"I'm fine with the kids just messing around and occasionally learning.\"Persaud is having similar throw-up-her-hands moments these days. \"Where before I might have raised my voice, now I give in,\" she said. She will allow her son to skip a nap or eat with the television on. \"Surely, we can't yell and scream every day, right?\"This \"laissez faire\" parenting style seems more than warranted during this strange, stuck-at-home period. But how can mothers fill up their tanks above empty? Is self-care even possible for mothers during the coronavirus era?May described her self-care during the pandemic as \"feast or famine.\" \"Some"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192526, Requested 7990. Please try again in 154ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192526, Requested 7990. Please try again in 154ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "in, the Afro-Chinese soup of New Orleans — which Sin made with Damon for one dinner — or what chef Eddie Huang and others call “hood Chinese” food, like the chicken wings served in many New York neighborhoods. The Chinese diaspora, it turns out, is remarkably agile.“China is this incredible beacon in food,” said Marcus Samuelsson, the Ethiopian-Swedish host of “No Passport Required,” who has made Harlem his home. “We’re relearning about heritage, culture and history. That’s not just happening in the food world but everywhere. We see it in taking down monuments, for example. We’re having a really important conversation in America and, if America is having it, very often the world is having it.”Sin’s mission is beyond gastro-diplomacy, approaching gastro-activism, using the culinary intersections as conversational starting points. Chefs are happy to join in.Damon, founder of Supper Club From Nowhere, a culinary history project inspired by civil rights chef Georgia Gilmore, cheered Sin’s request to collaborate. Amid what she called exhausting “Black for pay” propositions, designed to help brands burnish their political credentials by partnering with people of color, she called Distance Dining “a breath of fresh air.”“He’s already doing the work in himself,” she said. “There’s relief because I’m not doing this work alone. I’m not having this conversation alone.”Damon, whose mother is Gullah Geechee and father is Creole, sighed. “When we face those ugly parts of us and our distance, and we come together to reconcile with that, what does that taste like?” she said. “I want to look back and know what I was doing during the great COVID pandemic of 2020, when there was a nationwide, global movement for all Black lives. I want to know that I’m proud of what I was doing. I was cooking for a better future, cooking for a better me. When everything feels so bad, doing this feels good.”Chintan Pandya, chef at Adda Indian Canteen in Long Island City, Queens, said Distance Dining was an opportunity to showcase dishes that are authentic in a surprising way. The chile chicken he made with Sin, for example, was invented in the 1970s, when soy sauce and cornstarch were substituted for garam masala by members of the Chinese community that has flourished for decades in Kolkata.“It’s a very nostalgic dish,” Pandya said. “We’ve grown up eating it. The way we were exposed to Chinese food was this dish. What General Tso’s is to America, this dish is for India. It’s integral.”But the Chinese diaspora is not all about history or nostalgia. China is an emerging power in Africa, for example, and Pierre Thiam, chef of Teranga, in East Harlem, has noticed the nascent Chinatown of his native Dakar, the capital of Senegal.His collaboration with Sin used dawadawa (fermented locust beans) in an efo riro stew, as a nod to douchi (China’s fermented black beans), as well as fonio, a Senegalese grain, as tribute to China’s ancient, pre-rice Five Grains. They were substitutions he borrowed from his home kitchen, cooking with his fiancée, Lisa, who is Chinese-Japanese. Sin’s Eight Treasures pudding, in turn, was evocative of similar Senegalese desserts like sombi and thiakry.“When I first came to New York and wanted to cook something with dawadawa, I would go to Chinatown and get fermented beans there,” Thiam said. “The closest substitute I could get to the flavors of home would be the Chinese markets. There’s going to be a revolution in cuisine because Chinese are getting more prominent in Africa. I’m looking forward to that.”Sin’s idea, however, is not exactly new to anyone who has raved about Mei Lin’s mapo lasagna at Nightshade in Los Angeles, or the XO tartare at C.A.M. in Paris, or who remembers David Chang’s mapo ragù in 2006. And half a century ago, Chinese-Cuban restaurants sprang up across Manhattan. Chinese crossover has been on menus both divey and luxurious.Sin’s approach is emboldened and elevated by current events. After the death of Floyd, Sin knew the value of solidarity, he said, having seen it for years in Hong Kong’s own protests.Two years of collaboration at the Museum of Food and Drink, he added, taught him that “African American cooking really is the bedrock of American food culture.” The Black Lives Matter movement got him to act on that knowledge, embracing the cuisines of communities in crisis.Sin’s new education has been tumultuous. He said he now knows that Chinese restaurants have often seen historically redlined African American communities as “decent business opportunities,” which can become a potentially exploitative relationship.He is more and more angry at what he calls the white supremacy of the Anglicized renaming of “pot stickers” or “soup dumplings,” for example, in ways that are not applied to bratwurst, paella, pâté or ravioli. He has become more offended that Chinese food is “at the bottom of the price-point food chain” and more appreciative of flavors like the sweet-and-sour mumbo sauce of Chinese restaurants in Washington, D.C., because it’s “not part of the white palate.”Overall, he said, Distance Dining has amplified his creative sense of possibility — and cross-cultural optimism. His aim is to make the dinners permanent.“Many people celebrate history and tradition with food,” said Cecilia Chiang, the 99-year-old godmother of Chinese restaurants nationwide, who has eaten at the storied"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194966, Requested 7864. Please try again in 849ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194966, Requested 7864. Please try again in 849ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "SAN FRANCISCO – Mandy Rong was terrified her 12-year-old daughter had COVID-19. It was 2 a.m. and the young girl was hours into a fierce fever and a racking cough. She was weak and didn’t want to eat. What few medications were on hand had expired. She sipped warm water instead.“Mommy, why are my eyes on fire?” asked Amy Rong.The mother and daughter, along with Rong’s parents, live in an 80-square-foot windowless single-room-occupancy Chinatown building that is a home of last resort for many impoverished Asian immigrants. Hallways are cramped, bathrooms and kitchens are communal. A ripe setting for the spread of the highly contagious novel coronavirus.That early March night felt endless. Rong, 42, repeatedly touched Amy’s forehead, wondering if her child would die in the small loft that the two shared. Down below, her father slept on the floor while her mother took the lone sofa bed. The grandparents were eager for updates on Amy’s fever, but they worried their whispers would wake her.In the morning, the fever had vanished, only to return a week later. Once again, the family endured a restless night. Rong made soup, but Amy wouldn’t eat it. She cooked porridge and spoon-fed it to her daughter.Getting tested for COVID-19 didn’t seem like an option for the Rongs. The rumor was that the tests were expensive. Rong also feared the reaction from neighbors.“If you test positive, everyone would be scared of you,” said Rong. “Everyone would think you are the devil.”It is easy to mistake San Francisco for a thriving Asian American haven. The city, which is its own county, boasts a bustling Chinatown, as well as a popular Japantown. Native Hawaiians, Pacific Islanders, Vietnamese, Indians and Filipinos also have made their homes here. All told, Asians in San Francisco represent upward of 20 countries.But many Asian American immigrants in the county lead a fragile existence rendered even more precarious with the arrival of COVID-19. So far, 38% of the 123 COVID-19 deaths reported by the San Francisco Department of Public Health are Asian American residents, the most of any ethnicity.Experts also are concerned that positivity rates among Asian Americans in San Francisco could be far higher than the 12% reported, a by-product of the decades-in-the-making model minority myth, which characterizes this ethnic group as financially successful, physically healthy and upwardly mobile. This belief has caused segments of the Asian American community to long be overlooked when it comes to social services for housing, employment and health.San Francisco is one of the few places in the nation tracking data on Asian Americans and COVID-19 deaths at a time when officials don’t know the ethnicity of the person affected in nearly half of the nation’s 7.8 million coronavirus cases. Around 17 million Americans are of Asian descent, or 5.6% of the population.In many cases, Asian Americans in this city have received imprecise or no information in their native language about testing, safety tips, housing and other critical care services during the pandemic. At the same time, the community is struggling with inadequate access to comprehensive health care, the need to keep front-line employment and growing incidents of anti-Asian hate crimes.“This model minority thing, that’s not us,” said Judy Young, executive director of the Southeast Asian Development Center, a San Francisco nonprofit that helps area residents from Vietnam, Laos and Cambodia. She said 80% of her clients have lost their mostly service industry jobs during the pandemic.“There is the language barrier and our community is small,” Young said. “So the city doesn’t think we have any problems when we do.”That risk of invisibility is only heightened by the pandemic. Since city health officials do not break down COVID-19 statistics beyond “Asian American,” many advocates for the city’s various groups said they are left to speculate about coronavirus infection and death rates within their individual communities. How many people are dying, and are those people Japanese Americans? Vietnamese? Korean? Filipino? No one knows.“There’s this feeling that there's excess death out there,” said Jeffrey Caballero, executive director of the nonprofit Association of Asian Pacific Community Health Organizations. “That high mortality rate among Asian Americans means either there isn’t enough testing or people are waiting far too long to get care.”What is the model minority myth?Xing Tam’s mother tested positive for COVID-19 in March. Her symptoms were mild. Medical officials told her to quarantine at home and avoid others.Suddenly, the working class Bayview district home where Tam, his mother and 17 other relatives and friends live together became uncomfortably crowded. Tam's mother was given one of the three-story home's 12 rooms. For weeks, everyone in the house moved about gingerly, hoping not to inflame the virus in their midst.Tam’s mother avoided the communal kitchen. Concerned about her health, she quietly voiced a worry to her son that she might not make it. He was too scared to offer any comforting reply.“I just told her, ‘Don’t be scared. You will be OK,’” said Tam, who emigrated from China in 2008 and works in the city at a restaurant. “But I was scared she might die.”America’s legacy of denying Asians access to equal health care and other social services is one that Native Hawaiians and Pacific Islanders like Wai Sing Lee know too well. A Chinatown social worker at the nonprofit NICOS Chinese Health Coalition, Lee said his team has spent the pandemic trying to make members of the Asian American community feel less alienated.But he concedes their efforts have fallen short. Elders frequently ask him if the pandemic is an omen of doom, a question no one can answer to any degree of satisfaction.“No one seems"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195536, Requested 6268. Please try again in 541ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195536, Requested 6268. Please try again in 541ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Clara Kan and her mother were just beginning a seaside stroll last week in Richmond, B.C., when two men in a car hurled a racial slur at them and shouted: \"Go back to China.\"\nThe women, who were wearing masks, confronted the driver, an older white man, who then revved the engine and lurched the car toward Ms. Kan before driving off.\nMs. Kan's 65-year-old mother screamed for help and the men drove off. Ms. Kan immediately reported the incident to the RCMP.\nSince the altercation, she says, her mother is scared to resume their daily walks in public parks and only feels comfortable shopping at Chinese supermarkets, where the majority of other shoppers are East Asian.\nMs. Kan is adamant that she will never remain silent about the kind of abuse she, her mother and many other East Asian Canadians are facing.\n\"What concerns me most is a lot of Chinese people or Asian people they're like, ‘Oh, let it go, just walk away,' but I feel like maybe in a way because I was bold enough to confront these racists maybe that's why physical violence didn't occur,\" says Ms. Kan, 33."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193083, Requested 8172. Please try again in 376ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193083, Requested 8172. Please try again in 376ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "equal health care and other social services is one that Native Hawaiians and Pacific Islanders like Wai Sing Lee know too well. A Chinatown social worker at the nonprofit NICOS Chinese Health Coalition, Lee said his team has spent the pandemic trying to make members of the Asian American community feel less alienated.But he concedes their efforts have fallen short. Elders frequently ask him if the pandemic is an omen of doom, a question no one can answer to any degree of satisfaction.“No one seems to care about us,” said Lee, 62.When anti-Asian violence increases, so does neglect, experts saySan Francisco’s Shanti Project, a revered nonprofit founded in the 1970s to help dying AIDS patients, expanded its services to Asian American residents of Chinatown this spring. Volunteers and staff distribute food and offer telehealth support, senior services and emergency supplies.Even before the pandemic, members of these tight-knit immigrant communities were familiar with the feeling of being neglected and overlooked. Because of this, many Asian Americans in San Francisco now cope with the belief that asking for help, whether it’s from a doctor or other service provider, is futile.Dr. Tung Nguyen, professor of medicine at the University of California, San Francisco, said the pandemic has only heightened this feeling of helplessness.Nguyen pointed out that Asian Americans are the least likely of any racial group to use health and social services. Public health officials aren’t sure whether the low usage rate is due to barriers caused by language, culture, immigration status or a general mistrust of government proposals.Nguyen also co-authored a report released in May by the Asian American Research Center on Health that called attention to the fact that 50% of San Francisco’s 31 COVID-19 deaths at that time were among Asian Americans. It was a disproportionately high percentage since Asian Americans make up just over a third of the city’s population.Although the percentage of deaths has since dropped, Nguyen said a lack of detailed data about Asian Americans often means that city funds aren’t allocated to this group.“The truth is we are the ones who lose out as a result of this stereotype,” he added.To be sure, the fortunes and contributions of many Asian Americans have skyrocketed in past decades. The median annual income of households headed by the nation’s 22 million Asian Americans is $73,060, compared with $53,600 for all U.S. households, according to the Pew Research Center.But these success stories obscure the troubling reality facing many Asian Americans.“You simply cannot look at Asian Americans as a monolithic group because if you do that, you’re going to miss how different communities experience the pandemic,” said Jarvis Chen, a lecturer in social and behavioral sciences at Harvard University’s T.H. Chan School of Public Health in Massachusetts.A closer look at San Francisco's two dozen Asian ethnicities reveals many groups within this broad categorization are struggling financially and remain outside the mainstream. About 43% are non-English speakers, according to a USA TODAY analysis of U.S. census data. About a third of San Franciscans are foreign-born, and 13% are not U.S. citizens.Discrimination also is keeping some Asian Americans from getting tested for COVID-19. The website Stop AAPI Hate, the acronym for Asian American Pacific Islander, has logged more than 2,500 incidents of discrimination across the U.S. since mid-March. The attacks have ranged from verbal assaults to acts of physical violence.When Asian Americans hear President Donald Trump, who contracted COVID-19 in October, repeatedly call the virus the “China virus” and “Kung Flu,” “it makes them less likely to seek help, a bit like early in the AIDS epidemic when the gay community was stigmatized,” said Karthick Ramakrishnan, professor of public policy at the University of California, Riverside, and chair of the California Commission on Asian and Pacific Islander American Affairs. “We fear many Asian American families have gone underground.”Decades of racist policies have limited Asian American’s standard of livingChinese citizens began passing through San Francisco’s then bridgeless Golden Gate en masse during the Gold Rush of 1849. By 1851, some 25,000 had arrived, lured by the hope of riches in a land called Gum Saan in Cantonese, or “gold mountain.”By the late 1800s, the Chinese were not just vilified but outright barred from entering the country, with few exceptions, by the Chinese Exclusion Act of 1882. White officials charged they were taking jobs from other Americans, despite having been integral to the Gold Rush’s boom and the construction of the Transcontinental Railroad.At the height of World War II, Japanese Americans around the country were rounded up and sent to internment camps, feared as the traitorous “yellow peril” after years of citizenship. Despite painful and humiliating treatment at the hands of the U.S. government, many Asians resolved to engrain themselves in the society at large with an image of themselves as patriotic, hardworking Americans. Japanese Americans were among the most decorated U.S. soldiers during the war, and others excelled in academics and commerce.The model minority image gained momentum during the civil rights movement of the 1960s. Asian American success stories were highlighted by white U.S. officials both as a way of signaling to other nations, namely the Soviet Union, that America was not racist, but also to shame other ethnic groups, notably Black Americans.The logic went that if Asian Americans were doing so well, surely failure on the part of other ethnic groups was their own fault.Then came the Vietnam War, a quagmire that resulted in a U.S.-sponsored evacuation of 125,000 refugees followed by countless others who escaped Southeast Asia in rickety boats. Many landed in San Francisco.“The stereotype about us is broad and includes the notion that we’re all studious, we don’t"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197354, Requested 7400. Please try again in 1.426s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197354, Requested 7400. Please try again in 1.426s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "As the coronavirus first reported in China now ravages the U.S., Asian Americans are continuing to wrestle with a second epidemic: hate. Hundreds of attacks on Asian people\nhave been reported, with few signs of decline.\nAsian Americans in California reported 832 incidents of discrimination and harassment in the last three months, according to the new data collected by \"Stop Asian American\nPacific Islander Hate,\" the leading aggregator of incidents against Asian Americans during the pandemic.\nThe incidents included 81 assaults and 64 potential civil rights violations.\nThe new report released on Wednesday by Stop AAPI Hate shows that incidents of racism and discrimination are not isolated to any particular area but are a statewide problem\nin California as Asian Americans have reported incidents in 34 counties so far in the most populous state of the United States, adding that incidents are reportedly taking place\nin California in retail stores, in the workplace and online.\n\"Anti-Asian American harassment has also been further stoked by President Trump's repeated use of the term 'Kung Flu' in recent rallies, and as recently as last night,\ncomments on Twitter scapegoating China for the United States' devastating failure to control the coronavirus,\" said the group in a statement.\nThey noted that discrimination and harassment of Asian Americans in California has drawn national attention recently after a series of videos in Torrance, California featured a\nwoman using graphic racist language against Asian Americans.\n\"The viral video of racism in Torrance is one example of hundreds. It's the tip of the iceberg of anti-Asian American hate and discrimination,\" said Russell Jeung, chair and\nprofessor of Asian American Studies at San Francisco State University. \"Without government accountability, we risk COVID-related racism against Asian Americans becoming\ndeeply entrenched, ultimately impacting the lives of millions of people in California and around the country.\"\n\"Racist demagoguery matched with anti-immigrant policies have always been used to deny Asian Americans full social and political rights,\" said Cynthia Choi, co-executive\ndirector of Chinese for Affirmative Action.\n\"In California, we have to do more than condemn racist rhetoric – we must take bold action today to address attacks whether they happen in grocery stores, in the workplace or\nin the schoolyard,\" Choi added.\nStop AAPI Hate sent a letter to California governor Gavin Newsom Tuesday night to recommend the establishment of a Racial Bias Strike Team comprised of key state agencies\nand departments to oversee workplace and employment discrimination, provide mental health services to vulnerable communities, and offer support to local Asian American-\nserving organizations.\nMeanwhile, Asian Americans in New York have been helping themselves confronted with the increase of targeted hate crimes by equipped themselves with GoPros and guns.\nWhen Eddie Song leaves his Manhattan home, it can feel like heading into battle. The Korean American startup founder and avid rider dons his armored motorcycle jacket,\nmotorcycle gloves, a skull face mask and a GoPro camera.\n\"The GoPro is on all the time whenever I leave the house now. Basically it's a rolling camera,\" Song said. \"With the combination of looking intimidating and having the camera –\nif they pick a fight with me, they know I'm prepared.\"\nDuring the pandemic, an online hate reporting center has received nearly 1,500 reports of racist abuse against Asians nationwide since it launched March 19. Stay-at-home\norders mean in-person run-ins are down somewhat but vandalism of Asian-owned homes and businesses is up, according to the advocacy groups running the portal.\nIt's difficult to predict whether incidents will dramatically drop once society goes back to \"normal,\" Levin, a former NYPD officer, said, noting that the pandemic is\nunprecedented.\n\"Generally, when there's a catalytic event, hate crimes tend to decline and have a bit of a half-life,\" he said. \"But that presupposes a singular catalytic event as opposed to a\nrolling one.\"\nCopyright © People's Daily Online 2020. All Rights Reserved"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197988, Requested 7294. Please try again in 1.584s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197988, Requested 7294. Please try again in 1.584s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "prompted the creation of the nonprofit Asian Americans Advancing Justice, which now has chapters nationally.And for years, the stereotype of an Asian carrying cash resulted in Asian Americans being targeted for robberies and carjackings, which by and large aren't categorized as hate crimes despite their racial element, said Debbie Chen, OCA’s executive vice president and a civil engagement programs director for the greater Houston chapter.Myth of 'model minority' harms Asians, othersMany Asian Americans feel their communities have long been ignored by mainstream politics, media and entertainment, especially when coupled with the myth of Asians as a “model minority” who are more successful than Blacks or Latinos.That myth has long been used by white Americans to pit and separate Asian Americans from other people of color, and to justify institutional racism. It may also account for the fact that according to one of the few existing research reports on anti-Asian hate, published this January in the U.S. National Library of Medicine, Asian Americans have a relatively higher chance than Blacks or Latinos of experiencing hate crimes perpetrated by non-white offenders.The report found that despite hate crimes against Asian Americans being on the rise, studies rarely look at such incidents. They are “largely ignored\" by researchers, and as a result the nature and characteristics of the offenders, victims, and situations are largely unknown, the report says.“The outrage, the decrying of these recent incidents, is because of centuries of invisibility, of feeling like the history of anti-Asian racism is not known, that what happens to our community is minimized, is overlooked,” said Choi, of Stop AAPI Hate.Compounding the challenge, the Asian American community is not monolithic. Instead, it’s broad and encompasses people who trace their ancestry from China, Japan, the Philippines, Korea and other countries, all with their own distinct languages and cultures.Shieh, of Asian Americans Advancing Justice of Chicago, said it’s often hard to persuade victims of racial harassment or hate crimes to come to police because they speak a language other than English, or worry about their immigration status. Chicago alone has 19 separate Asian communities.\"We're not even a singular group that can unite,” said Shieh, 28. “We forget that our country is not necessarily a black-and-white paradigm or dichotomy.\"Chen, of OCA, said Asian immigrants might worry about causing \"trouble.\"“They're being targeted as Asians because they (perpetrators) don’t think Asians are going to make as big as a fuss. It goes along with the stereotype that Asians are less likely to be vocal about things,” Chen said. “That’s changing as we have more and more young people growing up here, but so long as your majority of your population is first generation, like Houston’s AAPI majority...they don’t want to cause trouble, they just want to do their job, make a living, make sure their family is OK. They’re just trying to survive.”Lieu said Trump inflamed ’ passions when he wrongfully tied Asians to COVID-19.“It’s going to take education and time to mitigate the harm that was done last year,\" Lieu said, \"It’s not like you can flip a switch and people will stop engaging in discrimination.\"Oh, the Denver attorney, said the coronavirus pandemic has exacerbated existing racial tensions – and the increasing violence has made it easier to speak up about them.Oh said she became a civil rights attorney in part because she felt Chicago police never took seriously the complaints from her parents that the frequent break-ins and robberies of their small restaurant were driven by the Korean heritage.\"Feeling heard has been so powerful for the Asian community,” she said."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195392, Requested 7865. Please try again in 977ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195392, Requested 7865. Please try again in 977ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Lucy Li tries not to let fear dictate her interactions with patients as she makes the rounds in the covid-19 intensive care unit. But the anesthesiology resident at Massachusetts\nGeneral Hospital cannot erase the memory of what happened after work at the start of the pandemic.\nA man followed the Chinese American doctor from the Boston hospital, spewing a profanity-laced racist tirade as she walked to the subway. \"Why are you Chinese people killing\neveryone?\" Li recalled the man shouting. \"What is wrong with you? Why the f--- are you killing us?\"\nStunned at first, then relieved she was not physically attacked, Li is now saddened and angered by the irony that she spends her days and nights helping to save lives. Her work\ninserting tubes in patients' airways has grown riskier since the coronavirus emerged - each procedure releasing droplets and secretions that could carry viral particles.\n\"I'm risking my own personal health, and then to be vilified just because of what I look like,\" said Li, 28, who is wary that one of her patients, too, could harbor such prejudices.\n\"I try not to think about that possibility when I'm at work taking care of patients. But it's always there, at the very back of my mind.\"\nAcross the country, Asian American health-care workers have reported a rise in incidents of bigotry. The racial hostility has left Asian Americans, who represent 6 percent of the\nU.S. population but 18 percent of the country's physicians and 10 percent of its nurse practitioners, in a painful position on the front lines of the response to the coronavirus\npandemic. Some covid-19 patients refuse to be treated by them. And when doctors and nurses leave the hospital, they face increasing harassment in their daily lives, too.\nAsian Americans have experienced a sharp increase in racist verbal abuse and physical attacks during the pandemic, with the FBI warning of a potential surge in hate crimes\nagainst Asians as the coronavirus death toll mounts and stay-at-home orders are lifted.\n\"People are worried about transmission of a disease that they associate with foreignness and Asian faces,\" said Grace Kao, a Yale University sociologist. \"Nothing erases what we\nlook like.\"\nThere is no comprehensive data measuring anti-Asian bias during the pandemic. An analysis of self-reported incidents by Russell Jeung, chairman of the Asian American studies\ndepartment at San Francisco State University, shows a steady rise in reports of harassment and assaults against Asians since mid-March, with twice as many women as men\nsaying they have been mistreated.\nJeung, who is researching racism and xenophobia amid the coronavirus pandemic, said a multilingual website set up by his department in a partnership with civil rights groups\nto document anti-Asian harassment has recorded more than 1,800 incidents since its March 19 launch. Victims said they were spat on, stabbed while shopping, shunned for\nwearing masks and barred from entering ride-hailing vehicles.\nSome academic experts on race say President Trump's rhetoric regarding the virus and China has contributed to the rise in racial harassment. For weeks, Trump deliberately\nreferred to the coronavirus as the \"Chinese virus\" despite guidance from public health officials to avoid attaching locations or ethnicity to a disease. He has since tweeted that\nAsian Americans are not to be blamed for the virus's spread.\n\"Words matter. People are making that close association between the virus and Chinese people because he insisted on using that term,\" Jeung said.\nDuring a media briefing last week, Trump lashed out at CBS News White House correspondent Weijia Jiang, who is Chinese American, telling her to \"ask China\" after she\nquestioned him on why he insisted on making testing a global competition at a time when so many lives are being lost. Jiang previously tweeted that a White House official had\ncalled the virus \"the Kung-Flu\" to her face.\nWhite House adviser Peter Navarro, in an ABC News interview Sunday, accused China of sending \"hundreds of thousands of Chinese\" to \"seed\" the coronavirus around the\nworld. And Sen. Ben Sasse (R-Neb.) drew criticism after he blamed the virus's spread on \"thugs in China\" in a high school graduation speech over the weekend.\nJeung said he expects harassment and violence against Asian Americans to grow in coming months as states reopen their economies and people return to work, school and\npublic life.\n\"With the China-bashing and with the economy tanking and more deaths from covid-19, we expect anti-Asian bias to only increase,\" Jeung said. \"People make automatic\nassumptions, especially in times of threats, and go into fight-or-flight mode. The fight mode is attacking or harassing Asians, and the flight mode is shunning Asians.\"\nFormer Democratic presidential candidate Andrew Yang has encouraged Asian Americans to step up and \"show our American-ness in ways we never have before,\" in response to\nthe rise in racist abuse. In a controversial op-ed last month, he called on Asian Americans to be part of the \"cure.\"\nBut many Asian Americans felt offended by seemingly having to justify their existence. \"It shouldn't matter if you're a front-line worker,\" said Esther Choo, an emergency room\nphysician in Portland, Ore., who hosts a podcast on the coronavirus pandemic. \"Every time something like this happens, there's this wave of, 'But we're so good, and we don't\ndeserve this.' No, you don't deserve this because you're human.\"\nBeing a front-line worker didn't help Li - and when she texted her colleagues to warn them before they left work, one of them responded with her own story of being harassed\njust a week earlier.\nGem Manalo, a Mass General anesthesiology resident who is of Chinese and Filipino descent, was riding the \"T,\" as the subway is"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194745, Requested 6421. Please try again in 349ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194745, Requested 6421. Please try again in 349ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Authorities are investigating a possible hate crime after a Willamette University student reported being pushed and kicked by two men in\ndowntown Salem while they allegedly made derogatory comments about her race.\nThe 21-year-old woman reported she was attacked at about noon Sunday on the corner of Capitol and Chemeketa streets while walking\nto Safeway, according to Lt. Treven Upkes, a spokesperson with the Salem Police Department.\nOn Monday, Willamette University Vice President of Student Affairs and Dean of Students Dr. Lisa Landreman sent an email to the school\ncommunity about the incident. \"Today we were horrified to learn about a racially-motivated hate crime involving physical violence and\nharassment, targeted at a Willamette student.\"\nUniversity officials have not identified the student involved. But in the letter, Landreman commented on the rise in violence against Asian\nAmerican and Pacific Islander individuals since the start of the COVID-19 pandemic.\n\"It's important to acknowledge these crimes and for all members of our community to resist ill-informed, biased perspectives about\nAsian people and to denounce this violence,\" she said.\nThe woman was approached by two men and pushed \"while derogatory comments were yelled at the student about their race,\"\nLandreman wrote.\nThe student fell to the ground and one of the suspects allegedly kicked her. She suffered minor injuries, officials said.\nThe student reported the incident to the Salem Police Department and Willamette University Campus Safety.\nThe first man is described as 5 feet 6 inches tall with a big build, long curly blond hair and facial hair. He was wearing a gray v-neck shirt\nand black snow hat.\nThe second man is described as 5 feet 11 inches tall with a thin build and thin brown hair. He was wearing a button-down shirt and a\nsnow hat.\nThe two men could face second-degree bias crime and harassment charges, Upkes said. Police have not made any arrests.\nSome politicians, including former President Donald Trump, blamed China for the COVID-19 pandemic, said Russell Jeung, who created a\ntool that tracks hate incidents against Asian American Pacific Islander communities called the Stop AAPI Hate tracker.\nSince the tracker was started nearly a year ago, more than 2,583 incidents have been reported nationwide. Of those, 30 were in Oregon.\n\"When President Trump began and insisted on using the term 'China virus,' we saw that hate speech really led to hate violence,\" said\nJeung, chair of the Asian American studies department at San Francisco State University. \"That sort of political rhetoric and that sort of\nanti-Asian climate has continued to this day.\"\nAnyone with more information about the incident is asked to contact the Salem Police Department at 503-588-6123.\nUSA Today reporter N'dea Yancey-Bragg contributed to this story.\nVirginia Barreda is the breaking news and public safety reporter for the Statesman Journal. She can be reached at 503-399-6657 or at\nvbarreda@statesmanjournal.com. Follow her on Twitter at @vbarreda2.\nThe woman was approached by two men and pushed \"while derogatory comments were yelled at the student about their race,\" the\nuniversity said."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195631, Requested 7385. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195631, Requested 7385. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "fectant and respirator masks, which are critical to protect against viruses.\nIn Venezuela, Human Rights Watch has documented a health system in utter collapse. Hospitals have closed or are operating at a fraction of their capacity, many without regular access to electricity or water. Vaccine-preventable diseases such as measles and diphtheria have returned long before the pandemic hit.\nBroad sanctions imposed by the US on Iran have drastically constrained the ability of the country to finance humanitarian imports, including medicines. This has caused serious hardships for ordinary Iranians. Concerned governments should support Iran's efforts to combat the COVID-19, including by providing access to medical devices and testing kits.\nIn Thailand, public health capacity has been diminished by corruption. Medical personnel lack surgical masks and local supplies have been diverted and shipped to China and other markets in part due to corruption.\nThe Health Ministry in Egypt in February sent doctors and medical teams to a quarantine facility without informing them that their transfer was part of the COVID-19 response or of the risks entailed. Medical staff said they were tricked into the assignment.\nIn Lebanon, the spokesperson for the country's medical supply importers told Human Rights Watch that the country had run out of gloves, masks, gowns, and other supplies necessary to deal with the coronavirus outbreak due to the financial crisis that had prevented them from importing needed goods. She added that medical supply importers have brought in just US$10 million of the $120 million in goods they have sought since October and nearly all transactions have been frozen since February due to the country's ongoing economic crisis. The head of the Syndicate of Private Hospitals said that the government owes private hospitals more than $1.3 billion, compromising their ability to pay staff and purchase medical equipment. Yet the Lebanese government has not put in place any measures to address the economic crisis threatening access to medical care, medicine, and medical equipment.\nRecommendations:\nGovernments should take measures so that health care is available to all, accessible without discrimination, affordable, respectful of medical ethics, culturally appropriate, and of good quality.\nGovernments should ensure that health workers have access to appropriate protective equipment and that social protection programs are in place for the families of workers who die or become ill as a result of their work, and ensure such programs include informal workers, who represent a large share of the caregiving sector.\nIn past epidemics, fear of exposure has led to attacks on health workers. Governments should monitor for such attacks to deter them, and ensure that they can quickly, adequately, and appropriately respond if attacks occur.\nFulfill the right to education-even if schools are temporarily closed\nMany countries have closed schools since the COVID-19 outbreak, disrupting the learning and education of hundreds of millions of students. In times of crises, schools provide children with a sense of stability and normalcy and ensure children have a routine and are emotionally supported to cope with a changing situation. Schools also provide important spaces for children and their families to learn about hygiene, appropriate handwashing techniques, and coping with situations that will break routines. Without access to schools, this prime responsibility falls on parents, guardians, and caregivers. When schools are closed, government agencies should step in to provide clear and accurate public health information through appropriate media.\nTo ensure education systems respond adequately, UNESCO has recommended that states adopt a variety of hi-tech, low-tech and no tech solutions to assure the continuity of learning. In many countries, teachers already use online learning platforms to complement normal contact hours in classrooms for homework, classroom exercises, and research, and many students have access to technological equipment at home. However, not all countries, communities, families, or social groups have adequate internet access, and many children live in places with frequent government-led internet shutdowns.\nRecommendations:\nOnline learning should be used to mitigate the immediate impact of lost normal school time. Schools deploying educational technology for online learning should ensure the tools protect child rights and privacy. Governments should attempt to recover missed in-person class time once schools reopen.\nGovernments should adopt measures to mitigate the disproportionate effects on children who already experience barriers to education, or who are marginalized for various reasons - including girls, those with disabilities, those affected by their location, their family situation, and other inequalities. Governments should focus on adopting strategies that support all students through closures - for example, monitoring students most at risk and ensuring students receive printed or online materials on time, with particular attention provided to students with disabilities who may require adapted, accessible material.\nGovernments should adopt mitigation strategies, for example by working with teachers, school officials, and teachers' unions and associations to factor in plans to recover teaching or contact hours lost, adjusting school calendars and exam schedules, and ensuring fair compensation for teachers and school personnel who are working additional hours.\nIn countries with high numbers of out-of-school children, school closures may jeopardize efforts to increase school enrollments and retention, particularly at the secondary level. Governments should place additional measures to monitor compliance with compulsory education - and ensure government education officials monitor school returns once schools reopen. Education officials should focus attention on areas with high incidence of child labor or child marriage and ensure all children return to school. Officials should also ensure that schools with refugee students adopt outreach measures to ensure refugee children return to school, including by working with refugee parent groups and community leaders.\nSudden school closures may also leave low-income families struggling to make ends meet and provide necessities. Governments should guarantee continued meal provision during school closures for children in low-income families who will miss subsidized meals.\nAddress disproportionate impacts on women and girls\nOutbreaks of disease often have gendered impacts. Human Rights Watch found that the 2014 Ebola virus disease outbreak and the 2015-2016 outbreak of the mosquito-borne Zika virus in Brazil had particularly harmful impacts on women and girls and reinforced longstanding gender inequity. News reports and public health analysis suggest that COVID-19 is disproportionately affecting women in a number of ways.\nThough risks specific to pregnant women exposed"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194677, Requested 7306. Please try again in 594ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194677, Requested 7306. Please try again in 594ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "who will miss subsidized meals.\nAddress disproportionate impacts on women and girls\nOutbreaks of disease often have gendered impacts. Human Rights Watch found that the 2014 Ebola virus disease outbreak and the 2015-2016 outbreak of the mosquito-borne Zika virus in Brazil had particularly harmful impacts on women and girls and reinforced longstanding gender inequity. News reports and public health analysis suggest that COVID-19 is disproportionately affecting women in a number of ways.\nThough risks specific to pregnant women exposed to COVID-19 are not yet clear, the outbreak could negatively affect sexual and reproductive health and rights. Overloaded health systems, reallocation of resources, shortages of medical supplies, and disruptions of global supply chains could harm women's access to contraception and pre- and post-natal and birth care. Although the risk of infection through breastfeeding is not known, the UN Population Fund has recommended that breastfeeding mothers who become ill should not be separated from their infants. Past epidemics, such as the Ebola outbreak in Sierra Leone, have impacted the availability of routine prenatal and maternity care, leaving women more at risk to preventable maternal deaths or morbidities.\nIn China, press reports suggest an increase in domestic violence under quarantine. Crises - and lockdowns - can trigger greater incidence of domestic violence for reasons including increased stress, cramped and difficult living conditions, and breakdowns in community support mechanisms. Crises can often further limit women's ability to get away from abuse, and place victims in an environment without appropriate access to services, such as safe shelter away from abusers and accountability for abuse.\nWomen globally do almost 2.5 times as much unpaid care and domestic work as men, and they are more likely than men to face additional care giving responsibilities when schools close, making it harder to maintain paid employment. Japan responded to the potential for a disproportionate impact on families with young children by offering to offset costs to businesses for workers taking paid leave to care for children during school closures, though the amount offered was low. Italy was considering measures to mitigate the effects of the lockdown on families with children. These could include emergency paid parental leave or vouchers for families with children up to 12 years old (or children with disabilities without any age limit) who need to pay for childcare amid the prolonged school closures.\nUp to 95 percent of female workers in some regions work in the informal sector where there is no job security, and no safety net if a crisis like COVID-19 destroys their earnings. Informal work includes many occupations most likely to be harmed by a quarantine, social distancing, and economic slowdown, such as street vendors, goods traders, and seasonal workers. Women are also over-represented in service industries that have been among the hardest hit by the response to COVID-19.\nWorldwide, 70 percent of health and social service providers are women - meaning women are at the front lines of containing the spread of COVID-19 and may be heavily exposed to the virus through work in the health sector. Fear in communities about the exposure that health workers face may lead women in this sector to be shunned or face stigma, adding an extra burden to the challenge of trying to protect their and their families' health. This may manifest itself, for example, in trying to access or secure childcare while they work on the front lines.\nSome female care workers are migrant domestic workers. They can be vulnerable to abusive employment conditions in normal times, and are at heightened risk of abuse, losing employment, being frontline caregivers without adequate protections, and of being trapped and unable to reach their homes during a crisis. They may also face barriers to protecting their own health.\nMoves toward telecommuting - for school and work - as a means of social distancing can disproportionately harm women and girls. Women are up to 31 percent less likely to have internet access than men in some countries, and worldwide about 327 million fewer women than men have a smartphone.\nEven when women have access to the internet, gender disparities may make them less able to use it for reasons including cost, socialization, and family pressures. When multiple members of a household need access to limited computing resources within the home, gender inequality may mean women and girls have less access.\nRecommendations:\nAuthorities should take steps to mitigate gendered impacts and ensure that responses do not perpetuate gender inequity.\nWhen education is moved online, governments and education providers should monitor participation and retention of students in online courses for a gendered impact and respond quickly with strategies to retain and reengage women and girls if their participation falls off. They should also address the particular risks of job losses to women who may take on additional caregiving during school closures.\nMeasures designed to assist workers affected by the pandemic should ensure the assistance of workers in informal work and service industries, who are predominantly women.\nGovernments should ensure public awareness campaigns address how victims of domestic violence can access services, and should ensure that services are available to all victims of domestic violence, including those living in areas under movement restrictions or under quarantine and those infected with COVID-19.\nGovernments should support frontline health and social service care workers with the recognition that these workers are mostly women. Support should include consideration of their needs as caregivers within their own families and the impact of stigma on them and their families.\nBoth source and destination countries for migrant domestic workers should adopt special measures to locate and assist migrant domestic workers to prevent abusive labor conditions and provide assistance relating to managing COVID-19.\nGovernments and international bodies should closely monitor the impact of COVID-19 on pregnant women and act to mitigate the impact of the pandemic on the right of women and girls to access sexual and reproductive health services.\nRoot out discrimination and stigma, protect patient confidentiality\nDuring previous public health crises, people with infection or disease and their families have often faced discrimination and stigma. For example, Human Rights Watch found that people living with HIV in Kenya, South Africa, the Philippines, and the US faced discrimination and stigma due to their HIV"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196080, Requested 7074. Please try again in 946ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196080, Requested 7074. Please try again in 946ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": ", but it wouldn't have to be said at all if the president himself had not fueled xenophobia in the way he referred to the virus.\"\nOuYang noted that the World Health Organization already advised not to attach locations or ethnicities to a disease as it could fuel stigmatization.\n\"History is repeating itself,\" OuYang said. \"Whenever there's a war or there's an economic [downturn] in the United States, oftentimes it's the minorities and immigrants who are scapegoated.\"\nIn recent weeks, the public hysteria as a result of the outbreak has reminded some Long Islanders of past periods of American history.\nJasleen Sabharwal, 60, of Oyster Bay still remembers the scene of a group of schoolchildren greeting her on Sept. 11, 2001, by yelling \"you ... [expletive] people did it again? Go back to your country\" as she walked from Manhattan to Queens.\nSabharwal, who emigrated from India in 1982, said the children yelled at her when she still had the white powder and fine glass on her face from the remains of the collapsed World Trade Center that day. Working at the time as an employee at Fiduciary Trust Company International, she lost nearly 100 colleagues after the south tower came down.\n\"Today, it's the Chinese. Tomorrow, it's the Indian,\" said Sabharwal, who survived the attacks because she had not yet reached her office on the 96th floor. \"It needs to stop now.\"\nGoing back even further in history, Glen Cove resident Bob Machida, 75, grew up learning about internment camps from his family members.\nDuring World War II, President Franklin D. Roosevelt issued an executive order to intern 120,000 people of Japanese ancestry in 10 camps after the Pearl Harbor attack on Dec. 7, 1941.\nBefore Machida was born, his aunt and uncle on the West Coast were sent to an internment camp in Utah, while his mother and father on the East Coast were tracked by an FBI agent.\n\"In a climate that's divisive like this, you could run into situations where some extremists may take advantage of a situation and say things they shouldn't,\" Machida said. \"We're one country, one multiracial country. And we're all in this together.\"\nFarrah Mozawalla, executive director of Nassau County's Office of Asian American Affairs, said people can report discriminatory incidents in New York by calling the Attorney General's Civil Rights Bureau at 800-771-7755 or the Nassau County Human Rights Commission at 516-571-3662. Long Islanders can find more resources on the Nassau County Asian American Affairs Facebook page. The latest facts on COVID-19 can be found by calling the Nassau County Coronavirus Call Center at 516-227-9570. Mozawalla said her office can translate all state and county information into Mandarin,"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195490, Requested 6041. Please try again in 459ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195490, Requested 6041. Please try again in 459ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": ", his parents would drive him and his siblings to Chinatown in Manhattan to visit their grandparents,\nrelatives and friends. By the end of the day, he would go home with dozens of money-filled red envelopes,\nknown as hongbao.\n\"It seemed that whole day, every single adult except your parents would give you something,\" he recalled\nwith a laugh. \"When you were a little child, you never had your own money. Receiving hongbao was exciting.\nIt didn't happen at any other time of the year. It didn't even happen during Christmas.\"\nQuan, 39, doesn't remember what he did with the money. What he does remember is taking the crisp bills\nout of the envelopes, putting them in a pile and counting them before putting the money away in a shoebox\nunder his bed.\n\"As a child, I didn't have a job. I didn't get an allowance,\" Quan said. \"Back then, I felt like it was the most\nmoney I ever had in the world.\"\nWhat else Quan remembers were the packed streets, crowded festivities and loud fireworks going off in\nChinatown at night.\n\"It felt like a day of fun,\" he said. \"It was special.\"\nNow a father of two, Quan said his family will downsize a typical 20-person inaugural New Year's meal at his\nparents' Bayside house to a small one with his wife, daughters and mother-in-law at their New Hyde Park\nhome.\nFor the bigger family that includes his parents, siblings, nephews and nieces, Quan said the family will likely\nget together on Zoom.\n\"It has been a hard year, and I think it's going to be a hard year still,\" Quan said. \"I'm optimistic about it,\nand I want to approach it in a positive way. But I think everybody realizes that there's still a lot of hard\nwork to be"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195306, Requested 7355. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195306, Requested 7355. Please try again in 798ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "JUSTICE DEPARTMENT\nUnited Airlines Pays\nTo Settle Mail Case\nUnited Airlines Holdings Inc. agreed to pay more than $49 million to settle criminal charges and civil claims\nrelated to the transportation of international mail, the Justice Department said Friday.\nUnited was accused of submitting false delivery-scan data between 2012 and 2015 to make it appear that the\nairline and its partners had transported mail in a timely manner to the intended recipient, the Justice\nDepartment said. Under the agreements, United was entitled to full payment when scans were provided and the\nmail was timely delivered, the agency said.\nThe Justice Department said United admitted to concealing problems related to scanning and mail movements\nthat, if known, would have subjected the carrier to financial penalties under the international contracts.\nUnited said it was \"glad to have remedied these procedures\" and called the U.S. Postal Service a valued\ncustomer.\nUnited agreed to pay about $17.3 million in criminal penalties and $32.2 million under a false claims settlement, the department said.\n-- Michael Dabaie\n---\nNEW YORK\nSchools Chancellor\nWill Leave Post\nNew York City Schools Chancellor Richard Carranza will leave his post in March as education officials work to\nreopen some public schools that have remained closed for most of the past year because of the Covid-19\npandemic.\nMeisha Porter, a longtime city Education Department official and the executive superintendent of Bronx schools,\nwill succeed Mr. Carranza, Mayor Bill de Blasio's office said Friday. Ms. Porter, 47 years old, will be the first Black\nwoman to serve in the role. She will succeed Mr. Carranza on March 15, city officials said.\nMr. Carranza said that he was proud of what he has accomplished in his time serving as the chief of the nation's\nlargest school district.\n\"While the work is never done, we have created a lot of important change together,\" he said.\nHe added that in the past year he has lost 11 family members and close friends and needed time to grieve.\nMr. Carranza tried to debunk speculation that his departure stemmed from tensions between the chancellor and\nthe mayor over decisions about reopening and policies on admission to selective programs.\nHe said he appreciated that the mayor \"has allowed all of us at the table to have differing views and to argue\nthose views and to advocate those views and come to a consensus.\"\nMr. de Blasio, a Democrat, leaves office on Dec. 31 because of term limits.\n-- Katie Honan\n---\nNEW YORK\nHate Crime Alleged\nin Man's Stabbing\nA Brooklyn man was charged with attempted murder as a hate crime after allegedly stabbing an Asian-American\nman in the back in an unprovoked attack in Manhattan's Chinatown, New York Police Department officials said\nFriday.\nThe suspect, Salman Muflihi, allegedly approached a 36-year-old man from behind on Thursday and stabbed\nhim, police officials say. The victim, who wasn't identified by police, was listed in stable condition at Bellevue\nhospital Friday. Mr. Muflihi may have been suffering from psychological distress at the time of the attack,\naccording to the officials.\nThe attack comes during a rise in violence in the city and after a string of unprovoked assaults on Asian-American residents, including incidents that police say were motivated by racial bias.\nMr. Muflihi was initially charged with attempted criminally negligent homicide, the officials said, but\nsubsequent investigation resulted in a reclassification of his alleged actions as a hate crime.\nNew York City saw 468 homicides in 2020, up nearly 47% from 2019.\n-- Ben Chapman"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197903, Requested 8101. Please try again in 1.801s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197903, Requested 8101. Please try again in 1.801s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "foreign workers for violating a mandatory 14-day leave of absence from work and ban them from working in the country raises concern of disproportionate penalties.\nRecommendations:\nGovernments should avoid sweeping and overly broad restrictions on movement and personal liberty, and only move towards mandatory restrictions when scientifically warranted and necessary and when mechanisms for support of those affected can be ensured. A letter from more than 800 public health and legal experts in the US stated, Voluntary self-isolation measures [combined with education, widespread screening, and universal access to treatment] are more likely to induce cooperation and protect public trust than coercive measures and are more likely to prevent attempts to avoid contact with the healthcare system.\nWhen quarantines or lockdowns are imposed, governments are obligated to ensure access to food, water, health care, and care-giving support. Many older people and people with disabilities rely on uninterrupted home and community services and support. Ensuring continuity of these services and operations means that public agencies, community organizations, health care providers, and other essential service providers are able to continue performing essential functions to meet the needs of older people and people with disabilities. Government strategies should minimize disruption in services and develop contingent sources of comparable services. Disruption of community-based services can result in the institutionalization of persons with disabilities and older people, which can lead to negative health outcomes, including death, as discussed below.\nProtect people in custody and in institutions\nCOVID-19, like other infectious diseases, poses a higher risk to populations that live in close proximity to each other. And it disproportionately affects older people and individuals with underlying illnesses such as cardiovascular disease, diabetes, chronic respiratory disease, and hypertension. Eighty percent of the people who have died of COVID-19 in China were over the age of 60.\nThis risk is particularly acute in places of detention, such as prisons, jails, and immigration detention centers, as well as residential institutions for people with disabilities and nursing facilities for older people, where the virus can spread rapidly, especially if access to health care is already poor. States have an obligation to ensure medical care for those in their custody at least equivalent to that available to the general population, and must not deny or limit detainees, including asylum seekers or undocumented migrants, equal access to preventive, curative or palliative health care. Asylum seekers, refugees living in camps, and people experiencing homelessness may also be at increased risk because of their lack of access to adequate water and hygiene facilities.\nIn nursing facilities and other settings with large numbers of older people, visitor policies should balance the protection of older and at-risk residents with their need for family and connection. The US Department of Veterans Affairs announced a no visitors policy at its 134 nursing homes around the country in response to the risk of COVID-19. While the risk to older people is serious, blanket policies do not take into account public health guidance or the needs of older people.\nPeople in prisons, jails, and immigration detention centers frequently do not receive adequate health care under normal circumstances, even in economically developed countries. Severely substandard health care has contributed to recent deaths of immigrants in the custody of US Immigration and Customs Enforcement. Populations in custody often include older people and people with serious chronic health conditions, meaning they are at greater risk for illness from COVID-19.\nMany people in US jails have not been convicted of a crime but are locked up simply because they cannot afford to pay the bail set in their case. Older men and women are the fastest growing group in US prisons due to lengthy sentences, and prison officials already have difficulty providing them appropriate medical care. As a response, in one county in the US state of Ohio, the courts expedited review of people in jail, releasing some and transferring others to prisons. The American Civil Liberties Union has filed a lawsuit that seeks to challenge ongoing immigrant detention in the context of the virus.\nPrisoners in Iran have reportedly tested positive for the coronavirus, including in Evin prison in Tehran and in the cities of Euromieh and Rasht. In an open letter in February, families of 25 prisoners detained for peaceful activism sought their at least temporary release amid the outbreak and lack of sufficient prison medical care. In March, the Iranian judiciary 1⁄4Դ3⁄4ÅҴ! ¡ÒÃÍéҧÍԧ¡ÒÃàa×èÍÁâ§ËÅÒÂÁԵÔäÁè¶١μéͧ about 85,000 prisoners for the Persian New Year (Nowruz), a substantially greater number than normal for the holiday, apparently because of health concerns surrounding the coronavirus outbreak. However, dozens of human rights defenders and others held on vaguely defined national security crimes remained in prison.\nOn March 12, Bahrain's King Hamad bin Isa Al-Khalifa reportedly pardoned 901 detainees for humanitarian reasons, in the backdrop of the current circumstances, likely in reference to the coronavirus outbreak. The Ministry of Interior announced that another 585 detainees would be released and granted non-custodial sentences.\nIn Italy, prisoners in over 40 prisons have protested over fears of contagion in overcrowded facilities and against bans on family visits and supervised release during the coronavirus pandemic. In response, authorities have authorized for the first time the use of email and Skype for contact between prisoners and their families and for educational purposes and announced a plan to release and place under house arrest prisoners with less than 18 months on their sentence. The main prisoner rights organization in Italy, Antigone, estimated this could benefit at most 3,000 prisoners, while the penitentiary system is at around 14,000 over capacity. The organization called for broader measures to ensure the release of a greater number of detainees, including in particular older detainees and those with at-risk health profiles, among other measures. Civil society organizations have also called for alternatives to detention for all people currently detained"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197540, Requested 8172. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197540, Requested 8172. Please try again in 1.713s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "and Political Rights (ICCPR), requires that restrictions on rights for reasons of public health or national emergency be lawful, necessary, and proportionate. Restrictions such as mandatory quarantine or isolation of symptomatic people must, at a minimum, be carried out in accordance with the law. They must be strictly necessary to achieve a legitimate objective, based on scientific evidence, proportionate to achieve that objective, neither arbitrary nor discriminatory in application, of limited duration, respectful of human dignity, and subject to review.\nBroad quarantines and lockdowns of indeterminate length rarely meet these criteria and are often imposed precipitously, without ensuring the protection of those under quarantine - especially at-risk populations. Because such quarantines and lockdowns are difficult to impose and enforce uniformly, they are often arbitrary or discriminatory in application.\nFreedom of movement under international human rights law protects, in principle, the right of everyone to leave any country, to enter their own country of nationality, and the right of everyone lawfully in a country to move freely in the whole territory of the country. Restrictions on these rights can only be imposed when lawful, for a legitimate purpose, and when the restrictions are proportionate, including in considering their impact. Travel bans and restrictions on freedom of movement may not be discriminatory nor have the effect of denying people the right to seek asylum or of violating the absolute ban on being returned to where they face persecution or torture.\nGovernments have broad authority under international law to ban visitors and migrants from other countries. However, domestic and international travel bans historically have often had limited effectiveness in preventing transmission, and may in fact accelerate disease spread if people flee from quarantine zones prior to their imposition.\nIn China, the government imposed an overly broad quarantine with little respect for rights:\nIn mid-January, authorities in China quarantined close to 60 million people in two days in an effort to limit transmission from the city of Wuhan in Hubei province, where the virus was first reported, even though by the time the quarantine started, 5 million of Wuhan's 11 million residents had left the city. Many residents in cities under quarantine expressed difficulties obtaining medical care and other life necessities, and chilling stories have emerged of deaths and illnesses: A boy with cerebral palsy died because no one took care of him after his father was taken to be quarantined. A woman with leukemia died after being turned away by several hospitals because of concerns about cross-infection. A mother desperately pleaded to the police to let her daughter with leukemia through a checkpoint at a bridge to get chemotherapy. A man with kidney disease jumped to his death from his apartment balcony after he couldn't get access to health facilities for dialysis. Authorities have also reportedly used various intrusive containment measures: barricading shut the doors of suspected infected families with metal poles, arresting people for refusing to wear masks, and flying drones with loudspeakers to scold people who went outside without masks. The authorities did little to combat discrimination against people from Wuhan or Hubei province who traveled elsewhere in China.\nIn Italy the government has imposed a lockdown but with greater protections for individual rights. The Italian government adopted progressively restrictive measures since the first major outbreak of COVID-19 cases in the country in late February. Authorities initially placed ten towns in Lombardy and one in Veneto under strict quarantine, prohibiting residents from leaving the areas. At the same time, they closed schools in affected regions. Citing a surge in cases and an increasingly unsustainable burden on the public healthcare system, the government on March 8 imposed a slew of new measures on much of the country's north that put in place much more severe restrictions on movement and basic freedoms. The next day, the measures were applied across the country. Further measures imposed included restrictions on travel except for essential work or health reasons (upon self- certification), closure of all cultural centers (cinemas, museums), and cancellation of sports events and public gatherings. On March 11 the government closed all bars, restaurants, and stores except food markets and pharmacies (and a few other exceptions) across the country. People who disobey the travel restrictions without a valid reason can be fined up to 206 euros and face a three-month prison term. All schools and universities were closed throughout the country. People have been allowed out to shop for essential items, exercise, work (if unable to perform work from home), and for health reasons (including care for a sick relative).\nOther governments, such as those in South Korea, Hong Kong, Taiwan, and Singapore have responded to the outbreak without enacting sweeping restrictions on personal liberty, but have reduced the number of travelers from other countries with significant outbreaks. In South Korea, the government adopted proactive and ramped-up testing for COVID-19. It focused on identifying infection hotspots, conducting a large number of tests on at-risk people without charge, disinfecting streets in areas with high numbers of infections, setting up drive-through testing centers, and promoting social distancing. In Hong Kong, there have been concerted efforts to promote social distancing, handwashing, and mask-wearing. Taiwan proactively identified patients who sought health care for symptoms of respiratory illness and had some tested for COVID-19. It also set up a system that alerts the authorities based on travel history and symptoms during clinical visits to aid in case identification and monitoring. Singapore adopted a contact-tracing program for those confirmed to have the virus, among other measures. However, the government's decision to deport four foreign workers for violating a mandatory 14-day leave of absence from work and ban them from working in the country raises concern of disproportionate penalties.\nRecommendations:\nGovernments should avoid sweeping and overly broad restrictions on movement and personal liberty, and only move towards mandatory restrictions when scientifically warranted and necessary and when mechanisms for support of those affected can be ensured. A letter from more than 800 public health and legal experts in the US stated, Voluntary self-isolation measures [combined with education, widespread screening,"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193031, Requested 7075. Please try again in 31ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193031, Requested 7075. Please try again in 31ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Hi everyone! Welcome to the \"This is America\" newsletter. If you don't recognize my name, no worries, because I'm new here! I'm Jenna Ryu, a Life and Travel editorial intern, and a senior at Georgetown University.It's the jolly month of December, and we're still living in a revolutionary coronavirus pandemic caused by the so-called \"China Virus,\" also known by its catchier moniker, \"Kung Flu.\"Funny, right? As punny as it sounds, this wordplay (along with the incessant bat jokes) is just one example of the normalized racism that's come into the spotlight this year.Often referred to as the model minority, Asian Americans are praised for things like economic success and work ethic. The stereotypes: We're hard-working, but we don't bad-mouth the country. We economically outpace others while staying polite and reserved. We worked our way up to the American Dream without publicly voicing our struggles with prejudice, racism and discrimination along the way.It feels like these microaggressions, xenophobia and the whole model minority myth have become normalized to the point where they are overlooked. And in a year of so much reckoning – yet so much blatant racism against Asian Americans – it has reached a breaking point for Asian Americans' mental health.But first: Race and justice news we're watching\nImportant stories of the past week, from USA TODAY and other news sources.\nAsian Americans in San Francisco are dying of COVID-19 at alarming rates: Racism is to blame\nMichigan man imprisoned for nearly 4 decades exonerated after witness admits lyingI should be flattered that I'm stereotyped as \"studious,\" right?It seems like a compliment to be known as \"hard-working\" and \"good at math,\" right? Better than being referred to as a \"thug\" or \"bad hombre.\"Here's the thing: The model minority myth inherently raises the standards for Asian Americans – both internally and externally. It's assumed that because of how we look and where we're from, we should all be able to achieve great feats without difficulty.I feel an immense pressure to meet ridiculously high standards that are imposed on Asians, for the fear that if I don't, I'm automatically \"dumb,\" \"disappointing\" and a \"failure.\" Insecurity and self-doubt are all too familiar when others express shock that math isn't my strong suit, or that my English is surprisingly \"good\" for an Asian.Dr. Derek Iwamoto, an Asian American counseling psychologist, notes that some Asian Americans may \"crumble under the pressure\" and experience depression and anxiety when failing to meet expectations such as attending a prestigious university or becoming a successful doctor.Asian Americans are three times less likely to seek mental health services compared with their white counterparts, despite similar rates of depression and anxiety.Iwamoto attributes this not only to cultural differences, but also the model minority trope, which encourages Asians to be successful while being independent.\"If you believe all Asian Americans are successful, but you're Asian with unemployment issues, you might not seek help because you feel you must be self-reliant,\" he said. \"It's hard to live up to an unrealistic notion to achieve all of these things when it's really difficult given all the barriers Asian Americans experience.\"Asians are not exempt from racismSo let's talk about those \"barriers,\" as Iwamoto calls them.Assumptions that we're intelligent and successful often overshadow a reality in which our cuisines are seen as \"weird\" and \"smelly,\" or that our eye shape is considered ugly for not conforming to Eurocentric beauty standards. Worst of all, we're taught that these insensitive behaviors are less serious, and therefore more acceptable.When I've complained about those moments of harmful microaggressions, I've been told to shut up.This summer, in a trend that spanned the country, students and alumni of color at my private high school turned to Instagram to expose racism we'd experienced on campus.My voice finally felt heard when I shared my story about a dorm head who made little effort during my first year to learn my name and mistook me for another Asian girl for over a month.But my pride in sharing was soon overshadowed. The faculty member's friends and family members began invalidating my experience, questioning my authenticity and sarcastically mocking what happened and how it made me feel.Being told that the appropriate response to racism is to lighten up, laugh and stop being sensitive is a form of gaslighting. If Asian Americans are so successful, then how could these jokes about eye shape or accents actually be harmful?This year's Census Bureau data found that the percentage of Asian Americans who experienced anxiety- related issues increased from 28% to 34%, an increase of about 800,000 people.Even before the racism borne of the pandemic became common, mental health issues have been on the rise for Asian American young adults. SAMHSA's National Survey on Drug Use and Health reported that the percentage of Asian Americans age 18-25 reporting serious mental health issues rose from 2.9% to 5.6% between 2008 and 2018.No matter how successful we are, we still don't belong.Sometimes, it feels like no matter how hard we work or how successful we become, Asian Americans are still just a minority that doesn't belong. It's a phenomenon Iwamoto refers to as being \"perpetual foreigners.\"We all felt the sudden shift toward scapegoating Asians during the pandemic. \"Go back to China!\" \"You're all foreigners!\" It's as if the country that so proudly lauded the Oscar-winning \"Parasite\" and K-pop music suddenly forgot about our contributions.According to the 2020 Asian American Voter Survey, 51% of respondents were concerned about COVID-19- related hate crimes, and more than"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195374, Requested 7949. Please try again in 996ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195374, Requested 7949. Please try again in 996ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Coughing is now a doubly serious concern for Asian Americans. Like everyone else, we're afraid of contracting the coronavirus. As a racial group, we have an additional fear:\nbeing profiled as disease-carriers and being maliciously coughed at.\nAfter news of the coronavirus broke in January, Asian Americans almost immediately experienced racial taunts on school campuses, shunning on public transit and cyberbullying\non social media. When President Trump insisted on labeling the coronavirus the \"Chinese virus\" in early March, these attacks became more virulent and common.\nThe FBI now warns of an increase of hate crimes against Asian Americans, but we've already experienced a surge.\nSince the Stop-AAPI-Hate website, a project of the Asian Pacific Planning and Policy Council and Chinese for Affirmative Action, launched on March 19 to track anti-Asian\nharassment, it has received more than 1,000 reports from people in 32 states detailing verbal abuse, denial of services, discrimination on the job or physical assaults.\nSeveral people have reported others coughing at them, including this frightening incident: \"A white man on open sidewalk approached and stepped directly in front of me and\ncoughed in extremely exaggerated manner in my face -- loudly, mouth wide open, about 2 feet from my face, said 'take my virus.'\"\nIn Texas, the FBI is investigating the stabbing of a father and two children from Burma by an assailant who blamed them for the pandemic. This case harks back to the 1982\nmurder of Chinese American Vincent Chin, whose killers believed he was Japanese and responsible for the decline of the American auto industry.\nSadly, this kind of racist resentment is a recurring pattern in American history, particularly during health crises or in wartime.\nWhen the bubonic plague arrived in 1900, the San Francisco Health Department quarantined that city's Chinatown with barbed wire and ropes. While white citizens were allowed\nto leave, 30,000 Chinese were segregated and confined. During World War II, fear and racist hysteria led to the unconstitutional relocation and imprisonment of 120,000\nJapanese Americans.\nThe current vilification of Asian Americans is reminiscent of the scapegoating of Arabs, Muslims and South Asians after 9/11. Hate crimes against Muslim Americans surged in\n2001 and remained elevated above pre-9/11 levels years later.\nEthno-religious groups, like Sikhs from India, also suffered attacks and discrimination after 9/11.\nImmediately after 9/11, President Bush denounced those who intimidated Muslims and called on the nation to treat this American community with respect. In contrast, Trump\nrepeatedly used the term the \"Chinese virus\" despite warnings that this would incite racist threats against Asian Americans. His tepid retreat on that rhetoric last week did little\nto stem the harm. The damage is done.\nRepublicans in Congress continue to blame China as the source of the virus, rather than focusing on how to control the spread of COVID-19. Trump's language, along with media\ncoverage of his remarks, have framed how Americans view Chinese people, and correspondingly, Asian Americans.\nEven the face mask has become racialized, with some Asian Americans fearing that wearing one in public would draw attention and make them targets for physical attacks.\nAmerican history repeats itself in another way. Asians in the U.S. have always fought the discrimination they faced. Chinese Americans filed lawsuits against the Chinese\nExclusion Act and engaged in mass resistance to unjust government policies. Japanese Americans won reparations for their wartime incarceration. Muslim Americans are gaining\npolitical power and have run for office in record numbers, with two recently elected to Congress.\nLast week, Rep. Grace Meng (D-N.Y.) introduced a resolution calling on all public officials to condemn anti-Asian bias and on federal law enforcement officials to collect data to\ndocument and investigate hate crimes tied to COVID-19.\nState leaders like Gov. Gavin Newsom should form a multi-agency task force to coordinate efforts to combat racial bias spurred by the pandemic. They should notify stores to\nprovide safe access to their goods and warn employers about workplace discrimination. And schools and colleges need to provide culturally competent mental health services to\nAsian American students and other affected communities.\nThis pandemic requires us to stop the spread of both COVID-19 and racial hatred. Asian Americans need allies who will intervene when they see racial profiling happening.\nWe need to learn from American history and have the courage and leadership to counteract fear and anxiety in this time of crisis.\nCredit: Russell Jeung is chair of Asian American Studies at San Francisco State University. Manjusha P. Kulkarni is executive director of the Asian Pacific Planning and Policy\nCouncil. Cynthia Choi is executive director of Chinese for Affirmative Action."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193155, Requested 7926. Please try again in 324ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193155, Requested 7926. Please try again in 324ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "It's been about a year since Ellen Hartman took a walk with her husband and two young daughters along the Susquehanna riverfront in\nHarrisburg, trying to get some fresh air and a respite amid a brewing pandemic.\nShe found little peace. Hartman, the senior vice president of the Central PA Korean Association, said she was accosted during the walk\nby four young men shouting at her.\n\"There were a group of four males who began taunting me, yelling, calling me the coronavirus, throwing their arms towards me,\" she\nsaid.\nBecause she had her 3-year-old daughter with her, Hartman said she didn't want to react.\n\"You could tell these four males are very angry and in their tone profanities, and my 3 year old was waving at them. She thought, 'oh\nlook, they're saying hello to me,'\" she said.\nHartman didn't report the incident to the police or state officials. Instead, she reported it to the Community Responders Network, a\ngrassroots coalition that works to reduce bias incidents in central Pennsylvania.\nHer account was the third report on a list of 33 for 2020. The first report describes an unknown man in a pickup truck driving past an\nAsian American Dickinson College student and yelling, blaming her for the coronavirus.\nThat was last March, when there were less than 50 reported cases of COVID-19 in Pennsylvania. Since then, the virus and\ndiscrimination against Asian Americans and Pacific Islanders have grown in the state.\nWhile the Community Responders Network has tallied at least 33 incidents, Chad Lassiter, the executive director of the Pennsylvania\nHuman Relations Commission, is reluctant to quote a figure, but recognized there has been an increase.\n\"We've seen an uptick throughout the state. It's challenging to gather numbers. Often times, these incidents go unreported or\nunderreported,\" he said.\nFor example, Hartman did not report her case to the state Human Relations Commission.\nThe uptick in Pennsylvania is in line with what's happening nationally:\nThe New York Police Department reported a 1,900% increase in reported hate crimes against Asian Americans in 2020.\nAlmost daily reports from the Bay Area of California show an increase in violence against Asian Americans, particularly elderly Asian\nAmericans.\nStop AAPI Hate, a database that has tracked racial violence during the coronavirus, received more than 2,800 reports of anti-Asian\ndiscrimination from March through the end of December.\nA rise in hate crimes against Asian Americans\nDiscrimination against Asian Americans and Pacific Islanders continue to be reported this year.\nShortly after taking office in January, President Joe Biden signed an executive order Jan. 26, condemning racism, xenophobia and\nintolerance against Asian Americans and Pacific Islanders.\nThe order also instructed the COVID-19 Health Equity Task Force to issue guidance and best practices for advancing cultural\ncompetency, language access and sensitivity towards Asian Americans and Pacific Islanders.\nBiden also asked the U.S. Attorney General's Office to expand the collection of data and public reporting regarding hate incidents\nagainst Asian Americans and Pacific Islanders.\nOn Feb. 22, Pennsylvania Gov. Tom Wolf's Advisory Commission on Asian Pacific American Affairs condemned recent attacks on Asian\nPacific Americans across the country and called for all Pennsylvanians to stand up against anti-Asian hate and racism in all forms.\n\"From Oakland, California to Brooklyn, New York, innocent Asian elders have been severely injured or killed in wanton acts of violence.\nHere in Pennsylvania, witnesses have reported more insidious forms of hate, including threats of bodily harm to Asian American high\nschool students and the casual use of the term \"COVID\" to name Asian-influenced food,\" a statement from the governor's office said.\nBut discrimination against Asian Americans and Pacific Islanders is not a new problem. It's an injustice that has been pervasive for\ndecades.\nReporting and documenting\nHartman said she has experienced racist hate crimes since childhood. Growing up outside Philadelphia, her family's house was tarred\nwith swastikas, a brick was thrown through her sister's window, and an attempt was even made to blow up her father's car.\nBut the perpetrators never faced justice, she said.Now that hate crimes and racism against Asian-Americans and Pacific Islanders are getting more attention, Hartman and other Asian-\nAmericans in Central Pennsylvania point to some recent generators of bigotry.That includes disinformation surrounding the coronavirus and the words of former President Donald Trump, who called COVID-19 \"the\nChina virus\" and \"kung flu\" during his 2020 campaign stops in Pennsylvania and throughout his nationally televised speeches.\nHartman said the actions of Trump created an environment where scapegoating is permitted, accusing the former president of \"reducing\nus to the virus, dehumanizing us, and using phrases like 'the China virus' and the 'kung flu.'\n\"There is that stereotype, you know, the \"kung flu,\" said Joseph Romanoff, who is biracial and teaches Zumba classes at the New York\nFitness Club in Lebanon and whose mother came to the U.S. from the Philippines.\n\"But (this has) been going on for a long time. It's not in our heads.\"\nThe difference, he said, is that now, \"racist attacks are being recorded and documented on social media.\"\nState Rep. Patty Kim, D-Dauphin County, has served the 103rd district since 2013 and is a former Harrisburg city councilwoman. She\nsaid she has been concerned that many hateful incidents against Asian-Americans were going unreported.\n\"Growing up, I've had kids calling me names and pointing out I'm different. I thought that was the way of life,\" Kim said. \"People feel\nvery bad and embarrassed, and just want it to go away"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193223, Requested 7889. Please try again in 333ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193223, Requested 7889. Please try again in 333ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK — The first disaster in a year of perpetual crises hit the New York Police Department in March, when the virus tore through the force, killing dozens and sickening entire detective squads. Soon, the courts ground to a halt. By June, hundreds of officers were reassigned to cover mass protests against police brutality and racism, where police and protesters sometimes clashed violently. By August, gun violence was surging. And as December drew to a close, New York's 447 homicides made 2020 the city’s bloodiest year in nearly a decade. The hobbled criminal justice system strained to contain the rise in violent crime caused by the pandemic’s societywide upheaval. “I can’t imagine a darker period,” Police Commissioner Dermot F. Shea said in a year-end briefing with reporters on Tuesday, citing the confluence of the pandemic and the protests. The year’s crime numbers give shape to 2020’s tumult: Transit crime and grand larceny, often the stealing of laptops or iPhones of straphangers, plummeted as trains emptied out. But burglaries and car thefts spiked in a hollowed-out city. And bodegas, neighborhood staples during the throes of the pandemic, saw an increase in robberies and shootings. By summer, the frustrations of shutdowns and economic collapse had burst onto the streets. Shootings had doubled, and most of them were concentrated in the areas hardest hit by the coronavirus and unemployment. The increase in violence resembles trends in many big U.S. cities, where shootings and homicides have risen even as the pandemic has driven down other crimes. “We’ve never had a year like this in policing, when you’ve had a combination of a worldwide health epidemic and a challenge to community trust,” said Chuck Wexler, the executive director of the nonprofit Police Executive Research Forum. “That has been a combustible mixture. And the police in many ways were not prepared.” Shea said Tuesday that the department was caught flat-footed this summer when tens of thousands of protesters took to city streets after the killing of George Floyd at the hands of Minneapolis police officers. The demonstrations kicked off a national debate about policing and led many departments, including New York’s, to slice budgets and redirect funding to community programs. “There’s things that we probably should have done years and years ago,” Shea said, noting that the department had spent the months since June retraining many patrol officers and rethinking its approach to protests. Dozens of incidents were recorded of police officers brutalizing demonstrators, many of them peaceful, during protests. The state attorney general kicked off an investigation into the department’s aggressive enforcement. And earlier this month, a city watchdog chided police officials for being unprepared and out of touch with the current discourse about police brutality. Police officials in New York have pointed to gang disputes as a key driver of the violence over the summer, but several bystanders were caught in the cross hairs: a 43-year-old mother, killed by a stray bullet that went through her bedroom window in Queens; a man fatally shot on a handball court in Brooklyn; a 1-year-old boy, dead after a gunman opened fire on a cookout, also in Brooklyn. But many cases were stalled because the pandemic had forced the courts to operate virtually. Hardly any new trials were conducted, and the progress of many cases was significantly slowed. “I think we’ve struggled a little bit because of COVID, and how courts were closed, but when things start opening up, we have a lot of great work in the hopper ready to go, to really close some of the violence that we saw in 2020,” said Rodney Harrison, the Police Department’s chief of detectives.Combating street feuds has become a sort of routine for the police, particularly in the warmer months when turf battles and social media fights can lead to spikes in gun violence in certain neighborhoods. But officials and experts have said that something about the summer’s violence felt less predictable, and that made anticipating trends more difficult. “I think it’s about something more, something out there about the anxiety, and the fact that a lot of our institutions are not functioning the way they usually do,” Wexler said of the violence. “If it was just New York, I think that would be one thing. But because the crime increase in homicides is widespread, I think it says something bigger about what’s going on.” Despite the violent summer, crime numbers in the city remained well below the dark days of the 1980s and 1990s, when New York saw more than 2,000 murders a year. Homicides and shootings have plummeted in recent years, even in some of the city’s most notoriously dangerous corners. Had 2020 not been such an anomaly, officials have said, that trend might have continued. This year, as crime increased, the police solved less of it. Police Department records, for example, showed that officers solved 26.3% of serious crimes in the second quarter of the year; department figures show that 35.8% of serious crimes were solved over the same period in 2019. “I think COVID played a role earlier in the year, where we had a significant amount of people out,” Shea said, noting that in the early days of the pandemic when many officers became sick, entire teams of detectives filled in for other squads, often in unfamiliar neighborhoods. The clearance rate improved from 26.3% later in the year, he said, but still fell well short of 2019’s level. Critics of the police have questioned whether officers, chafed by the summer’s unrest and the national debate over law enforcement, began responding more slowly to calls. But some experts say much of the department’s low clearance rate is tied to difficulties caused by the pandemic — officers cannot interact as widely with the public, and most people, including criminals, are wearing masks."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195615, Requested 7013. Please try again in 788ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195615, Requested 7013. Please try again in 788ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "watch out because I know where you live,\" one person wrote.\nThe state gave Harris an around-the-clock security detail and a ballistic vest. They told him to\nfling it on and lock his door if he heard a loud noise.\nHarris's experiences were not unique: State health officers around the country experienced\nsimilar treatment, with hundreds facing threats or resigning.\n***\nIvey's change in policy made Alabama one of the first states to lift restrictions following\nencouragement from Trump, who by mid-April had called for states to reopen their\neconomies.\nAfter the Alabama governor gave up the lectern to Harris, he struck a markedly different\ntone. His mask down around his chin, Harris spoke soberly about data emerging in Alabama,\nas it was throughout the country, that showed that the virus disproportionately impacted\nBlack and Hispanic Americans. Black Alabamians represented a quarter of the state but half of\nits COVID-19 deaths to that point.\nThe decision to reopen most of Alabama may have been welcome to many residents,\nincluding business owners. To Linda Gilchrist, a nurse who works at a Montgomery senior\nhome, it was devastating. After finishing a shift — followed by a lengthy clothes-washing and\nshowering routine so as to not infect her family — she caught Ivey's press conference on\ntelevision.\nDozens of Gilchrist's patients had died from the virus, and she had stood in a meeting that\nday where stakeholders discussed getting more patients into the rehab unit. In Montgomery,\ncoronavirus cases had been steadily rising. Ivey's policy change certainly wasn't based on the\nstate having flattened the curve.\n\"Oh my God,\" Gilchrist thought to herself. \"Why are we doing this?\"\nWilliam Boyd, a lifelong resident of deeply segregated, majority-Black Montgomery, thought\nhe had an explanation. Boyd had lost six relatives or close friends to the virus, including his\nbrother. He saw the growing apathy toward the virus in Alabama as a result of policy-makers\nlearning that Black people were most vulnerable. The CDC had just come out with that\ninformation on April 8.\nThe hospital where Boyd's brother died had a policy allowing loved ones access if they were\nnear death, and he described sharing a waiting area with those who were about to say\ngoodbye to a patient. \"It was an ugly sight, and I saw all Black people,\" Boyd said.\nBoyd addressed the Montgomery City Council in June when they considered a mask mandate.\n\"The question on the table is whether Black lives matter,\" he said. The mandate failed after\nthe eight-member council voted mostly along racial lines, Black members for the mandate\nand white members against it.\nThe next month, the Montgomery council members reversed themselves and passed the\nmandate. Ivey also revisited the statewide mask issue during another press conference,\nroughly six weeks after the one where she'd cracked the joke about her hairdresser. Harris\nwas back at her side.\nCases in Alabama had more than doubled in the month before the announcement, to more\nthan 60,000, adding more than 1,100 cases per day on average.\n***\nThe unique rub of the novel coronavirus is that attempts to defeat it tend to open up\nanother front of misery. Extended lockdowns deliver their own world of grief, as the\nmillions of newly unemployed Americans can attest. The tug-of-war between industry and\npublic health has often resulted in public policy that keeps the economy afloat at the\nexpense of the health, and often lives, of those with the least agency.\nNowhere has that been more clear than in the federal government's handling of the\nmeatpacking crisis.\nIn mid-April, Minnesota hog farmer Greg Boerboom heard from a representative of\nSmithfield Foods that the meat conglomerate had to delay its weekly purchase of several\nsemi loads of his animals due to illness at the company's Sioux Falls plant.\nBut Boerboom wasn't worried the disruption would extend any longer than a few days. The\nmeat industry had eradicated uncertainty a long time ago. Boerboom contracts with the\nfew multinational corporations that control the meat processing industry. It's a departure\nfrom the system he learned as a young man on his father's farm, where they'd haul 30\nhogs at a time behind their '59 Chevrolet three hours away to the stockyards in St. Paul to\nsearch for buyers.\nCritics deride the ruthless efficiency of the business but Boerboom, 59, sees progress. \"We\ncan glamorize the good old days,\" he said, \"but the fact of the matter is they weren't that\nglamorous.\"\nBut the drawbacks of the modern meatpacking model began to show when a\nrepresentative for Tyson Foods also called Boerboom to say COVID-19 cases were forcing\nthe temporary closure of the company's plant in Waterloo, Iowa.\nThe two plants represented roughly 60% of Boerboom's business. And as a month came\nand went, and the plants remained closed, his stock of roughly 100,000 overgrown hogs\ncrowded against each other in his barns. Boerboom had to consider ways to quickly reduce\nhis inventory, including whether to execute a large number of them with a handgun.\nHis colleagues were doing it, those who weren't pumping carbon monoxide into their barns\nto kill thousands of their hogs. All across the country, hog farmers faced the same grim\nscenario.\n***\nThe virus had forced farmer Greg Boerboom to revert to his dad's old way of doing\nbusiness, selling a few hogs here or there and stocking the butcher shops on nearby Main\nStreets. But Boerboom isn't the nostalgic type. When the Smithfield and Tyson buyers\nreturned in late May, he rejoiced"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193284, Requested 6905. Please try again in 56ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193284, Requested 6905. Please try again in 56ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Guwahati, April 2 -- The Covid-19 is set to dominate the future debates on globalization, but its impact is not merely limited to the field of health or international relations. Donald Trump, the President of the United States, called the Covid-19 as the 'Chinese virus' or 'China virus' on multiple occasions. In his defence, he had pointed out that the term was not racist at all as the virus was said to have originated in that country. However, he changed his position and tweeted that the spread of the novel coronavirus in the US was not the fault of the Asian-Americans. Partly responsible for this change of heart may be the fact that the Asian-Americans have been a target of growing number of racist and xenophobic attacks related with the virus.\nHowever, closer home, disturbing facts related to an 'old virus' but having mutated to a new form are doing the rounds in many parts of India now. This pertains to a spike in racist taunts/attacks against the northeasterners with reference to the novel coronavirus. One would like to call it Racism-19 for the sake of brevity.\nThere are instances galore of this latest trend which is raising its ugly head. The most infamous being how a Manipuri girl was spat on and called 'corona' while she stepped out to pick up essentials in North Delhi. Fortunately in this case, the police acted promptly and arrested the culprit.\nIn Kolkata, 'Go corona go' chants could be heard in a particular locality. No, this wasn't any community confidence building exercise to tide over the dark clouds hanging over all of us due to the pandemic, but a mob that had gathered outside the flat where three women from Nagaland resided. The crowd was adamant that the women ' so- called 'carriers of the virus' ' should immediately leave their house as well as the locality. When the incidents like these occur, it is difficult to understand whether the crowd was driven by genuine fear of Covid-19 or outright racism. As the videos doing the rounds on social media show, the crowd was not maintaining any social distancing, and hence the possibility of the former appears low.\nIn another harrowing tale, the members of a housing society in Ahmedabad harassed some students from the Northeast and pressurized them to vacate the house and move out. Fortunately, the cops intervened but imagine the mental agony of having to live with such neighbours who would do anything to push you out. What is difficult to understand is that the students neither had any foreign travel history nor had shown any symptoms of the virus.\nAnother video which is going viral on the micro blogging site Twitter is of some people from Nagaland not being allowed to buy groceries from a departmental store in Karnataka amidst the lockdown. The youth can be heard pleading that they are Indians, that they have Aadhar Cards and that they need groceries just like everybody else to survive. Hopefully, some agency of the State apparatus would intervene again to stop this madness.\nThe four instances prove that the problem is not confined to a particular part of the country but is all pervasive. In fact so widespread has been the problem that a New Delhi-based rights group, namely, Rights and Risk Analysis Group (RRAG), has recently released a report titled 'Coronavirus Pandemic: India's Mongoloid Looking People Face Upsurge of Racism'. They have cited at least 22 cases of racial discrimination or hate crimes against such people between February 7 and March 25, 2020. What is more worrying apart from the geographical spread is the fact that such acts have taken place in upscale restaurants and campuses of prestigious educational institutions.\nAs a northeasterner who has spent many years outside the region, this writer has been lucky to have been largely insulated from such racial slurs. However, whenever there is a discussion on racism against the northeasterners, the most common rebuttal is that many 'outsiders' are not treated fairly in the Northeast. Though this is a more complicated story driven by factors such as partition history, landlocked geography, issues of demography and the larger threats emanating from illegal migration, it would not be unwise to revisit this criticism and take stock of what better could be done.\nAmidst all the gloom, one positive development is that all these issues are getting a lot of coverage in the national media/web media. On many occasions in the past, many northeasterners feel that the mainstream media has been found wanting on important issues of the Northeast but not so this time. The Government has also taken a serious note of the issue and on March 21 last, the Ministry of Home Affairs has issued an advisory to all the States/UTs to take action against such racial attacks.\nIt is often said that crisis is the true test of character. The pandemic has caused large-scale panic and fear and the governments across the world are doing all they can to keep the citizens safe. However, as citizens we would also have to give a test of our character and resolve, and the best way to start is by calling out unjust and racist behaviour."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196708, Requested 7673. Please try again in 1.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196708, Requested 7673. Please try again in 1.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "My parents live on the outskirts of Atlanta, and I am worried about them.\nSince the start of the pandemic, there has been a significant uptick in hate crimes against Asians in America. In\nthe past few weeks, we've seen horrifying reports of Asian Americans being harassed and punched, an elderly\nman thrown to the ground and killed in San Francisco, another slashed in the face and left to bleed inside a New\nYork City subway car, as well as countless videos of enraged people shouting at Asian Americans to go back to\ntheir countries and even spitting in their faces.\nThe recent turn of events is shocking but not surprising. For many Asian Americans, anti-Asian sentiments are\nborne in silence and isolation. I grew up mostly being embarrassed about my Korean heritage. In the U.S., we\nKoreans have to prove that we are American, even if we were born and raised here, by pushing ourselves to\nexcel at school and work. As children, some of us refused to learn to speak Korean. By doing so, we shed much\nof our ethnic identities.\nI was always acutely aware of how I should try to fit in. When I was 4 years old, we moved to Vancouver,\nCanada, where we lived in a predominantly white neighborhood. When I started elementary school in the late\n1970s, many people had never heard of Korea. Another Asian girl in my class by the name of Ming Ming would\nsit at the corner of one of the long lunch tables with her thermos of noodles. The other children would wince at\nthe smell. I did not fare better. Next to her, I would open up my doshirak, a Korean lunch box packed with fried\nrice, meat and vegetables and a pair of chopsticks that fit neatly inside. Mimicking the other children, I would scrunch up my nose at my own lunchbox and pretend that its contents were equally offensive to me.\nWhen I got home, I cornered my mother. Why couldn't she just pack me a normal lunch like peanut-butter-andjelly sandwiches?\n\"How is that even healthy?\" my mother countered. \"There are no vegetables in there!\" But my mother obliged,\nand to my relief, I was sent to school with a Western-styled sandwich.\nOne of my mother's favorite words in Korean is cham-ah, meaning to endure. It is what she and her family had\nto do when they escaped North Korea during the winter of 1950. Traveling on foot from Pyongyang to Seoul,\nthey crossed icy rivers and ran for cover from bombings and artillery fire before riding on the roof of a U.S.\nmilitary cargo train from Seoul to Daegu.\nMy parents arrived in Los Angeles in the 1970s and worked as janitors at the Shubert Theater, saving money to\nbuy their first grocery store in a Spanish-speaking neighborhood. Unable to secure a permanent visa for our\nfamily, we moved to Canada, where they cleaned fish at a local market and bought a hardware store. When my\nmom got her medical technologist certification, she found a job in New York and we moved back to the States.\nThrough all of their struggles to make ends meet, they instilled in us the concept of enduring. If someone\nmistreated us, we were told not to make a fuss. Cham-ah. When my schoolteacher told my parents that my\nKorean name was too difficult to pronounce, they consulted our church minister, who provided us with\nAmericanized names. Giving up our Korean names was a small price to pay to fit in. Cham-ah.\nEven today, I will rarely speak out when someone throws out a racial slur. I think to myself, is it really worth it?\nIt could be so much worse. Cham-ah. At the counter of a sandwich shop, the woman who rings me up says very\nloudly with exaggerated enunciation how much my order costs, holding up seven fingers, in case I might be\nhearing impaired in addition to not understanding English. Cham-ah.\nAll my life I felt like being Asian in America was a shortcoming I had to make up for by working hard and not\nsticking out too much. But recently, this low-grade anxiety has morphed into actual fear. Fear for my aging\nparents, relatives and their friends.\nI am hoping the coming herd immunity against the coronavirus might bring with it a new era of racial tolerance.\nThat vanquishing COVID-19 might also mean diminishing these racial attacks, particularly on our elders whom\nwe have been raised to respect and honor. But we can't wait for that to happen. It is time to speak up about\nanti-Asian hate crimes."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194982, Requested 7942. Please try again in 877ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194982, Requested 7942. Please try again in 877ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "In California’s Alameda County, a Chinese American man was screamed at while mowing his lawn. The local prosecutor said the man was told to get out of America.In neighboring Santa Clara County, a Vietnamese couple was threatened while in a grocery store. Officials said the man turned his hand into the shape of a gun.In New York City, people of Asian descent were assaulted, kicked, pushed and accosted on subway trains.The theme: This virus is your fault.People of Asian descent have reported being shunned, verbally abused, name-called, coughed and spat on, even physically assaulted as the coronavirus pandemic continues to \nupend American life. As the political rhetoric blaming China for the pandemic escalates, law enforcement officials and human rights advocates have seen an increasing number \nof hate crimes and incidents of harassment and discrimination against Asian Americans.'They ... think I'm some kind of virus': What it's like to be Asian during the coronavirus pandemic.New York City, for example, has seen 16 coronavirus-related hate crime incidents; many of which involved Asian victims. The city’s human rights commission has received \nhundreds of harassment and discrimination complaints since February, the majority of which involved anti-Asian sentiments.Police and prosecutors are on high alert, releasing public service announcements, holding town hall meetings and opening hotlines to ask people to report hate crimes and hate \nspeech that, while not criminal, could escalate to violence.“We have a large Asian population, and we have a lot of elderly Asian population who are getting scared to death about being preyed upon by somebody who’s willing to resort \nto that behavior,” said Nancy O’Malley, the district attorney of Alameda County, where Asians are 32% of the population. “You have one ethnic group that’s targeted … and \nignorant people who think they can just scream at somebody because of their ethnicity. ... We cannot tolerate that.”O’Malley said there have been no hate crimes in her county, but there have been several incidents of verbal assaults and harassment that don’t rise to the level of a crime. Still, \nO’Malley’s office has encouraged people to report such incidents.COVID-19 and racism: Asian American lawmakers sound the alarm on coronavirus-related discrimination.“Speech is not a crime,” she said, “(but) we want to make sure we’re intervening before it becomes a crime, to educate somebody about cultures and diversity.”In Seattle, schoolteacher Kert Lin was driving into a Home Depot parking lot when another driver screamed at him: \"Open your eyes! Go back to China!\"Lin said the incident, which happened not far from Seattle's international district, rattled him. He told the Seattle Police Department what happened, but he said because no \ncrime was committed, there's nothing police can do.\"This is also our city,\" he said. \"We just want to be safe.\"In Stevens Point, Wisconsin, about 2½ hours from Milwaukee, police arrested a man who they said harassed Asian customers for wearing a mask inside a grocery store. In \nSanta Clara County, the man who threatened the Vietnamese couple by turning his hand into the shape of a gun has been charged with a misdemeanor hate crime.“I wish we didn’t have the one case, but the fact that we only had one case is a testament to the people that live here,” Santa Clara District Attorney Jeff Rosen said, although \nhe added that hate crimes are historically underreported. Asians are about 38% of the county’s population.Rosen’s office recently published a public service announcement calling on people to not assign places and nationalities to the pandemic. The World Health Organization has \nurged scientists, government officials and the media to avoid using geographic locations as names for public health crises.“This isn’t the fault of Chinese Americans that are here. … Asian Americans are very well integrated into our society,” Rosen said. “If you’re turning on the TV news to hear about \nthe latest research from Stanford about COVID-19, you might be looking at an Asian American epidemiologist.”Isolated and scared: The plight of juveniles locked up during the coronavirus pandemicPolicing during a pandemic: Police agencies are using drones to enforce stay-at-home orders, raising concerns among civil rights groups The FBI usually collects hate crime data, but the agency does not have nationwide statistics on violence tied to COVID-19. In a statement, the bureau said:\"The FBI will use all authority granted to us by federal law to investigate and hold those who commit violent acts accountable for their actions. The FBI remains committed to \nour mission to protect the American people and uphold the Constitution\".In New York City, Chief of Detectives Rodney Harrison said last month that the police department has arrested 11 people for hate crimes against Asians.The New York City Commission on Human Rights said it has received more than 300 harassment and discrimination complaints related to COVID-19 this year; 117 of which – \nnearly 40% – involved anti-Asian sentiments.'It would cripple us completely': Coronavirus takes toll on rural police agencies.“The numbers alone are quite astonishing,” said Carmelyn Malalis, head of the commission. Around the same time period last year, the commission received only five reports of \nanti-Asian harassment and discrimination, Malalis said.Malalis is no stranger to racist taunts. The daughter of Filipino immigrants remembers being told she doesn’t belong in America. As a child, she said other children mocked her \nAsian ancestry by slanting their eyes at her. Malalis said she has not personally experienced similar verbal taunts because of COVID-19, but she has heard from friends who \nhave.“What people have to remember is that … just because I’m a Filipino, just because someone’s Chinese, Vietnamese, what have you, does not mean they’re not going through \nthe same kinds of challenges,” Malalis said. “Having to telework"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195381, Requested 7698. Please try again in 923ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195381, Requested 7698. Please try again in 923ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "public health tweets got him branded as an alarmist. His inbox filled with threats or anti-Asian slurs,\nspurring Lee to take a hiatus from Twitter.\nAnd then, in the second week of March – the same week that the World Health Organization declared the\nvirus to be a pandemic – reality seemed to finally, if only partially, set in for the United States.\nIn a span of two days, the National Basketball Association suspended its season. Tom Hanks announced\nthat he had COVID-19. And Trump declared a national emergency while banning many Europeans from\nentering the United States, adding to a partial ban on some foreign nationals who had visited China.\nLee's employer, Mount Sinai, closed all biology labs, except for those doing research related to COVID-19.\nLee pivoted to making an ingredient to test people's blood for the virus.\nThe emergency declaration was, in the estimation of international scientists, long overdue, coming six\nweeks after the WHO had already declared the virus to be a global public health emergency.\nAmericans around the country would soon enter lockdowns. It was a fearful and painful moment, but also\none where the damage from previous inaction was not yet set, said Ranney, the doctor and professor. She\nsaid that the federal government could have attempted to catch up on infrastructures for testing, contact\ntracing, and distributing PPE.\n\"But we didn't,\" she said.\nEaster: 27,342 deaths\nOn March 17, two men hustled into a nearly-empty rest stop in Maryland. Four hours into their 1,200-mile\ndrive from New York to Florida, they realized they weren't going to make it on sandwiches and PowerBars alone, and stopped to fortify with Wendy's cheeseburgers.\nOne of the men was middle-aged and svelte. The other was more than two decades older, hobbled but\nstill sprightly, with an oxygen tank piped to his nose and slung over his shoulder like a school bag. They\nwore face masks weeks before the CDC recommended them, and latex gloves.\nAs they exited the rest stop for the highway, Tom Kirdahy, then 56, and Terrence McNally, 81, felt safer\nwith each mile they put between themselves and New York. It was as if they could feel the virus\nreceding behind them.\nThey were near the front of a great informal migration southward from the New York City area.\nWhile other countries instituted national lockdown orders to freeze citizens and the virus in place, the\nUnited States had only localized, and relatively lax, versions of such edicts. Americans responded by\nmoving more than ever.\nMcNally fled New York with husband Kirdahy because he had only three-quarters of a lung left and was\nterrified the virus would make quick work of him.\nThe couple's history together had another pandemic as a backdrop. In 1995, Kirdahy, an attorney\nrepresenting those infected with HIV and AIDS, was transfixed in a Broadway theater as he watched a\ngay male character kiss a purple scar on the chest of another.\nIn Kirdahy's view, society had decided that eradicating the AIDS epidemic was not a priority because\nthose it was killing, mostly gay men, were considered no great loss.\nHe had held the hands of many gay men as they died without the presence of family members who\nwere afraid of infection or disdainful of their sexuality.\nSo attending \"Love! Valour! Compassion!\" written by McNally, whom Kirdahy then only knew as a famed\nplaywright, was \"like being in church,\" he said.\nIt was the story of eight gay men on vacation together, including one, played by Nathan Lane, who\nkissed the AIDS lesion of another. Kirdahy and his companions at the theater, all of whom worked on the\nAIDS front lines or were infected with the virus, wept and hugged each other.\nKirdahy, a lifelong theater buff, met McNally six years later after inviting him to join a panel of famed\ngay playwrights he organized in the Hamptons, New York. They bonded over a shared familiarity with\nAIDS. Two of McNally's previous partners had died of the virus, and the playwright had seen firsthand\nthe isolation suffered by its victims in their last moments.\nThey married in 2003, with Kirdahy warning that he wasn't going to shelve his ambition and become\n\"Mrs. McNally.\" He ultimately left his law practice to become one of the most prolific theater producers\non Broadway and London's West End, racking up armfuls of theater awards to rival those of his\nhusband.\nBut a diagnosis only months after they had started dating injected their relationship with a perpetually\nfleeting quality. McNally's former three-pack-a-day smoking habit had stricken him with lung cancer.\nSurgery left him with a quarter of his right lung and only half of his left. For decades he was in and out\nof the hospital, including with potentially life-ending bouts of pneumonia.\nMcNally made light of it. When he and Kirdahy bought a vacation apartment in Sarasota, Florida, in\n2017, McNally predicted he would die there. \"I know how this story ends,\" he joked.\nAnd in June 2019, when he was awarded a Tony for lifetime achievement, he accepted it with his oxygen\ntank attached and told the crowd: \"Not a moment too soon.\"\nDuring the first weeks of March, when it became clear that New York City was usurping the Seattle area as a national epicenter of coronavirus, they made hasty plans to drive to the apartment in Florida\nuntil its spread blew over.\nAt a roadside motel room in North Carolina"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196532, Requested 6929. Please try again in 1.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196532, Requested 6929. Please try again in 1.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A crowd of about 200 New Yorkers gathered Saturday in Foley Square to denounce a wave of anti-Asian violence\nthat they say is fueled by racism and ignorance about the COVID-19 pandemic.\nThe throng heard community leaders and a parade of top political leaders vow to end the shocking spat of\nattacks and seemingly unending torment of slurs that have dramatically worsened during the pandemic.\n\"We are fighting a global pandemic but we are also fighting racism. ... We are getting spat on, shoved, punched,\nslashed, stabbed,\" said Jo-Ann Yoo of the Asian American Federation. \"Right now Asian-Americans are scared,\nwe are outraged, and we are devastated.\"\nMayoral candidate Andrew Yang, one of the nation's best-known Asian politicians, attended the rally but was not\ngiven a speaking spot and left without speaking to reporters.\nThe crowd gave a rousing reception to Noel Quintana, 61, a Filipino-American who was slashed in the face on\nthe L train last week. \"A lot of Asian-Americans [are] being attacked everywhere so I just want [people] to know\nthat this is happening to us,\" he said. \"Almost every day there are attacks and I don't understand why.\"\nMany New Yorkers were stunned to see a viral video of an Asian-American woman being slammed to the\nsidewalk last week in an unprovoked attack in Flushing, Queens.\nRep. Grace Meng (D-Queens) said the attacks are the tip of an iceberg of hate crimes, many of which go\nunreported.\nShe vowed that Asian-Americans would no longer settle for being \"invisible\" victims.\n\"Too many people who are afraid to leave their homes,\" said Meng, whose district includes Flushing, \"not just\nbecause of the virus, but because of bigotry.\"\nAsian-American leaders say they are regularly targeted for verbal and physical abuse by people who blame them\nfor the pandemic.\nMany blame the racist violence in part on former President Donald Trump, who often called COVID-19 the \"China\nvirus\" or other racist terms because it was first identified in China.\nMayor de Blasio, Sen. Chuck Schumer (D-N.Y.) and state Attorney General Letitia James all vowed to crack down\non anti-Asian violence in New York, saying the city needs to protect people targeted by hatred."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195070, Requested 7599. Please try again in 800ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195070, Requested 7599. Please try again in 800ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Caleb Lee wishes he was surprised to hear that someone spit on an Asian American friend biking in Golden Gate\nPark recently. A white woman told his friend, then 16 years old, to go back to China and called her a slur,\naccording to Lee.\nAnother friend was also told to go back to China by a classmate, before schools shut down, and called the same\nslur.\n\"I was more angry and pissed off at the fact that the world just doesn't care and disregards this hate toward\nus,\" said Lee, a 16-year-old junior at San Francisco's Urban School. \"It's sort of just become a harsh reality that\nso many [Asian American Pacific Islander] individuals have been dealing with racism and hate in the form of\nphysical and verbal harassment.\"\nCaleb's words echoed some of the despair felt by Asian American youth surveyed this year by Stop AAPI Hate, a\nreporting center launched by Asian Pacific Policy and Planning Council (A3PCON), Chinese for Affirmative Action,\nand San Francisco State University's Asian American Studies Department. The group has tracked more than\n2,600 incidents of racial animus since mid-March, when much of the country went into a shel-ter-in-place to\ncurb coronavirus.\nCalifornia accounts for 1,135 of those incidents, while 258 occurred in San Francisco as of Thursday. That's\nshockingly high compared to 108 in Los Angeles, 54 in San Jose, 50 in Oakland, 28 in San Diego, and 20 in Sacramento.\nLee and a few other Urban School students worked on a nationwide effort by Stop AAPI Hate to track impact on\nyouth, culminating in the report \"They Blamed Me Because I Am Asian\" released on Thursday.\nSeventy-seven percent of nearly 1,000 youth surveyed expressed anger over the spate of racism, 60 percent\nsaid they were disappointed, and 46 percent expressed sadness or depression. Twenty-three percent said they\nwere scared.\nThey received 341 reports from youth nationally over an 18-week period, with 81 percent reporting bullying or\nverbal harassment. Twenty-four percent said they faced shunning and social isolation while 8 percent reported\nphysical assaults.\nNearly 17 percent said they were harassed at school and online, respectively, and 14 percent were harassed in\npublic parks. Adults were present in 48 percent of the cases but bystanders intervened only 10 percent of the\ntime. Forty-one percent of incidents were perpetrated by youth, the rest by adults.\nNearly 60 percent of verbal harassment involved blaming Chinese people for the COVID-19 pandemic either as\nbeing infected or as the source, while nearly 26 percent related to dietary habits, like shaming Asian Americans\nfor supposedly eating bats or dogs. One 17-year-old reported that a white man online told him his insides were\nfull of bats and to die by suicide for being a \"dirty f________dog-eater.\"\n\"Historically, we've seen [upticks in racism] in pandemics, war and economic downturn,\" said Russell Jeung, a\nSan Francisco State professor who helped start Stop AAPI hate. \"We have all three. You think San Francisco is\nthis progressive, liberal town but again, I think, COVID fears and political rhetoric has really screwed up a lot of\nanger.\"\nSan Francisco has double the number of physical assaults compared to nationwide, according to Jeung. Density\nmay explain the concentration, even in places like The City and New York, where overtly welcoming attitudes of\nimmigrants are expressed.\nBut ultimately, Jeung said political rhetoric is to blame. President Donald Trump has repeatedly targeted China\nfor the virus and used epithets like \"Chinese virus\" and \"kung flu\" picked up by right-wing media. He has also\nheightened tensions between the two countries, leading to a Cold War-like state of relations that ripples to\nindividual behavior.\n\"I just find it very debasing and dehumanizing that others can treat people that way,\" said Jeung, whose wife\nwas deliberately coughed on by a man in Oakland. \"People are wanting to share their stories and wanting it to\nstop from happening. By having that collective voice, we're pushing for more bystander intervention.\"\nJeung said that on an individual level, witnesses should attend to the targeted person to make sure they are\nsafe and to show social support, though he doesn't recommend engagement. Should that not immediately\ntranspire, people being targeted should call out and ask for assistance or approach someone nearby and express\ntheir concern.\nOn a larger level, anti-racism training for teachers and administrators while implementing ethnic studies is part\nof the answer. Grassroots community workshops and signage in stores help foster an anti-racist culture, too.\nLee said the long-term goal is to change the narrative of Asian Americans as silent, complacent model minorities\nwho blend into white America. Because the social fallout from the pandemic has proven the \"very false and very\nharmful stereotype\" internalized in AAPI minds, too.\nIn the meantime, they hope workshops and preparing teachers will prevent aggression when the campus is\nopen again.\n\"There's a slight amount of fear and worry in the back of our mind that when we return we might receive\nhate or racist comments,\" Lee said. \"But more than that, we have hope for this. By spreading awareness, it\nwill eventually reduce the hate because people might understand what they hadn't before. COVID-19 is not\ngoing to be the last time.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194343, Requested 6578. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194343, Requested 6578. Please try again in 276ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Festivities for the Lunar New Year holiday, normally East Asia's busiest tourism season, are muted after China,\nVietnam, Taiwan and other governments tightened travel curbs and urged the public to avoid big gatherings\nfollowing renewed coronavirus outbreaks.\nIn China, Buddhist and Daoist temples that are usually packed with holiday worshippers are closed. Streets in\nmajor cities were largely empty.\nJi Jianping, 62, was among those gathering outside the locked gates of the Tibetan-style Lama Temple on\nBeijing's north side to burn incense and pray. She said she and her family skipped visiting their home town in\nthe northern province of Shanxi due to the pandemic.\nThe government's appeal to China's public to avoid travel is denting spending on tourism and gifts. But\neconomists say the overall impact might be limited if factories, shops and factories keep operating instead of\ntaking their usual two-week break.\nThe commerce ministry said 48 million more people in Chinese cities planned to celebrate where they lived\ninstead of travelling.\nThe China State Railway Group said railway journeys were down by almost 70 per cent from last year.\nDepartures from Beijing's two major airports were down 75 per cent.\nIn Taiwan, merchants said sales were up 10 to 20 per cent because people were celebrating at home with family\ndinners instead of travelling abroad.\nMeanwhile, police are stepping up patrols and volunteers are increasing their street presence after several\nviolent attacks on older Asians that have stoked fear in San Francisco Bay Area Chinatowns and subdued the\nLunar New Year mood.\nShops and restaurants are typically bustling in Chinatowns across the United States at this time of year, but the\npandemic and safety concerns have dampened the festive atmosphere.\nCity officials have visited Chinatowns in San Francisco and Oakland this week to address residents' safety\nconcerns and condemn the violence. They have vowed to combat a problem that has been simmering since the\nstart of the pandemic but sparked new outrage after two unprovoked attacks were caught on video within a few\ndays.\nIn one, a man shoved Vicha Ratanapakdee to the ground as he was taking his morning walk in San Francisco's\nAnza Vista neighbourhood on January 28. The 84-year-old Thai man's head struck the pavement, and he later\ndied in hospital. Prosecutors have charged a 19-year-old with murder and elder abuse.\nOn January 31, a security camera caught a man wearing a hooded sweatshirt barrelling into a 91-year-old Asian\nman in Oakland's Chinatown, causing him to fall face down on to the footpath. Police have arrested a suspect,\nand said he had assaulted a couple on the same block later that day, and another on February 1.\nIn just the last two weeks, authorities had recorded 18 crimes against Asian-Americans around Oakland's\nChinatown, said Nancy O'Malley, district attorney for Alameda County.\nThe recent attacks represent the latest spike in verbal and physical attacks against Asian- Americans since the\ncoronavirus, which emerged in China, reached the US. Stop AAPI Hate, launched by two advocacy groups to\nencourage Asian-Americans to report such incidents, has documented more than 3000 attacks to date.\nThe California attacks have prompted volunteers to offer to walk older residents to their cars or homes after\nshopping. Jacob Azevedo said more than 200 people signed up after he posted the idea on social media. They\nalso donated thousands of dollars to help him buy personal alarms for older Asians. - AP\nCAPTION:\nVisitors look at dragon toys in Bangkok's Chinatown as the Lunar New Year holiday begins. The Chinese diaspora\nof Southeast Asia is marking the new year - traditionally a time for people to celebrate with their extended\nfamilies - in a much quieter way\nthan usual. GETTY IMAGES"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193426, Requested 7426. Please try again in 255ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193426, Requested 7426. Please try again in 255ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "My parents live on the outskirts of Atlanta, and I am worried about them.\nSince the start of the pandemic, there has been a significant uptick in hate crimes against Asians in America. In the past few weeks, we've seen horrifying reports of Asian Americans being harassed and punched, an elderly man thrown to the ground and killed in San Francisco, another slashed in the face and left to bleed inside a New York City subway car, as well as countless videos of enraged people shouting at Asian Americans to go back to their countries and even spitting in their faces.\nThe recent turn of events is shocking, but not surprising. For many Asian Americans, anti-Asian sentiments are borne in silence and isolation. I grew up mostly being embarrassed about my Korean heritage. In the U.S., we Koreans have to prove that we are American, even if we were born and raised here, by pushing ourselves to excel at school and work. As children, some of us refused to learn to speak Korean. By doing so, we shed much of our ethnic identities.\nI was always acutely aware of how I should try to fit in. When I was 4 years old, we moved to Vancouver, Canada, where we lived in a predominantly white neighborhood. When I started elementary school in the late 1970s, many people had never heard of Korea. Another Asian girl in my class by the name of Ming Ming would sit at the corner of one of the long lunch tables with her thermos of noodles. The other children would wince at the smell. I did not fare better. Next to her, I would open up my doshirak, a\nKorean lunch box packed with fried rice, meat and vegetables and a pair of chopsticks that fit neatly inside. Mimicking the other children, I would scrunch up my nose at my own lunchbox and pretend that its contents were equally offensive to me.\nWhen I got home, I cornered my mother. Why couldn't she just pack me a normal lunch like peanut-butter-and-jelly sandwiches?\n\"How is that even healthy?\" my mother countered. \"There are no vegetables in there!\" But my mother obliged, and to my relief, I was sent to school with a Western-styled sandwich.\nOne of my mother's favorite words in Korean is cham-ah, meaning to endure. It is what she and her family had to do when they escaped North Korea during the winter of 1950. Traveling on foot from Pyongyang to Seoul, they crossed icy rivers and ran for cover from bombings and artillery fire before riding on the roof of a U.S. military cargo train from Seoul to Daegu.\nMy parents arrived in Los Angeles in the 1970s and worked as janitors at the Shubert Theater, saving money to buy their first grocery store in a Spanish-speaking neighborhood. Unable to secure a permanent visa for our family, we moved to Canada, where they cleaned fish at a local market and bought a hardware store. When my mom got her medical technologist certification, she found a job in New York and we moved back to the States. Through all of their struggles to make ends meet, they instilled in us\nthe concept of enduring. If someone mistreated us, we were told not to make a fuss. Cham-ah. When my schoolteacher told my parents that my Korean name was too difficult to pronounce, they consulted our church minister, who provided us with Americanized names. Giving up our Korean names was a small price to pay to fit in. Cham-ah.\nEven today, I will rarely speak out when someone throws out a racial slur. I think to myself, is it really worth it? It could be so much worse. Cham-ah. At the counter of a sandwich shop, the woman who rings me up says very loudly with exaggerated enunciation how much my order costs, holding up seven fingers, in case I might be hearing impaired in addition to not understanding English. Cham-ah.\nAll my life I felt like being Asian in America was a shortcoming I had to make up for by working hard and not sticking out too much. This week, this low-grade anxiety has morphed into actual fear. Fear for my aging parents, relatives and their friends.\nI am hoping the coming herd immunity against the coronavirus might bring with it a new era of racial tolerance. That vanquishing COVID-19 might also mean diminishing these racial attacks, particularly on our elders whom we have been raised to respect and honor. But we can't wait for that to happen. It is time to speak up about anti-Asian hate crimes.\nCaption: PHOTO: AT A RALLY against anti-Asian violence Feb. 20 in Los Angeles, a protester holds a poster of Vicha Ratanapakdee, a Thai immigrant who died in a street attack in San Francisco."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196483, Requested 8172. Please try again in 1.396s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196483, Requested 8172. Please try again in 1.396s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "At this crucial time of confronting systemic anti-Black and anti-Indigenous racism, the Canadian government\nmust take responsibility for its enduring role in propagating racism. This includes through its misleading self-\nportrayal in Discover Canada, the official study guide for the test taken by Canadian citizenship applicants.\nIssued in 2011, the guide aims to teach prospective citizens about Canada's history, geography, culture, and\npolitical and justice systems. Disgracefully, the document whitewashes colonialism, conceals genocide,\nminimizes systemic racism and its inhumane consequences, and portrays these as remnants of the past even as\nthe guide itself engages in racist discourse.\nWe write as female faculty members at the Dalla Lana School of Public Health, University of Toronto. Our\nperspective on these issues is borne of our shared concerns yet differing relationships to \"Canadian\" nationality.\nOne of us is Algonquin (Timiskaming First Nation), another is a woman of African ancestry, born in Canada, surviving transatlantic enslavement, two are new-ish Canadians (white European heritage, from the United\nStates and South Africa), another is a permanent resident from the U.S. (from Venezuela).\nRecognizing Discover Canada's flaws, the government set out in 2016 to remove certain offensive elements,\nincluding the portrayal of immigrants'\"barbaric cultural practices\" and the glorification of military exploits.\nA draft shared with The Canadian Press in 2017 incorporated coverage of the Truth and Reconciliation\nCommission's 2015 report, as well as discussion of discrimination against people of racialized backgrounds,\npeople with disabilities, LGBTQ2S+ people and other marginalized communities.\nDespite an expected 2017 release - during Canada's sesquicentennial - the new version, inexplicably, never\ncame to light. Outrageously, Discover Canada is still the welcome guide for new Canadians. Just a few of its\nshameful elements include: 1. It presents a sanitized account of Indigenous peoples, before and under\ncolonization. This erases the history and legacy of stolen lands and dispossession, the upheaval of nations,\ncultural genocide, broken treaties, assimilation policies such as residential schools, police and carceral racism,\ncontinued heinous living conditions on reserves and hugely inequitable health outcomes.\nFrom the Royal Proclamation of 1763, which states Indigenous people are sovereign, to the British North\nAmerica Act of 1867, there is no discussion of how the Doctrine of Discovery shaped the nation. Discover\nCanada also omits Indigenous people's resistance movements, past and present, that contest historical and\nongoing forms of colonialism and oppression. 2. The guide celebrates Upper Canada as \"the first province in the\nEmpire to move toward abolition\" and as a \"safe haven\" for enslaved Black people escaping the United States.\nBut there is no mention of Canada's own history of slavery, nor of the pervasive racist violence and human\nrights violations faced by African and Black Canadians. The guide's omission of the Code Noir (1685, revised in\n1789) further erases the reality of policed and enslaved African lives, which included forced religious conversion,\nsanctioned punishment and other brutalities.\nThis legacy, combined with successor policies, has generated overrepresentation of Black and Indigenous\nchildren in foster care and Children's Aid facilities and high rates of educational, food and housing insecurity, all\ngenerating worse health outcomes among Black Canadians.\nLikewise, anti-Asian racism is covered only superficially. The horrendous Komagata Maru event is overlooked.\nThe internment of Japanese-Canadians during the Second World War is discussed fleetingly, with no mention of\nnumbers affected (over 22,000), uncompensated liquidation of property, family separation or forced postwar\nrelocation.\nThe guide flags government apologies, but not the racist ideologies and practices justifying such policies. This is\nthe case with anti-Chinese discrimination, including the \"Head Tax, a race-based entry fee,\" and more recently\nwitnessed in hateful attacks on Asian Canadians during both the SARS and COVID-19 pandemics. 3. Amid\noverwhelming, long-standing evidence of systemic police brutality targeting racialized communities, the guide's\nuse of the phrase, \"Remember, the police are there to help you\" contradicts the everyday reality of policing for\nBlack, brown and Indigenous peoples. This includes: repeated instances of violence, racial profiling and\nharassment against Black and Indigenous communities, the RCMP's complicity in missing and murdered\nIndigenous women and girls, and grossly disproportional incarceration of Black and Indigenous populations. 4.\nIn discussing the 1982 Charter of Rights and Freedoms, the guide is highly selective regarding which rights are\nacknowledged and emphasized. While the guide repeatedly stresses the importance of religious freedom, it fails\nto mention equality and non-discrimination as fundamental rights. This selective presentation of key human\nrights protections gives prospective citizens a distorted picture.\nThe guide also misses the opportunity to endorse key human rights protections against racism, which are\ncentral to mitigating the effects of systemic discrimination on people's well-being, financial security, education\nand work aspirations.\nAdditionally ignored are past and contemporary policies that undermine the rights of racialized migrant\nworkers. Examples of these groups include Black migrants, who historically were only admitted as domestic\nworkers or in other menial roles; Filipino workers recruited to be home care aides and personal support\nworkers; and Latin American and Caribbean migrant farm workers today. Racialized migrants are frequently\ndenied landed immigrant status, family reunification, provincial health coverage, unemployment benefits,\nworkplace protections and other social entitlements and worker protections.\nA replacement citizenship guide is long overdue. A new guide should further highlight working-class struggles\npushing Canada's efforts in"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197780, Requested 7441. Please try again in 1.566s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197780, Requested 7441. Please try again in 1.566s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "While the coronavirus epidemic in China has been tamed, it has not stopped Western media, especially those in the US, from trying to exploit the crisis. They have now found another weapon to demonize China, calling it “racist” and “xenophobic.”\nThis hype has been constant over the past week. The New York Times claimed “as coronavirus fades in China, nationalism and xenophobia flare.”\nIn China, many foreigners enjoy preferential policies, and overall, foreigners have a better life than the average Chinese. If those Western media intend to label China as “racist” or “xenophobic,” they should dig deeper into specific incidents. The recent allegations of some Africans being discriminated in Guangzhou’s virus-control efforts were actually enhanced measures as the city is facing the rising risk of imported cases and the measures were applied to all residents regardless of race and nationality.\nIt is ludicrous to see Western media reports pinning the racism label on China while there is a growing number of news reports about Chinese and Asians facing racist abuse in the US, proving that xenophobia has flared there.\nIn the US, Asian Americans find themselves in the predicament of having to choose between wearing a face mask to protect themselves from the coronavirus, or not wear a mask, to protect themselves from discrimination.\nIn late March, an American spat on an Asian man in a Brooklyn subway station. Reports also show children of Chinese descent in the US are refusing to wear masks at school because they fear being verbal abused by their classmates.\nOn Monday night local time, Trump tweeted that he intended to sign “an Executive Order to temporarily suspend immigration into the United States,” a move some US netizens said was xenophobic.\nDespite it may be a method evaluated by experts to control the epidemic in the US. Some commented under Trump’s announcement by saying “when immigrant NYers make up more than half of the frontline workers in NYC, the epicenter of America’s #COVID19 pandemic, the President’s response is more xenophobia & fear mongering.”\nRacism issue has deep root in the US. The founding of the US south was facilitated by slavery. Even after the Civil War, which ended slavery, the country maintained institutionalized racial discrimination which only began to be eroded in the 1970s. The continued existence of the white supremacist hate group Ku Klux Klan, is proof. In 1882, the US enacted the Chinese Exclusion Act, prohibiting the immigration of Chinese laborers. Discrimination in the US against the Latino population, which rages on today, has its own disgraceful backstory.\nRacism and xenophobia are cancer in US society, which has not yet been cured. Among US conservative extremists, many are white supremacists.\nWhen the coronavirus outbreak began, the ingrained mentality in US society again came to the fore, with many blaming China for the pandemic which led to acts of violence toward Asian Americans. The Independent reported on March 24 that “hundreds of Asian Americans have been violently attacked in the last month because of “China virus” racism voiced by [US President] Donald Trump.”\nFox News has hyped its large audience with constant and groundless accusations against China. It wouldn’t exist if its disparaging remarks and extreme ideology weren’t supported by a large swath of Americans.\nA vicious circle has been created in the US – media hype accelerates the spread of racism and xenophobia, which perpetuates unprincipled views of people and creates a market that is supplied with unrelenting news stories and commentaries that support racist mindset.\nIt is uncertain whether the suspension of immigration to the US is an emergency measure or a xenophobic move that will continue after the pandemic ends.\nRegardless of whether Trump is to be reelected this year, the next US president is highly likely to make adjustments on Trump’s policies, not for noble reasons but because US politicians know immigrants have made the US what it is today. Racist and xenophobia policies only hurt the US’ unity and national interests."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194228, Requested 6891. Please try again in 335ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194228, Requested 6891. Please try again in 335ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "At a news conference Wednesday, Trump defended his use of the phrase when asked by reporters if he was being bigoted in saying \"Chinese virus.\"\n\"It's not racist at all,\" Trump said to reporters. \"No, not at all. It comes from China. That's why. It comes from China. I want to be accurate.\"\nLike many others, Chinese Americans in metro Detroit are anxious about the spread of coronavirus, but they and others of Asian descent are also worried about racist backlash.\n\"People are afraid,\" Shenlin Chen, said president of the Association of Chinese Americans (ACA), a group based in Madison Heights that advocates for Chinese Americans. \"People have fear about being discriminated against or treated unfairly.\"\nSince the spread of the coronavirus, there have been reports of Chinese Americans and other Asian American groups perceived to be Chinese being racially attacked or ostracized. In metro Detroit two weeks ago, an elderly Chinese American had his grocery cart spat on by someone, Chen said.\nAnd now, they're concerned this week about how President Donald Trump is increasingly using the phrase \"Chinese virus\" in his tweets and public remarks. Founded in 1972, the Association of Chinese Americans was expected to release a statement on Thursday criticizing Trump and others who use that phrase.\n\"As it is plainly clear, there is no reason to attach a racial or ethnic or country moniker on a virus pandemic,\" a draft copy of the statement reads. \"The Covid-19 is present in all of the 50 states of the United States, is prevalent throughout Europe, the Americas, and Asia. Covid-19 does not recognize political borders. ... Attaching a racial, ethnic, or country name to a virus, even informally, serves only to flame the fears and biases of people against Chinese and those of Asian background to no benefit in the battle against the virus pandemic.\"\nThe statement added: \"We should all work to avoid the misleading terms that detract from our common interest in working to contain the Covid-19 pandemic.\"\nOn Wednesday, Attorney General Dana Nessel said in a tweet that \"Trump is attempting to incite hatred, violence and discrimination against Chinese Americans. We won't tolerate it in Michigan. Report all suspected hate crimes to your local police dept or to our Hate Crimes Unit. 313-456-0200.\"\nState Sen. Stephanie Chang, D-Detroit, tweeted on Wednesday: \"Just a little reminder to not discriminate, attack, or promote hate during #COVID2019. The virus is a virus. It does not have a race or ethnicity.\"\nOn Tuesday, U.S. Sen. Elizabeth Warren, D-Mass., wrote on Twitter: \"Bigotry against people of Asian descent is unacceptable, un-American, & harmful to our COVID-19 response efforts.\" Hillary Clinton called Trump's remarks \"racist rhetoric\" and Joe Biden slammed them as \"xenophobic fear-mongering.\"\nAt a news conference Wednesday, Trump defended his use of the phrase when asked by reporters if he was being bigoted in saying \"Chinese virus.\" \"It's not racist at all,\" Trump said to reporters. \"No, not at all. It comes from China. That's why. It comes from China. I want to be accurate.\"\nAlso on Wednesday, U.S. Sen. John Cornyn, R-Texas, made comments seen as racist when he said \"China is to blame because the culture where people eat bats and snakes and dogs and things like that.\"\nThere are about 61,000 residents in Michigan of Chinese ancestry, according to 2018 Census figures.\n\"There is a concern about how people react when we are in various establishments and crowds,\" said Northville attorney Roland Hwang, secretary of the Association of Chinese Americans and a teacher of Asian American studies at the University of Michigan.\nThe 1982 murder of Vincent Chin, of Chinese descent, in Highland Park by autoworkers angry at Japanese people the integration of the workforce, is still remembered by many Asian Americans in metro Detroit.\nThey're worried about any resurgence of anti-Asian racism.\nHwang helped found American Citizens for Justice, an Asian American advocacy group in metro Detroit created after Chin's murder to bring justice for Chin and raise awareness of anti-Asian racism.\n\"I'm really alarmed by Trump calling the virus a Chinese virus,\" Hwang said. \"There is no geographic, political subdivision, racial or ethnic limitation with respect to the virus.\"\nIn addition to the issue of racism, the Chinese American community is grappling with the effects of the coronavirus spread. Linguistic barriers is an issue for some, which the Association of Chinese Americans is trying to help with, Chen said.\nA state health official held a meeting two weeks ago with Chinese Americans to work on ways to work together to help bring awareness, she said. Contact Niraj Warikoo: nwarikoo@freepress.com or 313-223-4792. Twitter @nwarikoo.\nAt a news conference Wednesday, Trump defended his use of the phrase when asked by reporters if he was being bigoted in saying \"Chinese virus.\" \"It's not racist at all,\" Trump said to reporters. \"No, not at all. It comes from China. That's why. It comes from China. I want to be accurate.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193416, Requested 8172. Please try again in 476ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193416, Requested 8172. Please try again in 476ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "As the coronavirus continues to spread in the United States, the Trump administration is fielding criticism from some public health officials, politicians and Asian Americans for labeling the illness a \"Chinese virus\", a term that experts argued is inaccurate and has led to increased incidents of racist attacks against Asians of different ethnicities.\n\"The president of the United States, and other White House officials, using the term 'Chinese virus', or 'Kung-Flu', or 'Wuhan virus', we believe is inciting and provoking this anti- Asian sentiment and is really responsible in the rise of these more violent acts,\" said Cynthia Choi, co-executive director of Chinese for Affirmative Action, a community-based civil rights organization in San Francisco.\n\"He's refusing to call it by its correct medical name, which is COVID-19, or the coronavirus. He's refusing to do that, even after it was called to his attention that this is harming and endangering the lives of Asian Americans,\" she added.\nAs of Sunday, 201 deaths and over 15,200 cases had been reported in the US, according to the World Health Organization.\nIn response to the increasingly frequent harassment resulting from the coronavirus, Choi's organization, along with the Asian Pacific Policy & Planning Council and the San Francisco State University's Asian American Studies Department, have launched a reporting center to track cases of discrimination against Asian Americans and Pacific Islanders throughout the country.\nThe center opened on Thursday, and by Friday, it had already received 60 reports of cases from community members on the site, Choi said.\nOne of the cases involves a 12-year-old Asian American boy in the San Fernando Valley in Los Angeles, who was so badly beaten by middle school classmates that he had to visit the hospital emergency room.\nAnother case occurred just a few days ago. It revolved around a Chinese woman who found herself accosted on the street by a stranger who cursed at her and blamed her for bringing the coronavirus to the US.\nOn March 14, a man from Myanmar and his young son were two of four people stabbed by a man at a grocery store in Midland, Texas. Although this case still warrants further investigation, Choi said she found out through her network that racism and coronavirus motivated the attack.\n\"We are starting to see a pattern that individuals who might not appear to be likely to defend themselves, or unable to defend themselves, are being targeted. We are seeing all kinds of racially derogatory comments being made toward people,\" she added.\nThe violence has spiraled so out of hand that David Liu, the owner of Arcadia Firearm & Safety in San Gabriel Valley, an area with a high concentration of Chinese immigrants, said gun sales at his store were 10 times the usual amount in the past three weeks, with many customers buying firearms to protect themselves against possible racial attacks and fear of social disorder caused by the coronavirus.\nTrump defended his use of the term on Wednesday, saying \"it's not racist at all\", he told reporters. \"It comes from China, that is why.\"\nHowever, cases of a strange, severe pneumonia were witnessed last November in Italy before the world even became aware of the COVID-19 outbreak in China, said Giuseppe Remuzzi, an Italian physician.\nIn an interview with National Public Radio, Remuzzi said some family doctors told him that they had seen the pneumonia, particularly in old people, in December and even in November.\n\"It means that the virus was circulating at least in Lombardy before we were aware of this outbreak occurring in China.\"\nMeanwhile, such terms have elicited a chorus of criticism from public figures, including former secretary of state Hillary Clinton, who argued that Trump is using racist rhetoric to distract from his administration's delay in preparing the country for the pandemic.\n\"The president is turning to racist rhetoric to distract from his failures to take the coronavirus seriously early on, make tests widely available, and adequately prepare the country for a period of crisis. Don't fall for it. Don't let your friends and families fall for it,\" she wrote on her Twitter account on Wednesday.\nAnalysts outside the US also believe that Trump's intentional use of the racist term is to distract the US public from the wide criticism of his missteps in his early denial of the seriousness of the pandemic. Now with the stock market gains of the past three years totally wiped out, he is eager to play the blame game, especially in a presidential election\nyear.\nShada Islam, director of Europe and Geopolitics of Friends of Europe, a think tank in Brussels, Belgium, said at times of crisis like the one the world is living through, it is time for leaders and people to come together and work together to tackle the challenge, not exchange insults, accusations and engage in conspiracy theories.\n\"It is a tragedy that US-China tensions, dating back to US President Donald Trump's arrival at the White House, are further complicating global cooperation to surmount the pandemic,\" she said.\nWorld Health Organization Director-General Tedros Adhanom Ghebreyesus said that the use of the name COVID-19 is to prevent the use of other names that can be inaccurate or stigmatizing. He and other WHO leaders have repeatedly warned against stigmatization in the past two months.\nScott Kennedy, a China expert at the Center for Strategic and International Studies, told The New York Times that the use of such terms \"can't but be interpreted as xenophobic and tinged with racist overtones\", given the Trump administration's long record of statements and actions on immigration, immigrants and issues of race.\nComedian Joe Wong and basketball player Jeremy Lin took to social media to speak up against the recent waves of harassment targeting Asian people.\n\"My AsAm friends told me last year, 'I know it's scary but as long as you stay in the two coasts (in the US), you are fine.' Now Asians are being assaulted in NYC, LA, London, and Australia. Asians have been enduring racism"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198673, Requested 7871. Please try again in 1.963s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198673, Requested 7871. Please try again in 1.963s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "- trying to distance themselves from FOBs\n(fresh off the boat)\".\nMore recent Asian migrants wrote about hating themselves and getting depressed about the racial\nstereotypes they had started to believe, he says.\nLike Asian communities, Pacific communities are extremely diverse - and both umbrella ethnic communities\ncan face being treated as \"forever foreigner\", Mayeda says.\nIn the early stages of the Covid-19 pandemic, East Asians in particular were cast as \"virus-spreading foreign\nthreats\", he says. That stereotype then shifted to Aotearoa's Pacific communities in mid-2020.\n\"In general, however, because Pacific peoples have closer cultural connections to Maori, and geographic\nconnections to nearby Pacific nations, they tend not to be cast as foreign threats in the same way as Asians.\n\"Research I've been involved in shows that Pacific peoples are subjected to racism that many Maori\nunderstand - being unfairly suspected as criminogenic, lazy in school and work.\n\"In contrast, Asians get cast as the distant foreign threats who will take over the economy, housing market\nand schools if not kept in check.\"\nHe believes it's important for New Zealand to recognise Maori as tangata whenua, and also that anyone who\nis not Maori, including Pakeha, comes from a migrant background.\n'All they see us as are sports stars and entertainers'\nThe University of Auckland's Dr Sereana Naepi says it was \"very telling\" how quickly and easily the rumours\ntargeting Auckland's Pasifika communities over the August Covid-19 cluster were believed.\nOne rumour suggested that a woman from a Pasifika family contracted the virus by sneaking into a managed\nisolation facility to see a man.\nThis fed into the \"dusky maiden\" trope stemming from colonial times, which sexualised and objectified Pacific\nwomen.\n\"People latched on to this fake story of criminals and sex,\" Naepi says.\nNew Zealand's colonising of the Pacific cemented the colonisers' views of Pacific communities as children to\nbe taken care of, instead of strong, independent nations capable of caring for themselves.\nColonisers took on an almost paternal role so that Pacific communities were incapable of being seen as a\nforeign threat, she adds.\n\"People are happy to have Pacific communities when they benefit from their labour and cultural capital, but\nthey're less likely to invest in these communities.\n\"All they see us as are sports stars and entertainers - in their minds, we have a specific space to fill in New\nZealand society.\"\nAnd when this wasn't filled? Pacific communities were labelled as people who did not contribute, as\nunderachievers and criminals, she says.\nGatekeeping who gets to be kept in and out\nGrace Gassin, a researcher and curator of Asian New Zealand histories at Te Papa Tongarewa, says \"for a\nlot of New Zealand's history, there's been a desire to keep New Zealand white for lack of a better way to\nput it\".\n\"I guess we're all dealing with the legacies of anti-Chinese discrimination and, broadly, discrimination\nagainst non- whites which happened legislatively, administratively and socially.\"\nAsian migrants had been around during seminal events and there were also instances of solidarity between\nMaori and Asian communities - moments often not heard about when New Zealand history is told.\nShe mentions Minnie Rose Alloo, a woman of Chinese and Scottish ancestry, as an example of this. Her\nsignature sits neatly on a suffrage petition in 1893. She was 19 at the time of signing, making her too\nyoung to do so.\nGassin says the way some New Zealand history was written makes it seem as if Pakeha were \"colonial\nmediators\".\nAs the dominant group, they had a central position, which meant they could determine the relationship\nbetween other ethnic groups, she says.\n\"Pakeha are colonisers here on this land, so it's interesting. You could argue that they're the ultimate\nforeigner and migrant in the context of New Zealand history.\n\"But it's often that dominant group who gets to decide who are the foreigners, and gatekeep who gets to\nbe kept in and out. They're the ones that get to form a nation's identity.\"\nAfter the Treaty of Waitangi was signed, people could enter the country, but those who were not British\nwere considered aliens.\nBut throughout history, there have been barriers to prevent people - particularly those of colour - from\ncoming to New Zealand.\nIn 1881, immigration restrictions were placed on Chinese people, who had to pay a poll tax to enter.\nAfter World War I, German people, Marxists and socialists were restricted from entering New Zealand.\nThe 1920 Immigration Restriction Amendment Act, better known as the \"White New Zealand Policy\",\nmade it difficult for anyone not British to come to New Zealand till the 1970s. Permission had to be\ngiven by the minister of customs.\nChinese migrants were recruited by the Dunedin Chamber of Commerce to work in the Otago goldfields.\nThe first 12 men arrived from Australia in 1868 and, by late 1869, more than 2000 Chinese men had\ncome to the goldfields.\nPacific peoples had four waves of migration, dating from 800 to 1000 years ago. The first wave was the\ncoming of the Maori. The second and third waves were tied to European colonisation of the Pacific, with\npeople coming to Aotearoa as trainee teachers, missionaries, sailors, whalers and civil servants.\nThe fourth wave was linked to migration for economic reasons, with Pacific peoples finding work in the\nmanufacturing and service sectors after World War II.\nAfrican Americans arrived in colonial times, but most of the arrivals from Britain's African colonies in the"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193452, Requested 7156. Please try again in 182ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193452, Requested 7156. Please try again in 182ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Naperville officially denounces racism, moves toward inclusion action steps A resolution is on the books and an action plan is in place celebrating diversity and denouncing all acts of racism, intolerance and discrimination in Naperville.\nThe steps come after several occasions of reported racist acts at schools, restaurants and gas stations in the city and after what the resolution calls an \"increase in anti-Asian sentiment across the country caused by the COVID-19 pandemic.\"\nCity council members approved the resolution unanimously during a virtual meeting after hearing written comments from 10 residents and spoken statements from three others calling for public solidarity against racism. Speakers represented the Chinese, Japanese, Jewish and Latino communities.\n\"The most recent incidents concerning African Americans, and most recently, Asian American backlash, have sadly opened up new wounds, as I have felt that our city has been progressively moving toward greater acceptance of diversity,\" Naperville native Christine Simonson said in written comments read aloud during the meeting. \"As Naperville continues to become more diverse, I feel it is necessary for everyone to embrace and respect everyone’s differences.\"\nThe council also unanimously approved recommendations that call for implicit bias training and diversity recruiting enhancements for city staff members, development of a human relations commission, and engagement with community groups to further equity and inclusion.\nCity council members thanked Chinese community leader Nancy Chen Chen — she reached out a month ago seeking a city denouncement of acts against Asian Americans — and city council member Benny White for his creation of an inclusive discussion group called Naperville Neighbors United.\n\"We applaud your leadership in standing up against racism in all forms,\" Lucy Chen, board chairwoman of the The United Chinese Americans Illinois Chapter, said in written comments.\nResident Karen Peck said in written comments she is \"saddened that such a resolution needs to be codified.\"\nBut a few residents in written comments shared stories of what Simonson described as \"daily tormenting\" based on her race, or what Jennifer Chen called \"an ugly racial incident\" at a football game.\n\"As much as I loved and benefited from the Naperville I grew up in, I am sad to say that I can still recall every act of discrimination I witnessed in our school,\" Chen said in written comments. \"I do not wish these types of life lessons on any kid today. And that’s why this resolution — this commitment to our community, our kids, our families and our businesses — will continue to bolster Naperville’s strong reputation for being a place people choose to live.\"\nMayor Steve Chirico said it was sad but important to hear stories of past discriminatory treatment. He called the resolution and action steps denouncing racism \"a long time coming.\"\nWhite encouraged everyone to follow intent of the city actions by speaking up when improper treatment occurs. \"Step up and say something about it,\" White said. \"Say, ‘Hey, not in my town.’\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197045, Requested 7037. Please try again in 1.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197045, Requested 7037. Please try again in 1.224s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Reports of hate crimes and bias incidents in Anne Arundel County increased slightly in 2019. However, Montgomery\nCounty surpassed Anne Arundel as the jurisdiction with the highest instance of reports in the state, according to a new\nreport from the Maryland State Police.\nThe State of Maryland 2019 Hate Bias Report shows 81 reports of hate crimes or bias incidents in 2019, up from 78 in\n2018, continuing the trend of increases beginning in 2014. Many of the incidents were reported in the northern part of\nAnne Arundel County, with a heat map showing each zip code by color based on the range of incidents\nreported.Between 12 and 19 incidents reported in Pasadena ZIP code 21122; and between six and 11 incidents reported\nin each of Glen Burnie ZIP codes 21060 and 21061 and in Millersville ZIP code 21108.\nThese numbers include both hate crimes - or a crime motivated by prejudice - and hate bias incidents, which do not rise\nto the level of criminal charges.\nThe report distinguishes incidents as verified, meaning an \"investigation leads a reasonable and prudent person to\nconclude that the offender's actions were motivated, in whole or in part, by their bias\"; inconclusive, meaning the\nevidence is conflicting or incomplete; or unfounded, meaning that evidence or an investigation \"definitively indicates\nthat it was not motivated by bias.\"\nOf Anne Arundel County's 81 reported hate bias incidents, five were verified, 72 were inconclusive and 4 were\nunfounded. Prince George's County accounted for the most verified incidents with 29, 87.9% of the total number of\nreports of incidents in the county.\nHouse Minority Leader Del. Nic Kipke said he thinks it's wrong to assume that Pasadena is more racist than any other\npart of the county because of its incident counts. Perhaps reporting has increased due to increased attention to racial\ndiscrimination over the past few years.\n\"We are trying to stop these types of things from happening,\" Kipke said. \"Our entire community condemns racism, and\nwe are all invested to ensure that those problems end.\"\nCounty and state officials were alarmed after last year's report showed Anne Arundel with the highest numbers of\nreports.\nIn response, County Executive Steuart Pittman moved to treat racism as a public health issue; attended training on\npolicymaking through a racial equity lens; organized an educational hate bias forum in February with the Human\nRelations Commission; established an Office of Health Equity and Racial Justice within the health department; and\namended the county slogan to: \"The best place for all.\"\nThe findings of the report underscore that the county's slogan is aspirational, Pittman said.\nHe said he wouldn't have expected these efforts to curb racism would affect the data immediately but hopes it will make\nthe county a safer and more equitable place for all people.\nHe said that last year's report showing Anne Arundel at the top of the list served as a wake-up call, driving county\nofficials to act. But moving below Montgomery County isn't good news.\n\"I don't celebrate the fact that Montgomery County had more - that's also bad news,\" Pittman said.\nSince state police released this report last year, Pasadena has seen a flurry of activism. Young organizers made it the\nsite of several demonstrations for racial justice over the summer as communities across the nation grappled with\nsystemic racism and police brutality, and hyperlocal groups like One Pasadena have popped up to address racism.\nHarry Freeman, a member of the group's steering committee, said the group was founded after the publication of last\nyear's hate bias report.\nNow, he said, they are trying to remind the community that, \"These are real stories that affect real people.\"\nThe group doesn't view racism along strictly political party lines but as a cultural and historical problem. Nor does the\ngroup view racism purely as a Black versus white issue, Freeman said, noting that Latinos and Asian Americans also\nexperience discrimination.\n\"The thing about racism is that it's not genetic. It's not something you're born with,\" Freeman said. \"You're either\novertly taught to hate, or it's learned.\"\nAcross the state, more than 64% of incidents appear to be motivated by bias related to race, ethnicity or ancestry.\nIn Anne Arundel County, these race motivated incidents include: an effigy hung on the property of the county's first\nBlack councilman last October that was said to be a Halloween stunt, a swastika found at South River High School, or a\nracist video made by students at Northeast High School.\nElementary and secondary schools are the most common location for people to report hate bias incidents across the\nstate, followed closely by homes, according to the report.\nPittman said every incident reported to county police is referred to the Anne Arundel Crisis Response, and victims are\noffered support or connected with counseling services if needed.\nSince last year's report was released, Anne Arundel state legislators strengthened hate crime laws by making it illegal to\nthreaten or intimidate a group or individual with symbols including nooses or swastikas. The legislation was championed\nby Del. Mark Chang, D-Glen Burnie, who is one of the first Korean Americans to be elected to serve in Maryland's\nGeneral Assembly.\nIt passed this year, in a session shortened by the coronavirus pandemic, after two failures during the two previous\nsessions. The new law took effect on Oct. 1.\nIn testimony for the bill, Chang recounted the story of discovering a dead cat hanging from a noose in his yard\nshortly after immigrating to the United States. He was 7, he said, but he"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199438, Requested 7889. Please try again in 2.198s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199438, Requested 7889. Please try again in 2.198s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "BERGEN COUNTY, N.J. - Yena Choe, 16, was at Times Square on Saturday with six friends to film a video for the Korean-American Association of New Jersey's \"Samiljeol\" event, a remembrance of Korean resistance to Japanese rule.As they walked with Korean flags, an unmasked, middle-aged woman glared at the teens and told them, \"Get away from me. Stay far away from me,\" while waving her hands, Choe said.\"We did nothing to provoke this woman,\" said the Leonia High School sophomore, who said she and her friends were all wearing double face masks. \"Yet based on our appearance, and quite possibly even based on the flags flying in our hands, we were treated as if we were a contagious disease.\"For the rest of the day, I was afraid,\" she said. \"I was afraid that something like that could happen again, that next time it could be worse than a glare and a rude remark.\"Days later in the city's Chinatown, Salman Muflihi was arrested after allegedly stabbing a 36-year-old Asian man, police said. Muflihi is facing charges of attempted murder as a hate crime, assault as a hate crime, forgery and criminal possession of a weapon, according to New York Police Department statements to USA TODAY on Friday.The incident is a part of a series of recent violent crimes against Asians and Asian Americans in New York and across the country.New York Mayor Bill de Blasio and the commander of the police department's Asian Hate Crime Task Force spoke at a news conference Tuesday about what the city is doing to prevent anti-Asian hate crimes.\"An attack on Asian New Yorkers is an attack on all of us,\" de Blasio said.Last year, there were 28 incidents of COVID-19 related hate crimes against Asians in New York and two so far in 2021, according to Deputy Inspector of the city's Anti-Asian Hate Crimes Task Force Stewart Loo.A string of high-profile attacks on Asian Americans are surfacing from coast to coast a year into the pandemic. In January, an 84-year-old San Francisco immigrant from Thailand died after he was shoved to the ground on his morning walk. In February, a Vietnamese woman was assaulted and robbed of $1,000 in San Jose and a Filipino man was slashed with a box cutter on the subway in New York City.The head of Oakland's Chinatown Chamber of Commerce has collected more than 20 incident reports and videos of small businesses getting robbed and owners and customers assaulted, he told San Francisco's KGO-TV.Attacks like these have continued despite President Biden's executive order banning the federal government from using racist language to discuss the pandemic, a turnaround from months of his predecessor mocking the \"China virus\" and \"kung flu.\"Around the world, as coronavirus has devastated the economy, Asians have become targeted for the pandemic that started in Wuhan, China. They have been yelled at, spat on and beaten up, according to media reports across the globe.Perpetual foreignerIn Wyckoff, New Jersey, last June, a Chinese restaurant was vandalized with the words \"coronavirus\" and \"go home\" spray-painted on the windows and sidewalk.North Jersey is home to one of the state's largest populations of Asian Americans with 17% of Bergen County identifying as Asian. In towns such as Leonia and Fort Lee, Asian Americans make up about 40% of the community.There were 47 race-based incidents against Asian Americans and Pacific Islanders in New Jersey from March 19 through Dec. 31, the eighth most of any state, according to the advocacy group Stop AAPI Hate. Of the cases, 64% were verbal attacks while 13% were physical assaults. In New York City, 259 incidents were recorded against Asian Americans in the same period with 81% being verbal and 12% physical.Burdened by the model minority myth, Asians are easy targets in times of downturn, said Russell Jeung, co-founder of Stop AAPI Hate and professor at San Francisco State University. Despite their deep roots in the U.S. Asian Americans will forever be considered foreigners by some people, he said, a stereotype that has persisted since the first immigration of Asians in the 19th century to help build the Transcontinental Railroad.\"Since we don't belong, we can be spat on,\" Jeung said.U.S. Rep. Andy Kim, who represents New Jersey's Third District, says the hate crimes seen over the past weeks are reprehensible.\"Seeing our elders targeted and attacked has been hard to watch and difficult to explain to my two boys,\" said Kim, whose sons are 3 and 5. \"New Jersey has been a place where generations of Asian immigrants have found open arms and incredible opportunities.\"We cannot put an end to this hate until we see tangible results at all levels of government and society,\" Kim said. He called for executive actions from the administration and legislation from Congress.Asian community activists in New Jersey are aware of the rise in hate crimes. While they are vigilant and cautious, they feel the Garden State is a welcoming place for immigrants.\"We are in a much better situation than in New York City,\" said Michelle Song, executive vice president of the Korean-American Association, who lives in Somerset. New Jersey values diversity, said Song, who emigrated from South Korea in 1995. It's also a suburban environment where automobiles are the main mode of transportation, she said, leaving fewer chances for random encounters that can flare into harassment.\"We hardly walk here, we don't see people face to face,\" added Kirby Tan, a Chinese community group organizer who lives in Tenafly.Tan has lived in New Jersey for 35 years. After arriving from Malaysia to study in San Diego at a time when there were few Asians, he recalls developing a buddy system for safety when he went outside. His neighborhood in Ten"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193068, Requested 7114. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193068, Requested 7114. Please try again in 54ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Today's streaming events\n1 p.m.: Spokane Mayor Nadine Woodward speaks with S-R reporter Adam Shanks\nRead more: In Wednesday's other Northwest Passages streaming events, the S-R's Theo Lawson chats with WSU coach Nick Rolovich (SPORTS) and Kip Hill talks with Lisa Brown (BUSINESS, 1).\nFor more information: Visit spokesman.com/bookclub/livestream/ to stream these events and watch previous ones.\nEastern Washington University sociology professor Pui-Yan Lam remains hopeful despite what she called \"an alarming\" increase in anti-Asian discrimination and hate crimes across the world as a result of the COVID-19 epidemic.\nIn a Tuesday interview conducted for The Spokesman-Review's Northwest Passages Book Club, Lam said there have been no recent reports of anti-Asian hate crimes from the Spokane Police Department. But last week a reporting center co-founded by San Francisco State University's Asian American Studies Department said it has received more than 1,100 reports of coronavirus discrimination from Asian Americans across the country.\n\"What I have heard firsthand is friends of Asian descent telling me they're worried to go out to the grocery store alone - that if they go out wearing a mask, they will attract negative attention,\" Lam said. \"That fear is here in Spokane communities, too.\"\nLam emphasized the power of language as a weapon to spread either hate or compassion.\n\"I think it's easy for well-meaning people to dismiss discrimination as 'just words that will go away,' \" she said. \"I would like people in Spokane to not only denounce hate, but also to choose compassion when we speak.\"\nLam said misplaced language creates an \"us vs. them\" dynamic that leads to the dehumanization of other groups - \"especially linking our group to a disease,\" she said, referencing the World Health Organization's May 2015 \"Best Practices for the Naming of New Human Infectious Diseases\" guidelines, which seek to \"avoid causing offense to any cultural, social, national, regional, professional or ethnic groups.\"\nSpokane is becoming more diverse, Lam said. \"One thing people would say to me when I first got to EWU was 'Spokane never changes' - we will always be very homogenous,\" she said. \"But I'm sure people are noticing the increased diversity, and I know most folks embrace that.\"\nShe is looking forward to seeing Spokane County's 2020 census data, and said the many things that can happen as a result of increased diversity can \"change Spokane for the better.\"\nAnd Lam believes the COVID-19 epidemic can be a defining moment for Spokane.\n\"When I take walks in my neighborhood, and people smile and wave at me - Spokanites do that - I appreciate it. At this moment, those simple gestures mean a whole lot more to me,\" she said.\n\"With all the anxiety we all have to face right now, just seeing smiling, friendly faces makes me feel like I'm part of the community.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197639, Requested 7976. Please try again in 1.684s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197639, Requested 7976. Please try again in 1.684s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Racial discrimination has always been a scar on the American society, one that has been continuously ripped wider open since\nthe COVID-19 pandemic.\nAs some U.S. politicians are bent on politicizing the pandemic and maliciously making China a scapegoat for their\nincompetence in containing the outbreak, the virus of racial discrimination against Asian Americans is also spreading across\nthe country.\nAsian Americans are frequently humiliated and even attacked in public, and suffering continuous discrimination on social\nmedia platforms.\nThe racial group faced a double whammy of unemployment and discrimination amid the outbreak, according to many reports\nrecently released by American research institutions and civil society organizations.\nHowever, some U.S. officials have played dumb about the situation, which has fully revealed the hypocrisy and cruelty of the\nso-called American human rights.\nRelevant data have been rather dreadful. Racial discrimination against Asian Americans is becoming more common, pointed\nout a survey report from American think tank Pew Research Center.\n31 percent of Asian Americans said they have been subject to racist slurs or jokes since the pandemic began, while 26\npercent said they’ve feared someone might physically attack them, the survey found.\nAs the number of racist attacks against Asian Americans in various areas of New York has increased significantly, the New\nYork City Police Department has even established an Asian Hate Crime Task Force recently, the first task force in the U.S. that\nis aimed at Asian hate crimes.\nAcross the country, Asian American health-care workers have reported a rise in bigoted incidents, according to an article\npublished on The Washington Post.\nThe racial hostility has left Asian Americans, who represent 6 percent of the U.S. population but 18 percent of the country’s\nphysicians and 10 percent of its nurse practitioners, in a painful position on the front lines of the response to the coronavirus\npandemic, the article said.\n“Some COVID-19 patients refuse to be treated by them. And when doctors and nurses leave the hospital, they face increasing\nharassment in their daily lives, too,” it continued.\nWhat lies at the root of such pernicious consequences is certain U.S. politicians’ misdeed of spreading “political virus”.\nSome U.S. officials have taken every opportunity and played every trick they can to attack and stigmatize China, including\ncalling the COVID-19 “Chinese virus” and “Wuhan virus”, which has fostered extremism and racism in the U.S. and become\nthe main culprit of the dramatically increasing attacks and threats targeting Asian Americans.\nThe attacks and smear campaign launched against China by some American politicians have ignited anti-Asian sentiment,\naccording to an article published by the British Broadcasting Corporation (BBC).\n“Yet public figures and politicians play a key role in promoting racial equality and non-discrimination principles. In this\nregards, it is dismaying to witness State officials adopting alternative named for the COVID-19 coronavirus,” said Tendayi\nAchiume, the United Nations (UN) Special Rapporteur on contemporary forms of racism, racial discrimination, xenophobia and\nrelated intolerance.\n“Indeed, instead of using the internationally recognized name of the virus, these (U.S.) officials have adopted names with\ngeographic references, typically referring to its emergence in China. This sort of calculated use of a geographic-based name\nfor this virus is rooted in and fosters racism and xenophobia,” she pointed out in a recent statement.\nSystemic racism is a malady afflicting the American society.\nColor has obviously played a major role in determining the fate of many Americans, U.S. scholar Thomas Sowell wrote in his\nbook Ethnic America: A History.\nIn history, the notorious Chinese Exclusion Act and crimes against Asian Americans such as sending a large number of Asians\nto concentration camps during the World War II have startled the world.\nAsian Americans now still suffer continuous harassment, exclusion and structural discrimination in the U.S., arousing great\nconcern among the international community and American people with breadth of vision.\nThis year, the special procedures of the U.N. Human Rights Council have repeatedly condemned the U.S. for its racial\ndiscrimination and hate speech.\nRecent hate crimes and violent assaults against people of Asian descent should sound an alarm for America, said nearly 200\nAmerican foreign policy scholars and former diplomats in a joint statement issued on USA Today.\nThey called upon U.S. leaders at every level and in every sector to take action against anti-Asian racism and express support\nfor Asian diaspora and Asian American and Pacific Islander (AAPI) communities.\nOnly those who lack competence themselves would deliberately invent divisions and confrontations against others.\nThe conflicts and antagonisms in the American society won’t stop worsening unless certain U.S. officials break off the political\nscheme to pass the buck to China for their misconduct in dealing with the epidemic.\nOtherwise, other groups of people, in addition to Asian Americans, may end up a new victim of some U.S. officials.\nIn the U.S., a country that claims to be a “beacon of freedom”, some U.S. politicians, however, have ignored the international\nhuman rights law, openly incited and condoned racial discrimination, and violated the bottom line of human civilization by\nblatantly trampling on human rights, which is by no means tolerable.\n(Zhong Sheng is a pen name often used by People’s Daily to express its views on foreign policy.)"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197243, Requested 6412. Please try again in 1.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197243, Requested 6412. Please try again in 1.096s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": ", who is one of the first Korean Americans to be elected to serve in Maryland's\nGeneral Assembly.\nIt passed this year, in a session shortened by the coronavirus pandemic, after two failures during the two previous\nsessions. The new law took effect on Oct. 1.\nIn testimony for the bill, Chang recounted the story of discovering a dead cat hanging from a noose in his yard\nshortly after immigrating to the United States. He was 7, he said, but he carries the trauma of that experience to this\nday.\nHe said he hopes the new law, and the punishments that come with violating it, will deter people from bias-motivated\nbehavior, but the work of the legislature to \"eradicate hate\" is not done.\nThough the data does not reflect the past seven months of the coronavirus pandemic, Chang said he has faced\ndiscrimination due to his Asian American identity.\nDespite guidance from the World Health Organization to avoid referring to the virus by names that include geographic\nlocations, President Donald Trump has continued to do so as recently as last week, while he is recovering from the\nvirus himself.\nChang called the narrative disgusting and said he was very disappointed it has seeped into this community.\n\"I have the honor of being a representative,\" he said. \"I just wonder, for those who are not in that position, what are\nthey experiencing?\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196508, Requested 7807. Please try again in 1.294s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196508, Requested 7807. Please try again in 1.294s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Sonny Vinuya hasn't decided if he'll vote again for Donald Trump in the battleground state of Nevada.\nThe Filipino American businessman in Las Vegas is personally offended by the president's use of a racist slur at\nrecent reelection rallies, where he mocked China and the COVID-19 pandemic's origins in Asia. But most\nimportant to Vinuya is the economy, which has also been sinking into a pandemic-triggered recession despite a\nrobust stimulus package.\nThough it's tough for the registered Republican to swallow the racism against his own community, Vinuya said\nhe doesn't think Trump is trying to alienate Asian American voters when the president uses derogatory terms at\ncampaign events or continues to call COVID-19 the \"Chinese virus.\"\nThe Pew Research Center recently declared Asian Americans to be the fastest-growing racial or ethnic group in\nthe U.S. electorate, but they are also arguably the least competitive voter block for Trump when considering\nwhere they live and how they relate to the Republican Party.\nIn Nevada, where Vinuya lives, Asians make up more than 10% of voters in a state both Democrats and\nRepublicans will fight for in the fall.\nWith his anti-Asian rhetoric, Trump is making the calculation that he has more to gain with his loyal base of older white voters thrilled by his inflammatory statements than to lose among the Asian American community,\nsaid Karthick Ramakrishnan, a public policy professor at the University of California, Riverside and founder of\nAAPI Data, which tracks Asian Americans.\nThat's because Asian Americans largely vote in very blue districts and otherwise non-competitive states,\nRamakrishnan said. A third of all such voters live in California alone.\n\"Part of the reason we don't see very much outreach is because Asian Americans tend to live in non-competitive\nstates in presidential elections,\" Ramakrishnan said.\nTrump's words have angered many Asian Americans and drawn condemnation from Trump's Democratic rival Joe\nBiden and former President Barack Obama. Republicans have also denounced the racist slur, notably Kellyanne\nConway, a White House counselor who is married to an Asian American, George Conway, whose mother was\nfrom the Philippines.\nWhite House Press Secretary Kayleigh McEnany defended the president's rhetoric, saying he's not being racist\nbut merely linking the virus to its place of origin. Trump in March also insisted that Asian Americans were\n\"amazing people\" and not at fault for spreading the virus.\nSince the virus took hold of the U.S. in March, advocates have also reported a rise in anti-Asian aggression and\nviolence from people blaming them for the pandemic. A group called Stop AAPI Hate said on July 1 that it has\ntracked 832 incidents of discrimination and harassment in California over the past 12 weeks.\nBiden at a June 27 town hall for Asian American voters slammed Trump's \"dangerous theories\" as xenophobic.\n\"Words matter and the president's words matter even more,\" Biden said.\nThe number of Asian Americans aligning themselves with the Democratic party has increased over the past 20\nyears while support for the GOP has trended down. A different Pew analysis in 2018 showed Democrats held a\n2-to-1 advantage among Asian American registered voters. The 27% who identified as or leaned Republican\nrepresented a 6% drop since 1998.\nFor Trump, AAPI Data found nearly all major Asian American ethnic groups held an unfavorable view of the\npresident. The only exception was the 62% of Vietnamese surveyed in 2018 who said they held a favorable\nimpression of him. On the other end of the spectrum, just 14% of Japanese voters felt the same way.\nIn the top 10 states with the largest Asian American voting populations, Trump in 2016 won only Texas and\nFlorida. But the ultimate swing state in this year's election, Ramakrishnan said, is unlikely to be determined by\nthe 3.6% of Florida voters who are Asian American given how large the state is.\nThat means in smaller states like Nevada, the deeper concentration of eligible Asian American voters -- the\nfourth-highest behind Hawaii, California and Washington state -- could potentially move the needle. About 11%\nof Nevada's voters are Asian.\nTrump lost the Silver State in 2016 to rival Democrat Hillary Clinton by just 2.4 percentage points, though\nNevada now leans more blue.\nFor Vinuya, who has been courted by the Trump campaign as the president of the Las Vegas Asian Chamber of\nCommerce, he knows he could be one of the few Asian American voters nationally who can make a difference\nfor the president's reelection prospects.\nVinuya said he's expressed his concerns about the anti-Asian slur to Trump's team, especially as he's trying to\nhelp his 600-plus members overcome the virus-related stigma and discrimination evident against Asian\nAmerican and Asian immigrant small business owners in Las Vegas.\n\"I gave them my two cents. That's pretty much it,\" Vinuya said. \"I don't want to waste my time on something\nI cannot control.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196094, Requested 7498. Please try again in 1.077s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196094, Requested 7498. Please try again in 1.077s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A March 21 attack on a Chinese American man jogging along a Naperville trail has left the Asian community there uneasy, says resident Nancy Chen.\nThe women spit on and threw sticks at the man and told him \"to go back to China,\" Chen said.\n\"It just does not reflect the kind of Naperville that I have been calling home for almost now 50 years,\" she said.\nA Naperville Crime Stoppers poster shows two unidentified women who are wanted for questioning and offers a $1,000 reward for information leading to an arrest. Naperville police are investigating.\nChen, an advisory board member of United Chinese Americans Illinois Chapter, is urging the Naperville City Council to adopt a resolution condemning racism and supporting Asian Americans.\nViral racism: Since March 19, the Asian Pacific Policy and Planning Council’s STOP AAPI HATE reporting center has received more than 1,100 reports nationwide of coronavirus-related discrimination against Asian Americans.\nA majority of incidents occurred in grocery stores, pharmacies and big box retail stores, the center reports.\nChen said there’s been a rise in anti-Asian sentiment and hate crimes in the wake of President Donald Trump and other politicians labeling COVID-19 a \"Chinese virus.\"\nTo report a hate crime, visit a3pcon.org/stopaapihate.\nE-learning access gaps: Suburban educators are realizing gaps in student access to e-learning due to technological challenges affecting low- income and minority families.\nAt Elgin Area School District U-46, officials are seeing a significant drop in participation among Hispanic and African American students. Attendance rates post-COVID-19 lockdown declined during the first week of distance learning — from 95% to 90% among Asians, from 94% to 89% among whites, from 92% to 76% among Hispanics, and from 90% to 66% among blacks.\n\"Internet access is obviously a big issue with some families,\" U-46 Superintendent Tony Sanders said. \"We are working on a solution for that. We are doing a second round of technology distribution this week. Another one is planned when additional devices arrive.\"\nFree food: The Islamic Foundation in Villa Park will begin distributing free food and groceries to community members in need starting today. The event will run from 1 to 4 p.m. at the mosque on 300 W. Highridge Road.\n\"We have been doing this for the last three years all through the year,\" foundation Chairman Aftab Khan said. \"This year, we have distributed food and groceries to over 1,000 needy families.\"\nThe foundation is seeking donations for its Chicago Food Distribution Program, which has collected nearly $12,700 toward a $100,000 goal.\nCOVID-19 support helpline: A coalition of religious, civic and social service organizations from the greater Chicago area has formed a task force to coordinate COVID-19 support services. It includes free tele-health consultations, access to medical, mental health and dental care, food distribution, emergency monetary assistance, and help filing for unemployment benefits. To access services, call the helpline, (847) 737-1785, staffed by professionals and volunteers.\nThe task force comprises members of Council of Islamic Organizations of Greater Chicago, local and national relief agencies, associations of medical and dental professionals and community service groups.\nMore black docs: Being comfortable with one’s doctor is key to dealing with any health crisis. For blacks, it’s harder to build that trust when most doctors don’t look like them, and that can be an issue in the suburbs, experts say.\n\"To open yourself up and be vulnerable, you want to do that typically among people who understand the nuances,\" said Dr. Courtney Coke, an African American radiation oncologist at Advocate Sherman Hospital in Elgin. \"That vulnerability requires certain sensibility on the part of physicians.\nHistorically, patients who are of diverse backgrounds may not feel comfortable venturing outside of their traditional spaces.\" Having a culturally competent physician empowers patients, he added."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195176, Requested 8132. Please try again in 992ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195176, Requested 8132. Please try again in 992ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "IN early March, Yuming Wang, a Philadelphia lawyer, watched with increasing concern reports of racist incidents against Asian Americans over coronavirus fears trickle in from other cities. A Chinese woman was attacked for wearing a mask in New York City. A 16-year-old Asian American student was physically assaulted in Los Angeles.\nWang, who is the honorary chairman of the Pennsylvania United Chinese Coalition, decided to create a WeChat group for Chinese Americans who were worried about the rising numbers of racist incidents during the pandemic. Soon after, he began working closely with state and local police, the Philadelphia Commission on Human Relations, and other law enforcement agencies to encourage Chinese Americans who experience racist incidents to report them.\n\"Chinese Americans have been increasingly worried about their safety every day, and the issue is getting deeper and deeper into their mental health concerns,\" Wang said. \"Among [the group members], the biggest concern is that when someone goes somewhere, [they] may be attacked, verbally or physically, because of being Asian or Chinese Americans.\"\nMany Philadelphians are reluctant to leave their homes because of fears over contracting COVID-19. But members of the city's Asian American communities have an additional worry - many are growing increasingly anxious about dealing with racist incidents, which can lead to negative mental health effects.\nThe link between racism and mental health\nThe number of hate crimes against Asian Americans has been decreasing for 15 years, according to the FBI's Uniform Crime Report data from 2003 to 2017. But in late March, federal law enforcement began to warn communities that hate crimes against Asian Americans will increase as the coronavirus crisis goes on.\nThis concerned mental health experts, as a growing body of research indicates a link between racial discrimination and anxiety and depression. In a 2013 review of 121 studies on the effects of racism on youth, researchers found that youth who experienced discrimination were significantly more likely to suffer from mental health problems. A 2007 study that surveyed 2,047 Asian Americans identified racial discrimination as a significant predictor of mental health disorders over a 12-month period.\n\"When people are treated unfairly, it can create a stress response called allostatic load,\" said Gilbert Gee, a professor in the department of community health sciences at the Fielding School of Public Health at the University of California Los Angeles. \"Allostatic load can impair the body in many ways, such as weakening our immune systems.\"\nEven before the pandemic began, Asian American communities were considered a high-risk group among mental health experts. Survey data from the 2011 National Latino and Asian American Study found that Asian American women have a 17% chance of developing a psychiatric disorder over their lifetime, and they are three times less likely to seek mental health services than white women are. Results from the 2018 National Survey on Drug Use and Health found only 6.3% of Asian Americans who were age 18 and over received mental health services in 2017, compared with 18.6% of white people.\nRacism against Asian Americans has existed for centuries - COVID-19-related violence is just a recent iteration. Kevin Nadal, a psychologist and professor at the John Jay College of Criminal Justice whose research focuses on the mental health effects of microaggressions, said it's important to acknowledge that the fear of experiencing racism is very real.\n\"Even if people aren't experiencing direct incidents, just the knowledge of it can cause them to feel anxious, depressed, or hypervigilant, which can lead to other mental health issues,\" Nadal said. \"It's a collective trauma - the anticipation comes from people of your shared identity having experienced violence.\"\nHate speech on social media is one example. In a study conducted by Gee, early results from an analysis of nearly one million tweets between November and March suggest that negative comments about Asians increased about 70%. Gee also found that negative comments about Asians increased 167% the week after President Donald Trump used the phrase China virus during a news conference.\n\"It's really important to acknowledge that language matters and has a direct effect on marginalized communities that may be targeted,\" said Nadal, who was not involved in the study. \"When the president assigned blame to China instead of acknowledging that this virus, like most viruses, can come from a number of places, he definitely emboldens people to act out on their racism.\"\n'Discrimination against Asians protects nobody'\nMin Son and her family began wearing masks to grocery stores in March, even before the Centers for Disease Control and Prevention recommended everyone wear a mask. But they worried that the masks could make them a target.\n\"We were super cautious whenever we had to go outside,\" said Son, a senior at Ursinus College. \"Our president calling COVID-19 a 'Chinese virus' also played a part into our fear of discrimination. I am not Chinese, but to others, I may look like one. And I am not more vulnerable against this virus than other races. Viruses don't discriminate, and discrimination against Asians protects nobody.\"\nThe stress that results from anticipating racism can negatively affect the immune system, according to Suzanne Chong, a psychologist at Ursinus College.\n\"The immune system will be temporarily put on half for the body to be vigilant for possible threatening situations,\" she said. \"The anticipation of harassment might manifest as somatic symptoms of headaches, gastrointestinal disturbances, physical aches and pains, and an inability to fight off infections.\"\nChong emphasized that people should not feel alone in facing fears over discrimination and race-based attacks. She pointed out that many organizations, like the U.S. Commission on Civil Rights, have condemned anti-Asian American rhetoric. She also encouraged people to reach out for support from friends, family members, community and religious leaders, and mental health professionals because \"acknowledging the reality of racism and the impact . . . will help dispel the heaviness and burden of being targets.\"\nChong encouraged people to speak out if they see someone being rude"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194534, Requested 7753. Please try again in 686ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194534, Requested 7753. Please try again in 686ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "The authorities in Alameda County, California, have created a special response unit focused on crimes against\nAsians, particularly older Asians.\nThe move last Monday came after a string of violent assaults on Asian Americans, including one in Oakland,\nCalifornia, in which a young man violently pushed a 91-year-old Asian man to the ground.\nThe video has sent a chill through the Asian American community. Two more Asian Americans, a 60-year-old\nman and a 55-year-old woman, were also attacked that same day - Jan 31 - and had to be taken to hospital,\nthe San Francisco Chronicle reported.\nA 28-year-old African American has been arrested. He is a suspect in all three cases.\nThe incidents came just days after Mr Vicha Ratanapakdee, an 84-year-old Thai man, died after he was similarly\nattacked while out on a morning walk in his San Francisco neighbourhood on Jan 28.\nA 19-year-old man has been arrested on suspicion of murder and elder abuse in that case.\nAt a press conference last Monday in Oakland's Chinatown, Alameda County district attorney Nancy O'Malley,\nannouncing the task force, said: \"The rapid increase in criminal acts targeted against members of the Asian\ncommunity, particularly Chinese Americans, who live and work in Alameda County is intolerable.\"\nLast week, the Committee of 100, an influential non-partisan group of prominent Chinese Americans in\nbusiness, government, academia and the arts, released a White Paper commissioned by the Economist\nIntelligence Unit on the contributions of Chinese Americans to the United States.\nThe release was in response to increased anti-China sentiment in \"an age inflamed with resurgent racism, and\nwith geopolitical tension reversing decades of fruitful exchange with China\".\n\"The US has reached a moment when it is critical to examine how diversity has benefited the society and how\nminority groups such as Chinese Americans have, over time, become identified with the country itself,\" the\nreport says.\nChinese Americans contributed more than US$300 billion ($373 billion) to US gross domestic product in 2019\nthrough consumer spending, supporting three million jobs, the report says.\nUNACCEPTABLE ACTS\n“The rapid increase in criminal acts targeted against members of the Asian community, particularly Chinese\nAmericans, who live and work in Alameda County is intolerable.”\nALAMEDA COUNTY DISTRICT ATTORNEY NANCY O'MALLEY, announcing the task force.\n\"There are over 160,000 Chinese American-owned businesses in the US, generating approximately US$240\nbillion in revenue and supporting 1.3 million jobs as of 2017,\" it adds.\nLast August , the United Nations special rapporteur on contemporary forms of racism, racial discrimination,\nxenophobia and related intolerance, in a note on the US, wrote: \"Racially motivated violence and other incidents\nagainst Asian Americans have reached an alarming level across the US since the outbreak of Covid-19.\"\nIt added: \"Chinese Americans and other Asian Americans, including those of Korean, Japanese, Vietnamese,\nFilipino, and Burmese descent, among others, have been subject to racist, xenophobic attacks.\"\nIt said the attacks included being spat on, blocked from public transport and beaten.\nThe Jan 6 storming of the US Capitol demonstrated the danger of anti-Chinese sentiment amplified to deafening\nlevels by right-wing media.\nTake, for example, Larry R. Brock, a retired US Air Force lieutenant-colonel photographed carrying zip-tie\nhandcuffs on the Senate floor during the insurrection by hundreds of Trump supporters.\nA week before, he wrote on Facebook that he saw no distinction among the Democrats, the Biden administration\nand \"an invading force of Chinese communists\".\nOn Jan 27, President Joe Biden signed an executive action directing federal agencies to combat xenophobia.\n\"Today, I'm directing federal agencies to combat the resurgence of xenophobia, particularly against Asian\nAmericans and Pacific Islanders, that we've seen skyrocket during this pandemic. This is unacceptable and it's\nun-American,\" the President said.\nThe Committee of 100 welcomed the statement, but feels more is needed, hence the White Paper.\nCommittee of 100 president Zhengyu Huang, who is based in San Francisco, told The Sunday Times: \"Last year,\nwe saw almost 3,000 documented cases of anti-Chinese and anti-Asian incidents.\n\"We knew we had to speak up forcefully, because racism and discrimination are unfortunate, negative aspects of\nsociety.\n\"Despite 175 years of contribution, we still suffer from the perpetual foreigner stereotype, and that\nstereotype has been exacerbated by two seismic trends - increasing tension and competition between the US\nand China, and Covid-19.\""
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194316, Requested 6792. Please try again in 332ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194316, Requested 6792. Please try again in 332ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "I was fighting racist mac and cheese in Philadelphia last week.\nVichar Ratanapakdee, an 84-year-old Thai man, was shoved to his death in Oakland, Calif., on Jan. 28 after recently receiving his first dose of\nthe COVID-19 vaccine. Christian Hall, a Chinese adoptee, was shot to death in December by Pennsylvania State Police in the Poconos, his\nhands up in supplication. These are just two examples from dozens of reports of violence and hate against Asians in America documented\nsince last March.\nAmid all this, I had to fight racist mac and cheese.\nYes, the mac is real. A Roxborough restaurant called housemade mac topped with “Chinese chili garlic sauce” its “COVID Mac,” as CBS\nreported last week. Days later, the Mayor’s Commission on Asian Pacific American Affairs, of which I am the treasurer, released a joint\nstatement with the full suite of the mayor’s commissions and the Office of Immigrant Affairs to disavow racist attacks on Asian Americans,\nincluding but not limited to the menu item. Meanwhile, its creators have since tried to delete it from memory, which is the most telling thing\nabout it.\nIn the lede of a Feb. 22 Washington Post article, the attacks on Asian Americans are described as “high profile.” High profile is not the phrase\nwe use in my family to describe being spat at, hearing slurs yelled at us. Those aren’t the words we use when a Chinese grandmother is\nbeaten up on the street. High profile isn’t how we describe being deported because we pray to the wrong God or pay too little to be here.\nHigh profile isn’t how we describe being jailed because we don’t speak English, being evicted because we don’t have citizenship. When those\nthings happen, we don’t call the story high profile. We use words like tragic, hate-filled, frightening. High-profile attacks is a phrase that\nhappens when we Asian Americans are treated as data, as ingredients. It reduces us to shocking news items that prompt brief hand-wringing\nbefore most coverage moves on. We don’t begin to understand how we are victims of hate by counting how many witnesses we had.\nThe news this month isn’t that Americans hate Asians. That is not news to us. The news is that others need the news to remind them that\nAsians in America deserve to be treated as complete humans. The news is that anti-Asian prejudice runs so deep I have to spend time\nfighting racist — yes, racist — mac and cheese that equates an Asian condiment with COVID-19.\nI see many Americans are reiterating the history of the exclusion act, internment order, the travel bans, to remind some abstract “us” that\nsome abstract “America” criminalized participation in the human experiment for Asian immigrants and their descendants. The secret is that\nthe “us” and the “America” are the same: a popular majority voted for the presidents who signed these orders. Yet, even after so many “high\nprofile” incidents of hate, America still hasn’t learned its lesson.\nOur so-called “minority group” runs full throttle to prove our humanity today. We want everyone else to see all of us. It is a lot of ground to\ncover, so let me acknowledge our collective fatigue. We are fighting for our dignity with the same depletion of energy as everyone else. We,\ntoo, are inundated by crowdfunded health-care services and funerals. We, too, are collapsing under the breathless ferocity of working around\nthe clock. We, too, are making the best of keeping everybody fed, housed, loved, while the world falls apart.\nBut on top of that, as an Asian American leader in Philadelphia, last week I was fighting mac and cheese.\nI take up what may sound like a “silly” fight because this is the complex reality of how we as Asians have to prove our humanity; because the\nonly rule of engagement that matters now is to fight like hell for every single thing in communities that continue to be devalued. Historically\nothered populations like ours have to define microaggressions, while also navigating justice for the murders of our elders.\nMy hope is not to be perceived as arch or ironic in addressing a fast-food item, or worse, to be perceived as apathetic because I am not\ndemanding redress for more serious crimes. To speak humbly is dangerous for Asian Americans because the perception of our passivity, of us\nas “quiet,” is the greatest lie we’ve ever belied. So let me be clear. The problem is not that we are silent. The problem is that we are not\nheard. And that’s why I keep fighting. Listen to us the first time we tell you we deserve to be treated as complete humans, so we do not\nbecome high-profile victims when we take our last breaths."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193456, Requested 8018. Please try again in 442ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193456, Requested 8018. Please try again in 442ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Biases are forms of discrimination in which we hold unfair beliefs about an entire group of people. These biases can be conscious (explicit) or unconscious (implicit).\nBusiness Insider spoke to Dr. Jasmine Marcelin, a doctor at the University of Nebraska Medical Center, and Dr. Chloë Fitzgerald, a researcher of implicit bias, to gain a greater understanding of bias in medicine.\nBoth explicit and implicit bias are prevalent in the medical field, and they can affect how patients of different races, genders, faiths, and sexualities are treated.\nDue in part to biases, black women are three to four times more likely to die from pregnancy-related causes than white women, according to the CDC.\nThough there has been great progress in reducing explicit biases, implicit biases are much harder to tackle, and they take active participation to identify and eliminate. Visit Insider's homepage for more stories.\nBiases are present in many human interactions. These typically unfair assumptions, which can encompass entire groups of people, affect how we feel about and treat members of other races, genders, faiths, weights, sexualities, and abilities. In healthcare, the effect of biases on patients can lead to dangerous consequences.\nExplicit biases, or conscious biases, are easier to detect because they are overt, according to Dr. Chloë Fitzgerald, who co-authored Implicit bias in healthcare professionals: a systematic review, which was published in BMC Medical Ethics.\n\"If someone had explicit bias, they might say, 'Oh, you know, obese people just don't work as hard, or they're lazy,'\" Dr. Fitzgerald told Business Insider. However, \"if it was implicit, they might choose someone else over an obese person to do a task because they'll assume the obese person will be slower to do it,\" she said.\nDr. Jasmine Marcelin, who co-authored The Impact of Unconscious Bias in Healthcare: How to Recognize and Mitigate It in The Journal of Infectious Diseases, said a lower presence of minorities in the medical field perpetuates bias.\nDr. Fitzgerald and Dr. Marcelin identified ways biases affect the healthcare industry.\nExplicit bias against diverse groups has led to extreme abuse within the medical profession, like the Tuskegee Syphilis Study conducted between 1932 and 1972.\nThe Tuskegee Syphilis Study was an experiment that spanned 40 years, starting in 1932. It involved 600 black men, 399 with syphilis, and 201 without the disease, to test the effects of untreated syphilis. The study was carried out without their consent, as researchers told the men that they were being treated for \"bad blood,\" a term used to describe several ailments, including syphilis, anemia, and fatigue. (In 1974, a $10 million settlement was reached out of court with the participants and their families, and in 1997, President Bill Clinton apologized to them on behalf of the country.)\n\"The Tuskegee study demonstrated how conscious bias, in this case manifested in the form of racism, led to the unethical treatment of black men that continues to have long- lasting effects on health equity and justice in today's society,\" Dr. Marcelin said.\nDue in part to biases, black women are three to four times more likely to die from pregnancy-related causes than white women, according to the CDC.\nDr. Fitzgerald said black women were historically abused by white doctors performing gynecological procedures on them without their consent. These doctors held the false belief that black women somehow experienced less pain.\nThe field of infectious diseases is also affected by implicit bias, according to Dr. Marcelin. She is passionate about reducing biases in the field of infectious disease.\n\"I definitely think from a pandemic standpoint there is a huge risk for minority groups who are affected. We have a lot of data that shows that the effects of systemic racism and bias on minority groups has led to worse outcomes in those groups,\" she said.\nBlack people are 2.4 times more likely to die from the coronavirus.\nData from AMP Research Lab revealed that there were 54.5 coronavirus-related deaths per 100,000 in America's black population, while there were 22.7 per 100,000 for America's white population.\nThough these deaths can be attributed in part to the way systemic racism makes black people less likely to have access to healthy food, green spaces, and livable wages, they could also be a result of the biases black people experience in healthcare.\nA review published in Academic Emergency Medicine compiled studies on implicit bias in healthcare professionals and found that a in a majority of studies, physicians showed an implicit preference for white patients.\nThe Centers for Disease Control and Prevention even released guidelines for healthcare professionals to ensure they are not allowing bias to influence their treatment of patients during the pandemic.\nBiases can also affect how medical professionals treat their colleagues, and minority medical professionals face microaggressions.\nMicroagressions are brief but common negative verbal or non-verbal behaviors against minorities.\nAn example Dr. Marcelin gave of a microagression is from a YouTube video in which an Asian woman is running on a trail in California, and another jogger stops her and asks her where she's from. The woman responds that she is from California, but the other person says, \"No, but where are you from, from?\" This assumes that just because the woman is of Asian heritage, that she cannot be from the United States.\nDr. Marcelin shared that she herself has experienced bias in the workplace — from both patients and colleagues — as an African American female doctor.\nDr. Marcelin, not pictured, said, \"There are many times that I have walked into patient situations and they have assumed that I am not the one that's in charge, even after I have identified myself as being the attending. I've had other healthcare professionals barging in on me in the middle of talking with patients and completely ignoring my presence"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194114, Requested 7214. Please try again in 398ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194114, Requested 7214. Please try again in 398ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Last week, I was shopping for groceries and preparing to hole up at home with my wife, Evelyn, and our two boys. There was an eerie, peculiar aura in the parking lot in upstate\nNew York as night fell and shoppers wheeled out essentials and snacks.\nThree middle-aged men in hoodies and sweatshirts stood outside the entrance of the grocery store. They huddled together talking. One looked up at me and frowned. There was\nsomething accusatory in his eyes. And then, for the first time in years, I felt it.\nI felt self-conscious - even a bit ashamed - of being Asian.\nIt had been years since I felt that way. I grew up with semi-regular visitations of that sense of racially tinged self- consciousness. It didn't help that I was an awkward kid. But\nafter adulthood, marriage, a career, parenthood, positions of leadership and even a presidential run, that feeling had disappeared - I thought.\nMy place in this country felt assured. I have it better than the vast majority of Americans of any background. When comedian Shane Gillis slurred me by name, I did not think\nhe deserved to lose his job. It barely registered when a teenager yelled \"Chink\" at me from the window of his car in New Hampshire a number of months ago. My only reaction\nwas to think, \"Well, I'm glad that neither of my sons was around because, then I might have to explain to them what that word means.\"\nBut things have changed.\nOver the past few weeks, the number of reported physical and verbal attacks on Asian Americans has increased dramatically. The percentage of Asians who use the not-forprofit Crisis Text Line to speak with a counselor has shot up from 5 percent of callers - about in line with our share of the population - to 13 percent, an increase of 160 percent.\nSome level of background disdain or alienation has grown into outright hostility and even aggression.\nAnd we all know why. The coronavirus is devastating communities and lives. People's livelihoods and families are being destroyed. And people are looking for someone to blame.\nBefore covid-19, too many Americans were already living paycheck to paycheck, working long hours just to get by. Now, we all are even more fearful for the future, worried\nabout our parents, grandparents and children. We are anxious about our jobs, bills and next month's rent or mortgage payment.\nIn early February, when I was still running for president, someone asked me, \"How do we keep the coronavirus from inciting hostility toward Asians in this country?\"\nI responded, \"The truth is that people are wired to make attributions based on appearance, including race. The best thing that could happen for Asians would be to get this virus\nunder control so it isn't a problem anymore. Then any racism would likely fade.\" This was weeks before \"Chinese virus\" became a thing.\nNow it is, and we have to figure out how to combat that, too. I'm an entrepreneur. In general, negative responses don't work. I obviously think that being racist is not a good\nthing. But saying \"Don't be racist toward Asians\" won't work.\nI have been thinking about ways to improve that encounter at the grocery store. People are hurting. They look up and see someone who is different from them, whom they\nwrongly associate with the upheaval of their way of life.\nNatalie Chou, a UCLA basketball player, said she felt better when she wore her UCLA gear, in part because the association reminded people that she was an American.\nDuring World War II, Japanese Americans volunteered for military duty at the highest possible levels to demonstrate that they were Americans. Now many in the Asian American\ncommunity are stepping up, trying to demonstrate that we can be part of the solution. Some 17 percent of U.S. doctors are Asian and rushing to the front lines.\nWe Asian Americans need to embrace and show our American-ness in ways we never have before. We need to step up, help our neighbors, donate gear, vote, wear red white\nand blue, volunteer, fund aid organizations, and do everything in our power to accelerate the end of this crisis. We should show without a shadow of a doubt that we are\nAmericans who will do our part for our country in this time of need.\nDemonstrate that we are part of the solution. We are not the virus, but we can be part of the cure."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197462, Requested 7833. Please try again in 1.588s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197462, Requested 7833. Please try again in 1.588s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Immigration to the US often suffers when the country faces a disaster, whether it is a disaster of war, economy or public health.\nThe novel coronavirus pandemic has already prompted the Trump administration to close borders and turn away asylum-seekers without sufficient processing. On Monday, President Donald Trump tweeted that he would suspend immigration to the US.\nThat statement was later clarified as a plan to temporarily halt giving foreigners permanent residence in the United States, which Trump claims will protect American workers during the coronavirus pandemic. The order, which is undergoing a legal review, would suspended the process to grant foreigners \"green cards\" for permanent residence, but a White House official who requested anonymity to discuss the process suggested the timeline for the order could be pushed back.\nCritics say the president's announcement is a move to take advantage of the coronavirus crisis to implement a long-sought policy goal ahead of this year's presidential election. Business groups expressed opposition to Trump's plan on Tuesday, arguing it would only further depress the economy.\nMany key details of Trump's planned executive order are still unknown, but crucially, while the order could block many people applying for permanent residence outside the United States, it is not clear how it would impact people already in the country seeking to become permanent residents. Trump said the order initially would last for 60 days and could be renewed for the same period or longer, and that a second immigration-focused order was under consideration.\nThe Washington-based Migration Policy Institute estimates that Trump's green card effort could prevent between 114,000 and 660,000 people securing permanent residence - if left in place for a year.\nErika Lee is an immigration history professor at the University of Minnesota and the author of \"America for Americans: A History of Xenophobia in the United States.\" The World's Marco Werman spoke with Lee about how the US has responded with changes to immigration policy and increased xenophobia during times of war, economic hardship and disease throughout history.\nRelated: For centuries, migrants have been said to pose public health risks. They don't.\nMarco Werman: Let's take a historical look at this. Your book, \"America for Americans\" is about the US' long history with xenophobia. You've seen the intersection of disease or other crises and nativism before in the past century. How does this current administration's response to this pandemic compare to another public health emergency, whether it's the 1918 flu or the response to the collapse of hygiene in New Orleans after Katrina?\nErika Lee: The 1918 flu pandemic is actually really fascinating to compare it to what's happening today. So first, we have to understand that, of course, during World War I, immigration was effectively halted due to the war and the end of passenger steamship travel. But even during the flu pandemic, in which the US lost 650,000 lives, the country didn't try to limit immigration. In fact, we still let in over 110,000 immigrants. And the Bureau of Immigration touted its kind treatment of sick immigrants.\nSo, take the immigration detention center in New Orleans, for example. There was an outbreak of flu epidemic there. There were 30 immigrants who were sick. But the Bureau of Immigration published a report in which it touted how it took care of its employees by requiring face masks, it sanitized the facilities, it put into place social distancing or isolation. And because of that treatment of immigrants, no lives were lost.\nRelated: Public charge rule has history of 'racial exclusion'\nErika, the stories right now of hate directed at Asians, Asian Americans - we're hearing stories of acid attacks. So personally, what has it's been like for you as\nsomeone whose grandfather arrived in this country during the Chinese Exclusion Act?\nYou know, I remember hearing stories of my parents insisting that during World War II, into the 1950s and '60s, they still felt the sting, not only of Chinese exclusion, but also just anti-Asian racism, in general. Obviously, Japanese American incarceration, as well. And their philosophy was, \"We need to show that we're Americans first and Chinese second.\" So for them, there was this sense of sort of ultra assimilation. \"We need to prove that we're loyal. We need to prove that we're patriots. We need to prove that we're assimilated.\"\nI think that the sense of worthiness, you know, is really being questioned for Chinese Americans and other Asian Americans, feeling like they're being suspected of bringing the virus and spreading the virus. And it takes its toll. And it's not just these horrific violent attacks, these physical attacks or the name calling or the social shunning. It's also just this internalized sense of, \"Oh, I thought we belonged. But look how easily the tables can be turned on us.\" And I think there's a palpable sense of fear now that we're all supposed to wear masks out in public. There is a racialized image of an Asian person in a mask that is quite different than any other type of person wearing a medical mask."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195801, Requested 6917. Please try again in 815ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195801, Requested 6917. Please try again in 815ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "found on social media once he began research for the paper. Like the others, he worried that the hostility already had spilled out in the offline\nworld.\n\"I always treat social media data as a good sensor of the real world,\" said Zhang.\nCharissa Cheah, a psychology professor at the University of Maryland at Baltimore County who is leading another study examining discrimination against\nChinese Americans during the pandemic, said anti-Chinese language such as \"kungflu\" and \"batsoup\" have their roots in historically racist tropes that have\nlong been used to denigrate Asian Americans and cast them as foreigners no matter how long they have lived in the United States.\n\"Covid-19 is merely refueling processes that are already there,\" said Cheah, whose research will also study Twitter activity related to the outbreak. \"I worry\nthat we might interpret these findings as, 'Oh, it's covid-19 that's causing it.' This is simply not true. The current pandemic is merely providing an outlet and\nmaking it easier to justify expressions of Sinophobia.\" Covid-19 is the disease caused by the novel coronavirus.\nAnother indicator of rising hostility toward China is the surging use of Twitter hashtags such as #BlameChina, #ChinaLiedPeopleDied, #ChinaVirus,\n#WuhanCoronaVirus and #ChinaCoronaVirus, according to an unpublished analysis done by Clemson University researcher Darren Linvill.\nThe hashtag #ChineseVirus surged to nearly 130,000 uses the day after Trump used it in a tweet. Linvill, an assistant professor of communication, said the\nuse was particularly prominent among accounts that regularly support Trump on Twitter.\nBut he cautioned that the character of anti-Chinese expressions varies sharply across social media platforms.\n\"Anti-Chinese sentiment is rising, and it manifests itself differently in different places,\" Linvill said. \"On 4chan, it's racist. On Twitter, it's political.\"\nTwitter spokeswoman Katie Rosborough said that the study by the international group of scholars, which like most academic research on Twitter relied on a 1\npercent sample of all tweets, may miss the \"nuanced political context around content that is posted on Twitter.\"\n\"We have zero-tolerance policies in place that address clear threats of violence, abuse and harassment, and hateful conduct,\" she said. \"If we identify accounts\nthat violate these rules, we'll take enforcement action.\"\nA week after Trump started publicly calling coronavirus the \"Chinese virus\" and defended his use of the label against critics who accused him of being racist,\nthe president implored people on Twitter to \"protect our Asian American community.\"\nThe president also stopped using \"Chinese virus\" on social media and during news briefings."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197259, Requested 6490. Please try again in 1.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197259, Requested 6490. Please try again in 1.124s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "NEW YORK — A spike in harassment of Asian-Americans since the coronavirus pandemic began has led community activists in the United States to fight back — forming street patrols, rallying on social media, and supporting each other online.\nAsians of varying national backgrounds have suffered a surge of attacks this year, which activists linked to the pandemic's emergence in China. Some said they feared harassment could worsen in a U.S. election year, with U.S.- China tensions ratcheting up over trade, Hong Kong and the coronavirus.\n\"When China is made the enemy, people who look like Chinese are the enemy. The economy is tanking, people are dying. They're angry and fearful and want to take it out on Asian Americans even more,\" said Russell Jeung, professor of Asian-American studies at San Francisco State University.\nSince March 19, over 1,800 cases of harassment related to the coronavirus pandemic have been reported to Stop AAPI Hate, a website Jeung created with two advocacy groups.\nNine out of 10 victims were targeted because of their race, with 37 per cent of incidents taking place in public areas, the organization said. Verbal harassment and shunning occurred in over 90 per cent of the cases. Victims were physically assaulted, or coughed or spat on, in some 15 per cent of cases.\nSome of the harassment has taken place inside of stores, Jeung said, adding that Stop AAPI Hate is working with retailers to figure out how to ensure safe access for customers.\nThe New York City Commission on Human Rights this week launched a multilingual campaign to combat COVID- 19-related discrimination after it received over 350 such complaints from Feb. 1 to May 15. Of these, 133, or 37 per cent, targeted Asians. That compared with just 11 complaints of discrimination targeted at Asians during the same period in 2019.\nThe FBI has warned of a rise in hate crimes against Asian- Americans since the coronavirus pandemic began, according to media reports citing internal documents.\nSOLIDARITY ON STREETS In some Chinatowns, volunteer patrols are forming to protect residents and confront harassers. The Guardian Angels, with patrols in over 130 cities worldwide, are recruiting in U.S. Asian communities for the first time in 41 years. Members include seniors and women, who are often targets of abuse.\n\"It's boring at home. I might as well do something,\" said laid-off dental assistant Sara Chin. The 46-year-old also patrols with New York's Chinatown Block Watch.\n\"Our mission is to observe, record and report,\" said Leanna Louie, a San Francisco businesswoman who started patrol group United Peace Corp. In eight weeks, it has filed 24 reports, including medical response incidents, auto burglaries, theft and shoplifting, leading to three arrests, she noted.\n\"We can break the old habits, old patterns and outdated stereotypes,\" Louie said. \"We have assisted in aiding the community to step out of the shadows and feel more empowered.\" The San Francisco Police Department has expanded its presence on foot and in cars around the city to act as a deterrent against such crimes, it said in a statement.\nIn New York, flyers and posters emblazoned with the words 'Don't be cruel' were created by Shirley Ng and her friends for display on Chinatown storefronts, with pointers on how to report a crime.\n\"I don't have to wait for support. It's my community,\" said Ng, an events assistant at the Asian American Legal Defense and Education Fund."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193465, Requested 8027. Please try again in 447ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193465, Requested 8027. Please try again in 447ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "SAN FRANCISCO – Mandy Rong was terrified her 12-year-old daughter had COVID-19. It was 2 a.m. and the young girl was hours into a fierce fever and a racking cough.\"Mommy, why are my eyes on fire?\" asked Amy Rong. The mother and daughter, along with Rong's parents, live in an 80-square-foot windowless single-room-occupancy Chinatown building that is a home of last resort for many impoverished Asian immigrants. Hallways are cramped; bathrooms and kitchens are communal. A ripe setting for the spread of the highly contagious novel coronavirus.That early March night felt endless. Rong, 42, repeatedly touched Amy's forehead, wondering if her child would die in the small loft that the two shared. Down below, her father slept on the floor while her mother took the lone sofa bed.In the morning, the fever had vanished, only to return a week later. Once again, the family endured a restless night.Getting tested didn't seem like an option for the Rongs. The rumor was that the tests were expensive. Rong also feared the reaction from neighbors. \"If you test positive, everyone would be scared of you,\" she said. \"Everyone would think you are the devil.\"It is easy to mistake San Francisco for a thriving Asian American haven. The city, which is its own county, boasts a bustling Chinatown, as well as a popular Japantown. Native Hawaiians, Pacific Islanders, Vietnamese, Indians and Filipinos also have made their homes here. All told, Asians in San Francisco represent upward of 20 countries.But many Asian American immigrants in the county lead a fragile existence rendered even more precarious with the arrival of COVID-19. So far, 38% of the 167 COVID-19 deaths reported by the San Francisco Department of Public Health are Asian American residents, the most of any ethnicity.Experts also are concerned that positivity rates among Asian Americans in San Francisco could be far higher than the 13.5% reported, a byproduct of the decades-in-the-making model minority myth, which characterizes this ethnic group as financially successful, physically healthy and upwardly mobile. This belief has caused segments of the Asian American community to long be overlooked when it comes to social services for housing, employment and health.San Francisco is one of the few places in the nation tracking data on Asian Americans and COVID-19 deaths at a time when officials don't know the ethnicity of the person affected in nearly half of the nation's 16 million coronavirus cases. About 17 million Americans are of Asian descent, or 5.6% of the population.In many cases, Asian Americans in this city have received imprecise or no information in their native language about testing, safety tips, housing, and other critical care services during the pandemic. At the same time, the community is struggling with inadequate access to comprehensive health care, the need to keep front-line employment, and growing incidents of anti-Asian hate crimes.\"This model minority thing, that's not us,\" said Judy Young of the Southeast Asian Development Center, a San Francisco nonprofit that helps area residents from Vietnam, Laos, and Cambodia.\"There is the language barrier, and our community is small,\" Young said. \"So the city doesn't think we have any problems, when we do.\"That risk of invisibility is only heightened by the pandemic. Since city health officials do not break down COVID-19 statistics beyond \"Asian American,\" many advocates for the city's various groups said they are left to speculate about coronavirus infection and death rates within their individual communities.\"There's this feeling that there's excess death out there,\" said Jeffrey Caballero of the nonprofit Association of Asian Pacific Community Health Organizations. \"That high mortality rate among Asian Americans means either there isn't enough testing or people are waiting far too long to get care.\"What is the model minority myth?For many Asian Americans in San Francisco, the high rate of COVID-19 deaths is directly linked to the corrosive and distorting effects of the model minority myth, said Dr. Tung Nguyen, a University of California, San Francisco professor of medicine.Nguyen co-wrote a report in May by the Asian American Research Center on Health that called attention to the fact that 50% of San Francisco's 31 COVID-19 deaths at that time were among Asian Americans, disproportionately high considering they make up just over a third of the population.To be sure, the fortunes and contributions of many Asian Americans have skyrocketed in past decades. The median annual income of households headed by the nation's 22 million Asian Americans is $73,060, compared with $53,600 for all U.S. households, according to the Pew Research Center.A closer look at San Francisco's two dozen Asian ethnicities reveals many groups within this broad categorization are struggling financially and remain outside the mainstream. About 43% are non-English speakers, according to a USA TODAY analysis of U.S. census data. About a third of San Franciscans are foreign-born, and 13% are not U.S. citizens.\"With Asian Americans, the average always is pulled way up by those doing very well, which means you miss the groups who clearly are not,\" said Margaret Simms, a nonresident fellow with The Urban Institute in Washington, D.C., who specializes in race and labor economics.Discrimination also is keeping some Asian Americans from getting tested for COVID-19. The website Stop AAPI Hate, the acronym for Asian American Pacific Islander, has logged more than 2,500 incidents of discrimination across the U.S. since mid-March, from verbal assaults to acts of physical violence.Decades of racist policiesChinese citizens began passing through San Francisco's then bridgeless Golden Gate en masse during the Gold Rush of 1849. By the late 1800s, the Chinese were not just vilified but outright barred from"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196294, Requested 8063. Please try again in 1.307s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196294, Requested 8063. Please try again in 1.307s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "The coronavirus pandemic has unleashed an explosion of anti-Asian racism in the United States, ranging from\nderogatory slurs, to blaming the outbreak on Chinese people, to violent physical attacks. In an open letter to\nschool leaders in Boston, Quincy, and Malden, a coalition of Asian-American students and their supporters is\nurging educators to take action now to ensure their safety as a new school year begins.“In school, we feel we are not equal to other students. Whether it be among our peers, with faculty members in-\nperson or online, it is suffocating to know there are those who look at us cautiously and choose to ignore therising issue of xenophobia in the U.S.,” the letter said. “Our mental health is deteriorating, and our grades will\ndrop, impacting our futures in higher education. We feel unsafe returning to schools with unaddressed racism.”\nAsian Americans have endured an onslaught of bullying and abuse in the months since the coronavirus outbreak\nerupted in China. Stop AAPI Hate, an initiative of several California-based Asian American civil rights\norganizations, has tracked more than 2,500 hate incidents against Asian Americans and Pacific Islanders\nbetween mid-March and early August, including reports of workplace discrimination, online harassment, and\nphysical assaults.\nStop AAPI Hate recorded 61 such incidents in Massachusetts.\nCivil rights groups, such as the Anti-Defamation League, have accused President Trump and public officials of exacerbating anti-Asian sentiment by referring to COVID-19 as “the Chinese virus” or “kung flu.”\nThe open letter, posted Sept. 21, has garnered more than 60 online signatures from students, teachers, parents,\nand community members in and around Boston. Seventeen-year-old Sarah Xu, one of the six students who\ncoauthored the letter, organized the campaign through the nonprofit Boston Chinatown Neighborhood Center.\nShe got the idea from a class at Boston Latin School, where she's now a senior, called “Facing History and\nOurselves.\" The class, Xu said, examines “the unspoken narratives of different oppressed populations.\" Shortly\nbefore Boston schools closed, the class began discussing the recent surge of anti-Asian prejudice. With help\nfrom the Boston Chinatown Neighborhood Center, she and the other students started drafting the letter this\nspring. They finished polishing it just in time for the new school year.\n“I was definitely afraid and anxious because this has never happened in my lifetime so far,\" Xu said in an\ninterview Monday, regarding the troubling rise in anti-Asian bigotry. “I'm thankful for the fact that I haven't\npersonally been attacked yet and I'm also in a community that keeps me safe.”\nBut in March, Xu still detected tension in her classes at Boston Latin, where roughly 30 percent of the students\nare of Asian descent.\n“It did feel uncomfortable at times when people would talk about COVID,\" she said. \"People couldn't help but\nlook at the Asians in the room.”\nIn the letter, the students described being stared at while walking down the street or riding the train. They\ncannot go to the grocery store, they wrote, “without fear of strangers harassing us.\"\n“For me, I'm Chinese, so I wore masks very early,” 17-year-old Xi Zheng, a senior at North Quincy High School\nand one of letter writers, told the Globe. She said she's afraid of leaving her home and “being attacked by a\nracist.”\n“People look at me very strangely,\" she continued, “and I feel very uncomfortable.”\nThe students asked school leaders “to stand in solidarity with communities of color” by publicly speaking out\nagainst racism; hiring more diverse faculty and staff; hosting monthly anti-bias trainings; and teaching Ethnic\nStudies. They also requested a system for “reporting incidents of racism,\" and listening sessions and healing\ncircles for students and families.\nLetter writer Mandy Sun, who is also 17 and a senior at Boston Latin, said racism against Asian Americans is\n“normalized,\" which she hadn't fully realized until the pandemic began. She recalled an incident in the spring,\nwhen a white student at a nearby suburban high school wrote a scathing Facebook post blaming the virus on\nChinese people. Despite the student's outburst, her school didn't reprimand her, Sun said, because the student\nlater apologized on social media.\n“It made me feel, at first, angry, and I started thinking, 'How are we going to move on from this? How are we\ngoing to create a better society?' I feel like the only people who got outraged were in the Asian-American\ncommunity,\" Sun said. The episode left her feeling “slightly hopeless.”\nBoston Public Schools spokesman Jonathan Palumbo said the district is reviewing the letter and working on a\nresponse to the students. In an interview with the Globe on Tuesday, Malden Public Schools superintendent John\nOteri reiterated the district's commitment to anti-racism teaching and training.\n“I think when you're doing anti-racism, anti-bias [work], you're trying to undo generations and, frankly,\ncenturies of systemic oppression. It's going to take awhile and it's always going to be a work in progress,” he\nsaid. “I want to make Malden the most equitable, welcoming, inclusive school community, where people feel like\nthey belong.”\nThe superintendent of Quincy Public Schools, Kevin Mulvey, did not return requests for comment.\n“Young people are very much concerned about these issues and very much want to use their voice to make\nchange, so this was a way for them to do that,” said Ben Hires, CEO of the Boston Chinatown Neighborhood\nCenter, who signed onto the letter as a supporter. “To me, it's just inspiring to see these young people use\ntheir voices"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193005, Requested 7858. Please try again in 258ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193005, Requested 7858. Please try again in 258ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "The Others. Them.\nThe very words sound sinister. No wonder they became the titles of horror movies.\nBut they're what we focus on, in a time of fear. Prejudice could be the one thing that spreads more easily than COVID-19.\n\"In an epidemic, a lot of feelings get very primitive,\" said psychologist and psychoanalyst Spyros D. Orfanos. \"People get very fearful and xenophobic. They need to externalize the blame. They need to find somebody who is at fault for this.\"\nGroups that can be \"blamed\" for the epidemic — Asians in particular — have been reporting an uptick of bias incidents in the last two months. Groups that have been associated with a particular outbreak, such as Orthodox Jews, have reported the same.\nEven the best of us — as we heroically come together to fight a common enemy — are liable to regard our fellow beings with just a wee bit of suspicion.\nWe worry about The Other — the neighbor we cross the street to avoid, the person in the store we back away from.\nAnd we worry, especially, about people seen as apart from the community — the ones whose ways are unfamiliar, and whose motives are suspect. Them.\n\"I think in general people are either consciously or unconsciously biased towards other races,\" said Orfanos, director of New York University's postdoctoral program in psychotherapy and psychoanalysis. \"And whenever something unexpected or fearful happens, a threat that is unprecedented, it interferes with our thinking.\"\nUnlike the Others of \"The Others\" (2001) — ghosts — and the Them of \"Them!\" (1954) — giant ants — our other-ing of others right now has a superficial basis in truth. There is, in fact, a disease called COVID-19 that some of us are carrying.\nIt did originate in a particular country — as all diseases do — though that's pretty much academic, since it's now worldwide. And there have been cases of localized outbreaks connected with some particular groups (and many other outbreaks not connected with them).\nBut it's telling that the bias incidents tend to be aimed mostly at populations that are already stigmatized. Asians and Jews, in particular.\nThere's been an incident\nIncidents ranging from verbal harassment (67.3%) to physical assault (10%) to being coughed and spat on (2.7%) to being banned from public transportation (1.0%) have been reported by the Asian Pacific Policy and Planning Council, which on April 3 cited over 1,100 reported incidents of anti-Asian bias nationally, clocked over a two-week period.\nOne such incident was recounted in a Los Angeles Times editorial: \"A white man on open sidewalk approached and stepped directly in front of me and coughed in extremely exaggerated manner in my face — loudly, mouth wide open, about 2 feet from my face, said 'take my virus.'\"\nThere are other such reports. The Asian woman who was punched on West 34th Street by another woman who made anti-Asian slurs (March 10). The man in the New York subway who sprayed air freshener at an Asian passenger (Feb. 6). The Asian student attacked in London by four men who told him, \"We don't want your coronavirus in our country\" (Feb. 24).\nMembers of the Orthodox Jewish community also started to worry, when an outbreak originating in a New Rochelle synagogue led in March to a \"containment area\" around the town.\n\"We're seeing stuff online,\" Evan Bernstein, vice president of the Anti-Defamation League's Northeast Division, told the The Journal News and lohud.com on March 4. \"We're getting more and more reports of those comments.\"\nIn Lakewood, tensions have flared between Orthodox Jews — there have been a few cases of weddings held, in violation of rules banning large gatherings — and the rest of the community. A local petition to shut Lakewood down was condemned by Gov. Phil Murphy.\n\"Scapegoating, bullying, or vilification of any community is completely unacceptable — today or ever,\" Murphy tweeted. \"There is a special place in hell for the small minority that do this during this crisis.\"\nA surge in anti-Semitic gibes, cartoons and conspiracy theories online, since the COVID crisis hit, has also been reported on the ADL website.\nThere seem to be two lines of thought, mutually exclusive: that Jews are more likely to have the disease, and that Jews are masterminds who somehow created it, for their own sinister reasons. A Wisconsin white supremacist named \"Uncle\" Paul Nehlen has floated the idea that COVID is a Jewish bioweapon, created in Israel, for the purpose of controlling China. \"You gonna let these jealous, vindictive jews get away with it?\" he posted on the messaging platform Telegram in January.\n\"I feel like I'm living in the middle ages,\" said American studies professor Michael Aaron Rockland of Rutgers. \"It's the mystery of this thing that has people scared.\"\nCoronabias\nThere are also milder forms of bias. The people, for instance, who avoided Chinese restaurants — back when you could still go to restaurants — for what they would say are rational reasons. After all, COVID-19 did originate in China, right? And you don't really know where the waiter might be from, right?\nActually there's no more reason to think that a person of Chinese ancestry had just visited Wuhan than a person of Swedish ancestry had just visited Stockholm. Still, business was down 70% in New York's Chinatown, the trade magazine Restaurant Hospitality reported in March.\nBut no one reported a comparable downtick in Italian restaurants. Not even in late March, when Italy's death toll surpassed China's, with as many as 250 people dying in a day. No one worried about whether Mario, at the pizzeria"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194850, Requested 7477. Please try again in 698ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194850, Requested 7477. Please try again in 698ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Vancouver journalist who writes about civic affairs Among the innumerable sorrows the coronavirus pandemic has heaped upon us, the surge of racist attacks against people of Asian descent is one of the most despicable.\nCanada, which fancies itself a country of people from diverse ethnicities who all get along, has not been immune.\nWhen people are fearful - and who hasn't been lately - they seek to blame. Right now, the scapegoat is China, where COVID-19 first struck, and by extension anyone who looks Asian.\nTake the recent attack by a white man on a Vancouver bus whose anger was fuelled by two Asian women wearing masks. Police say he yelled, \"Go back to your country. That's where it all started.\" When another woman spoke out against the remarks, he kicked her and yanked her hair so hard a clump of it fell out.\nIn Toronto an Asian nurse, also wearing a mask, reported she was yelled at, spit on and smacked with an umbrella by a white woman who yelled at her to go back to where she came from.\nThe sentiment that China and its people are responsible for the spread of the COVID-19 coronavirus is being perpetuated by people in the highest levels of government in the United States, including President Donald Trump himself.\nThe President's remarks are certainly egging on those looking to place blame. But to say Mr. Trump is responsible for racist displays here in Canada is to deny our own history.\nRacism underpinned policies prohibiting Chinese labourers from bringing their families to Canada when they came to build the railways. It also drove the decision to intern Japanese Canadians during the Second World War. More recently, it led to nasty rhetoric against wealthy Chinese investors who were encouraged by developers and politicians to buy homes and businesses here and then blamed for the exponential rise in property values.\nScratch the surface, and it seems Canada's \"Yellow Peril\" ideologies are still very alive.\nCOVID-19's origin in China isn't in and of itself racist. And to keep the virus in check, scientists must determine its origin (likely bats) and learn how it jumped to some other animal that people touch or eat. If, as most scientists believe, the virus originated in a Wuhan market where many types of live animals were sold, there will need to be changes.\n    The Chinese government has already made it illegal to sell or eat wild animals and further restrictions and hygiene improvements could be in order.\nBut while it is totally fair to scrutinize China's food chain in relation to health, blaming Chinese people for spreading a deadly virus is not. This week singer Bryan Adams, who is vegan, unleashed a profane Twitter rant against \"bat-eating, wet-marketanimal selling, virus-making greedy bastards.\" After he was excoriated for what sounded to many like a racist diatribe, he issued a lame apology and claimed he was only taking a stand against animal cruelty.\nBefore anyone gets too highminded about Western food choices and animal husbandry, let's not forget what led to the outbreak of mad cow disease (Bovine spongiform encephalopathy), which infected British cows in the 1980s and 1990s. The root cause was the practice of feeding cattle a \"protein\" mixture made from other cattle who died from the disease and sheep infected with a related condition called scrapie. When the disease jumped to humans, it killed 177 people.\nThere was no racist backlash against the British, at least not that I recall. The outbreak was widely regarded as a misfortune.\nAfter news broke about the latest racist attack aboard a Vancouver bus, North Vancouver MLA Bowinn Ma, whose parents immigrated to Canada from Taiwan, posted a heartfelt message on Twitter articulating her thoughts on anti-Asian racism, which she points out exists worldwide.\nThe problem with high-profile people such as Mr. Trump and Mr. Adams playing the blame game is it emboldens others to \"embrace their biases, their prejudices and ignorant ideas about other people as though they are righteous,\" Ms. Ma says. From there it's a short line to hate crimes.\nWe are a tolerant society, reluctant to fall into American style tribal politics. But, now more than ever, we all must call out racism where we see it because complacency allows it to grow."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199637, Requested 6938. Please try again in 1.972s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 199637, Requested 6938. Please try again in 1.972s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "nearby employees and residents, including many who are African\nAmerican, Hispanic, Southeast Asian, and Chinese students.\n\"Who knows when this pandemic will end,\" Zhang said. \"Luckily, people are much more cautious now. My customers are very good at\nwearing masks and social distancing. We'll just have to wait and see when God guides us back to a normal life.\"\n\"Are you a Christian?\" I asked, surprised at this discovery. Religion is discouraged in mainland China.\n\"Yes, I joined my husband's church when I first arrived. After taking some Bible-study classes, I felt, well, it's better to have faith.\"\nWhile I was talking to her, a white customer came over to order bubble tea. Zhang struggled with her English. I offered to help. The\ncustomer explained patiently. We all leaned closer while standing apart, trying to understand each other through layers of accents,\nmasks, and a plastic shield in front of the cashier.\nStruggling to fit in -- and survive\nOutside the food court and to the left, I saw racks of traditional Chinese qipao in bright colors standing in the middle of the corridor,\ndirecting my attention to Ms. Ma Custom Tailor. The shop has richly embroidered traditional Chinese gowns covering the walls andboldly designed robes, skirts, coats and pants hanging on the racks. Baby outfits and hats made from dyed silk are particularly eye-\ncatching. Yet no customer was browsing when I visited recently.\"It's very difficult to do business in America. I worked extremely hard over the past four years and earned a good reputation,\" the\nowner, Ma, said.\n\"But all the money I made has gone to paying taxes. In America, all you do is pay taxes! And now, the virus.\" Ma shook her head\nwhile sewing a yellow summer dress, squinting her bleary eyes.\nBefore coming to America, Ma was a well-known fashion designer in China with 40 years of experience. To support her son as a single\nmother, she had brought 70 yuan (about $10) to start her career in Shenyang, later running a successful clothing factory and two\nstorefronts.\n\"For over 20 years, I didn't need to touch the sewing machine,\" Ma said with pride mixed with remorse. \"My three dozen workers\npedaled for me. But in America, I can't afford to hire anyone. I have to do everything myself, starting from scratch.\" She cut the white\nthreads.\nIn 2016, Ma came to Atlanta for a fashion design competition and stayed on an O-1 Visa (a nonimmigrant visa for individuals with\nextraordinary ability or achievement). Her business back home was subsequently closed. Her family members remain in China.\n\"There was no other tailor in Atlanta who could design a decent traditional Chinese gown. I wanted to start a trend,\" Ma laughed,\npressing her knotted fingers on the yellow dress. When she first arrived, she could not speak any English or drive. When she finally\ngot behind the wheel, she could not see the signs well and often lost her way.\nFour years later, Ma still cannot speak much English and drives with great caution. A thicket of linguistic, cultural and legal brambles\nstands between her and her idea of America, misunderstandings being the daily casualties of her American dream.\nI couldn't help but arrive at the conclusion that Ma would be much better off in China. Though earlier immigrants fled famines, wars\nand political disasters, today's China provides unprecedented comfort and stability -- if one avoids politics.\nMa seemed to have guessed what I was thinking. \"I know, life is so much better in China. I miss my son and my grandson every\nday. But I can't go back like this. I need to set a good example for my kids.\"\nPlus, in the time of COVID-19 and U.S.-China dissension, it is increasingly difficult to secure a plane ticket, get customs clearance,\nand go through the isolating quarantine periods.\n\"I'm only in my 60s; I can still work! I'll be terribly bored if I go back to live with my son,\" Ma reassured herself.\nAs I soon found out, Ma is not alone in America anymore. This past February, she got married. Her husband Ronald is a retired\npolice officer and African American pastor. She made the wedding gown herself, white with lace.\nLanguage remains a barrier between the husband and wife. When talking to her husband, Ma uses a translation app, which often\nmakes mistakes.\n\"He often looks at me with tears in his eyes, searching for any sign of misunderstanding,\" Ma said with a shy smile. \"He keeps\ntelling me to say more, ask questions, and double-check the translation.\"\nShe often reminds herself, her Chinese family and her American friends that she and Ronald vowed to stay together for better or\nworse, rich or poor.\nAs I stood to leave, a well-dressed man with a sweet smile and a strong build showed up. He was Ronald. I beamed. \"Your wife is a\nforce of nature,\" I said.\n\"Yes, she is a remarkable woman,\" Ronald said, eyes gleaming. \"I feel very blessed every day. Last year, I prayed to God for a\nwoman in my life. And you know what? A few months later, I met her.\"\n\"Did you come to pick her up today?\"\n\"Every single day.\" His smile widened. \"It's my duty to escort my lady home.\"\n\"I bet you are a good driver!\" And we laughed.\nCREDIT: For the AJC, Staff\nCopyright CMG Corporate Services, Inc. on behalf of itself and the Newspapers Aug 30, 2020"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198430, Requested 7948. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198430, Requested 7948. Please try again in 1.913s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Hurt. Angry. Unsafe.\nThat's how Jasmine Yuan says she felt in March when a stranger in a car yelled “corona!” at her while driving by\nin a grocery store parking lot in North Austin.\nYuan, 39, said she loves Austin and considers it a diverse, multicultural city, but lamented that she now fears\nportions of town that used to be part of her everyday life. After the incident, Yuan said she has avoided public\nplaces that she feels might put her at risk of being targeted again.\n“I feel scared and intimidated. I just don’t want to have this experience happen again,” Yuan said.\nRacist verbal and physical attacks have been reported at growing rates around the country since March,\naccording to people who have started tracking the trend nationally.\nAnd local Asian American community leaders like Hugh Li, president of the education and philanthropy group\nAustin Chinese American Network, fear anti-Asian attitudes tied to the coronavirus pandemic could stick around\nlong after the pandemic and worsen due to waning U.S.-China relations and economic hardship.\n“I think this will naturally put Asian Americans, and particularly Chinese Americans, in a really bad bind,” he\nsaid.\nLi said his organization\nrecently circulated a survey asking its hundreds of members to report incidents of racism they have witnessed\nor experienced during the pandemic, and it has so far have received two reports.\nYuan, who moved to the U.S. 13 years ago and has lived in Austin with her husband and two children since\n2011, responded to the survey. She said she did not immediately report her experience at the grocery store,\nhoping it had been an isolated incident.\n“But if more and more of this happens, this becomes a trend,” Yuan said. “That makes me think that I need to\nstand up and talk about this.”\nAustin resident Jae Kwon, 35, who is Korean American, said he was the target of several aggressions that he\nsuspects were motivated by race.\nKwon, a University of Texas sociology doctoral student who has lived in Austin since 2015, said something was\nthrown at him, a woman screamed something inaudible from a car, and a stranger yelled obscenities at him\nduring a six-day span in May while he was running for exercise. On another occasion, Kwon said a person\nsprayed him with a water gun while driving by.\n“It’s unusual in the sense that things like this had not happened other times,” said Kwon, who also served 12\nyears in the U.S. Navy. “I can only think that these are unrelated incidents, but it’s related in the sense that it\nhappened in this time period.”\nG. Michael Pendon, 43, a local DJ, reported that a group of men harassed him using anti-Chinese and\nhomophobic slurs while he was riding a bicycle through a neighborhood near the UT campus July 18.\nPendon, whose parents came to the U.S. from the Philippines in the 1960s, said he could not sleep that night.\n“I was pissed off. I was tired. I felt like I needed to go down there. I was kind of obsessed about trying to figure\nthis out,” he said.\nAustin police have confirmed that they are investigating the matter. Pendon said he has also spoken to the UT\nPolice Department.\n\"I just want to identify these perpetrators,\" Pendon said.\nResurfacing biases\nExperts in the field of Asian American history say racial violence and tensions during events like a pandemic are\nnot a new phenomenon.\nRussell Jeung, chair of Asian American studies at San Francisco State University, said the pandemic has\nreignited old, yet lingering anti-Asian sentiments in the West.\n“There is a historical stereotype on Asians — the yellow peril — that they are a threat that would come dominate\nthe West with their hordes and their diseases,” Jeung said.\nJeung helped start Stop AAPI Hate, an online system in which people across the country can report racially\ncharged incidents against Asian Americans through a form translated into more than 10 languages.\n“We launched it, and immediately we got flooded with responses, and using the data we’ve been sharing it\nwith local jurisdictions,” Jeung said.\nJeung said that, from March 19 through July 1, the organization received more than 2,120 reports of racially\nmotivated attacks against Asian Americans from across the country.\nIn more than 500 cases, Jeung said people reported being told to go back to China. One respondent from\nAustin said they were told to go back to their country by a person who used a racial epithet to describe the\ncoronavirus.\nA relatively small number of cases — 63 incidents — were from Texas, but reports came from every major\nTexas city. About 22% of those reports involved physical attacks, more than double the national rate, Jeung\nsaid. About 63% of reported attacks in Texas were verbal.\nPeople of Chinese descent are not the only segment of the Asian American population targeted since the\npandemic started. On March 14, three people of Burmese descent, including 2- and 6-year-old children, were\nattacked in the West Texas city of Midland by a man with a knife.\nJeung said Trump's rhetoric on China makes Asian Americans more vulnerable to racist attacks.\nAccording to Factba.se, an independent online database of words from Trump’s speeches, media interviews\nand tweets, the president used the term “China Virus” or “Chinese Virus” a total of eight times between\nMarch and April, not including instances in which he blamed China for the pandemic without using those\nterms.\nIn July alone, the database showed that Trump used different variations of the phrase at least 32 times.\n\"Trump makes the connection that the virus"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 150, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196633, Requested 7152. Please try again in 1.135s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196633, Requested 7152. Please try again in 1.135s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Three days after Orange County's top health official resigns — she'd received threats for her face mask requirement — there's a change of heart. And bills on reparations and affirmative action move through the legislature. Plus: Stick around for my convo with a leading unemployment expert about ways to help the hardest-hit workers.\nIt's Arlene with news to know about this Thursday.\nBut first, we fact-check the claim that 449,000 Californians who were excused from jury duty because they are not citizens can vote. They cannot. But there's more to this falsity that pops up again and again.\nIn California brings you top stories and commentary from across the USA TODAY Network and beyond. Get it free, straight to your inbox.\nO.C. lifts mask order after threats to public health official, who resigned\nDays after his predecessor abruptly quit, Orange County's new interim health officer lifted a requirement that residents wear face coverings in public to help slow the spread of the coronavirus.\nThe move comes three days after Dr. Nichole Quick quit as the county's health officer following threats at a public meeting and on social media over her order that residents wear masks in public when near others. Residents have blasted the requirement at public meetings, and the sheriff said he wouldn’t enforce it.\nGolden State news straight to your inbox. Sign up for our California newsletter today.\nDr. Clayton Chau, the interim health officer, said the change was to make the county rules consistent with those of the state.\n“This decision is not because of public pushback,” he said.\nNew Floyd art, media moves, cop shows and is there really an urban/rural divide?\nPalm Springs commissions a mural of George Floyd, a black man killed in Minneapolis by a white police officer, for its downtown. Artist MisterAlek's work includes \"I can't breathe\" and a painting of Floyd.\nAudrey Cooper, the youngest woman ever to lead a major daily newspaper as editor of the San Francisco Chronicle in 2015, has been hired to lead WNYC's public radio operations.\nI'm urban, you're rural, but we're not really so different (Opinion).\nReparations, and ending a ban on affirmative action?\nTwo legislative proposals designed to help eradicate structural inequality are making their way through the state legislature, fueled in part by large demonstrations in response to the death of George Floyd.\nA proposal to establish a task force to study and prepare recommendations for how to give reparations to African Americans passed the California Assembly on Thursday.\n“We seem to recognize that justice requires that those who have been treated unjustly need the means to make themselves whole again,” said Assemblywoman Shirley Weber, a Democrat from San Diego who wrote the bill.\nIf the bill passes the Senate and is signed into law by Gov. Gavin Newsom, eight people with backgrounds in racial justice reforms would lead a study into who would be eligible for compensation and how it should be awarded.\nA day earlier, the Assembly passed a proposal to erase the state's 24-year ban on affirmative action. If passed by the Senate and approved by voters, it would strike from California’s Constitution the rules imposed by Proposition 209, which in 1996 prohibited government agencies and institutions from considering race, ethnicity or gender.\nAffirmative action policies were initially put into place to address historical and present-day discrimination.\nThe two measures are among the top priorities for California's Legislative Black Caucus, which Weber heads.\nWhat else we're talking about\nA man suspected of ambushing and seriously injuring a deputy, and killing a man near a train station on Wednesday, was killed Thursday in a shootout with police in Paso Robles. The ambush triggered a massive manhunt for the Visalia resident.\nState officials eye the Porterville Developmental Center as a possible treatment site for inmates hit by massive coronavirus outbreaks at crowded Kings County prisons. The Army Corps of Engineers in April spent $876,000 converting it into a hospital for a possible surge of coronavirus patients, but it's sat empty since.\nThe first incarcerated woman has died from what appears to be coronavirus complications. If confirmed, she'd be the 14th state inmate death linked to the virus.\nAn Asian American exercising at a Torrance Park was the target of a lengthy, racist tirade captured on video: \"This is not your place. This is not your home.\"\nThe San Francisco police union tells the city's transportation agency in a Tweet to \"lose our number\" next time it has problems with passengers. The agency previously had said it would no longer transport police to anti-police brutality protests.\nUnemployment claims are down, but is that a good thing?\nThe news on the surface sounds good: Unemployment claims are going down and/or being denied because more people are returning to work and earning more money.\nThen you dig deeper.\nThere, you'll see that going back to work, for many people, means taking home less than they did when they weren't working, because of the additional $600 in federal unemployment they would miss out on. And the situation is hurting already disadvantaged workers the most.\nThose are some findings from UCLA's California Policy Lab's unemployment analysis released Thursday. I spoke via email to co-author Till von Wachter, the lab's faculty director and a UCLA economics professor, more about their findings and what could help.\nOur conversation has been lightly edited.\nQ: Let’s start with some good news. What about the numbers cheer you?\nA: The fact that we see an increasing number of Californians whose UI (Unemployment Insurance) payments are being either reduced or denied altogether because they’re working is a good sign for the economy as we slowly re-open. That being said, going back to work also creates a challenging dilemma for some workers: If you’re going back to work on a reduced schedule (with reduced pay), this part-time work can result in your UI benefits being either denied or reduced.\nAnd, of course, there"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196356, Requested 8155. Please try again in 1.353s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196356, Requested 8155. Please try again in 1.353s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "ms, a nonresident fellow with The Urban Institute in Washington, D.C., who specializes in race and labor economics. The think tank found nearly 13% of Asian American senior citizens live in poverty compared to a 9% national average.Discrimination also is keeping some Asian Americans from getting tested for COVID-19. The website Stop AAPI Hate, the acronym for Asian American Pacific Islander, has logged more than 2,500 incidents of discrimination across the U.S. since mid-March. The attacks have ranged from verbal assaults to acts of physical violence.When Asian Americans hear President Donald Trump, who contracted COVID-19 in October, repeatedly call the virus the \"China virus\" and \"Kung Flu,\" \"it makes them less likely to seek help, a bit like early in the AIDS epidemic when the gay community was stigmatized,\" said Karthick Ramakrishnan, professor of public policy at the University of California, Riverside and chair of the California Commission on Asian and Pacific Islander American Affairs.Decades of racist policiesChinese citizens began passing through San Francisco's then bridgeless Golden Gate en masse during the Gold Rush of 1849.By the late 1800s, the Chinese were not just vilified but outright barred from entering the country, with few exceptions, by the Chinese Exclusion Act of 1882. White officials charged they were taking jobs from other Americans, despite having been integral to the Gold Rush's boom and the construction of the Transcontinental Railroad.At the height of World War II, Japanese Americans around the country were rounded up and sent to internment camps, feared as the traitorous \"yellow peril\" after years of citizenship. Despite painful and humiliating treatment at the hands of the U.S. government, many Asians resolved to engrain themselves in the society at large with an image of themselves as patriotic, hardworking Americans.The model minority image gained momentum during the civil rights movement of the 1960s. Asian American success stories were highlighted by white U.S. officials both as a way of signaling to other nations, namely the Soviet Union, that America was not racist, but also to shame other ethnic groups, notably Black Americans.The logic went that if Asian Americans were doing so well, surely failure on the part of other ethnic groups was their own fault.Then came the Vietnam War, a quagmire that resulted in a U.S.-sponsored evacuation of 125,000 refugees followed by countless others who escaped Southeast Asia in rickety boats. Many landed in San Francisco.California Assemblymember David Chiu, a Democrat who represents the eastern half of San Francisco and chairs the California Asian & Pacific Islander Legislative Caucus, said lawmakers must recognize that Asian Americans are a loosely linked group of immigrants with distinct challenges and needs.\"The attention being paid to the disparities endured during the pandemic by Black and Latinos is important, but our issue hasn't gotten the attention it deserves,\" he said.One small demographic victory for Asian Americans came in 1997 when President Bill Clinton directed the Office of Management and Budget to expand its data classification system to break out \"Native Hawaiian or Other Pacific Islanders\" from the Asian American group. That geographic list includes countries such as Micronesia, Tonga, Vanuatu, Guam, the Marshall Islands and Fiji.As a result, we know today that Pacific Islanders rank third in terms of COVID-19 deaths, behind Native Americans and Black Americans.A COVID-19 language barrierAsian American communities in San Francisco speak a range of languages including Mandarin, Cantonese, Japanese, Korean, Tagalog, Laotian, Samoan, Tongan, Vietnamese and Hindi. The city's website notes that COVID-19 information is available in English, Chinese, Filipino and Spanish.Efforts by city health officials to inform Asian residents about COVID-19 safety precautions and testing in their native languages have sometimes resulted in confusing or alienating translations.For example, information about pop-up virus testing sites sometimes can come across as demands, while in other cases the language is just plain confusing.One flyer written in the Filipino language of Tagalog told people to \"cover their entire face,\" said Luisa Antonio, executive director of the Bayanihan Equity Center, a Filipino American support group.Department of Public Health officials declined an interview request about outreach efforts.In California, about 5 million of 40 million state residents are Asian American, and in three-quarters of those homes, languages other than English are spoken regularly, according to the U.S. Census.Even some Asian Americans who speak fluent English said government officials have not made it easy to get information about the virus.Huiting \"Rita\" Huang grew alarmed when her mother-in-law told her that there had been a positive coronavirus case among the Chinese emigres to whom she was providing nursing services. The mother-in-law was unsure what to do and feared her poor English would make getting information about where to get tested even harder.Huang felt confident she could help. Her English was solid and she had experience getting COVID-19 information as a project coordinator and health educator for the nonprofit NICOS Chinese Health Coalition.After pursuing a series of online testing-site leads through a variety of city- and community-run websites – all requiring fluency in English – Huang soon learned that there were no available appointments at testing facilities close to their neighborhood.Huang eventually found a city-run testing site near Pier 30 along San Francisco Bay. The test was negative.\"That was frustrating for me, and I speak English,\" said Huang. \"I can't imagine what it would be like for someone like my mother-in-law.\"Some refuse to get testedAsian activists and health care workers trying to fill the void said they face a population that often is wary of Western medicine, fatalistic about getting the virus, culturally averse to passing along bad news to elders and nervous about losing employment.Kent Woo, executive director of the NICOS Chinese Health Coalition, said residents sometimes are suspicious of health care workers when they visit"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193344, Requested 7871. Please try again in 364ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193344, Requested 7871. Please try again in 364ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Activists are encouraging people to post photos of themselves wearing masks on social media to show solidarity with victims who have been attacked in hate crimes. Congress and activists have cited racist hate crimes against Asian-Americans who were wearing masks to protect them from COVID-19.\nThe CDC last week reversed course on government guidance about wearing masks during the pandemic by urging Americans to wear them.\n\"I used to refuse wearing masks when I was younger to avoid being made fun of,\" says Michelle Hanabusa, founder of UPRISERS, a Los Angeles-based clothing brand, which\nsells masks emblazoned with the hashtag on her website. The hashtag \"is meant to combat any of these stigmas.\"\nActivists against COVID-19-related hate crimes are leading a social media campaign using images of people in masks to fight back against attacks on Asian-Americans, which Congress and the FBI say are increasing.\nLast week the Centers for Disease Control asked Americans to wear non-medical masks in public, which the activists see as an opportunity to spread their message that racist attacks related to the virus must stop. The US surgeon general previously urged Americans not to buy or wear masks. Activists say masks have become a powerful symbol of the controversy and conflicts involving the virus.\nThe activists are encouraging people to post photos of themselves in masks with the hashtag #HateIsAVirus to illustrate that wearing masks – and the virus itself – are in no way specific to any race, region, or nation.\n—HATEISAVIRUS (@hateisavirus_) March 29, 2020\nOther hashtags, including #WashTheHate, #RacismIsAVirus, #IAmNotCOVID19, have protested hate crimes and hate speech, and condemned President Donald Trump's use of the phrase \"Chinese virus.\" The president said last week that while the CDC is urging people to wear masks, he chooses not to.\nMichelle Hanabusa, founder of UPRISERS, a Los Angeles-based clothing brand, sells masks emblazoned with the hashtag on her website. \"I used to refuse wearing masks when I was younger to avoid being made fun of,\" she says. The hashtag \"is meant to combat any of these stigmas.\"\nIndeed, it appears that the mask has become something of a symbol.\n\"From the data available in public reporting, as well as ongoing studies and online conversation that we've been aggregating on racismiscontagious.com, it is clear that the mask has been a polarizing object,\" says Selina Guo, a New York advertising director at the firm ADMERASIA, which runs the website in collaboration with several non-profits. Masks were the top target sparking attacks after race and ethnicity, according to data on the site. \"We are anxious to see if this announcement ultimately flattens or exacerbates these instances.\"\nRacist attacks on the rise\nLast week the FBI warned law enforcement across the country that hate crimes against Asians may increase due to the virus, according to a bureau report obtained by ABC News, \"The FBI makes this assessment based on the assumption that a portion of the US public will associate COVID-19 with China and Asian American populations,\" ABC quoted the report as saying.\nTwo-dozen federal lawmakers from the Congressional Asian Pacific American Caucus (CAPAC), wrote a letter urging Congress \"to help us prevent hysteria, ignorant attacks, and racist assaults that have been fueled by misinformation pertaining to the 2019 novel coronavirus.\" The letter cited an attack in February, on an Asian woman wearing a protective mask who was kicked and punched in a New York subway station.\nRacist attacks on Asian-Americans have surged to about 100 per day, according to Democratic Rep. Judy Chu of California, leader of the caucus, who said on MSNBC last week that some victims were \"assaulted just for wearing a face mask.\"\n  Organizers of the social media campaign aimed at stopping the attacks say the recent news events related to masks and attacks are a chance to communicate their cause.\n\"Masks have been a topic of huge debate. A lot of us have families in Asia, and our parents were worried that we weren't wearing masks. But then on the other hand they saw news reports about racist attacks on people wearing masks. The government urged people not to wear masks, and now it wants people to wear non-medical masks. That controversy shows what a powerful symbol they are,\" says Tammy Cho, cofounder of Executive Alerts, a social media startup acquired by the Meltwater public relations firm in 2015. Cho also cofounded BetterBrave, a nonprofit that gave tools and resources to employees who experience sexual harassment.\nCho and other leaders of the movement encourage people to post the hashtag on Instagram photos of themselves in masks to raise awareness of hate crimes, to discuss their own experiences with COVID-19 and racism, to share resources and links, and to support people who have been targeted in attacks.\nBryan Pham, who leads the 44,000-member Asian Hustle Network private group on Facebook, says the campaign \"aims to increase awareness against racism and xenophobia while normalizing the need for Americans to wear masks to protect themselves from COVID-19.\"\nOrganizers estimate the hashtag has reached more than 4 million, as it has been adopted by influencers including the 1.6 million-follower Instagram account of the magazine Elle Korea."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198559, Requested 7898. Please try again in 1.937s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198559, Requested 7898. Please try again in 1.937s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Peter Au felt encouraged to see that hundreds showed up Sunday in Mineola to condemn the violent attacks that\nhave spread fear among the Asian American community.\nAu, a financial adviser who commutes weekly to Flushing, Queens, for work, said he worries about his safety.\n\"On the way to my office even during broad daylight, I still feel the fear,\" said Au, 45, of Garden City, at the end\nof the hourlong rally. \"I don't know if anybody was going to walk behind me and start attacking me.\"\nJust last month, a man came up behind a 36-year-old Asian man walking on a street in Manhattan's Chinatown\nand stabbed him in the torso, leaving the victim hospitalized in critical condition.\n\"It's been a very, very tragic year in 2020. Not only did we have to fight COVID-19, as a minority living in this\ncountry we also had to fight racism,\" Au said. \"I'm glad that there's a huge turnout today and see so many\ndifferent groups supporting us.\"\nThe rally held outside the Nassau County Legislative Building drew about 300 attendees, many of whom held up\nsigns that said \"END VIOLENCE AGAINST ASIANS\" and \"Justice for Grandpa VICHA,\" the first name of Vicha\nRatanapakdee, an 84-year-old Thai man who died in January after being slammed onto the pavement in San Francisco. Ratanapakdee's death was one of the violent assaults that ignited a national outcry over the\ndiscrimination and harassment that some Asian Americans said they've endured since the pandemic began.\nPresident Joe Biden in a nationally televised speech last Thursday condemned violence against Asian Americans.\n\"One year later today, we already have the vaccines for the virus, but the hate and crime against Asian\nAmericans are still running out of control,\" said Gordon Zhang, president of the Long Island Chinese American\nAssociation.\nIn an emotional speech, Zhang evoked the history of minorities being \"scapegoated\" in times of crisis, pointing\nto the Chinese Exclusion Act of 1882 and the Japanese internment camps during World War II.\n\"But today, it's not the 1880s. It's not the 1940s. We are in the year of 2021,\" Zhang said. \"We can't have the\nugly and painful history repeat itself again.\"\nAfter recounting several attacks in recent months, Zhang said the reported cases only represent \"the tip of the\niceberg\" as many incidents go unreported - something acknowledged by Nassau Police Commissioner Patrick\nRyder.\n\"I know there have been many that have not been reported because people are fearful,\" Ryder said. \"They\nshould come forward. We want you to know that your police officers, the men and women that stand out here to\nprotect you today, took an oath to protect and serve.\"\nThe crowd occasionally broke into chants of \"stop the hate\" as elected officials as well as leaders from the\nNAACP, Jewish and Muslim groups called for unity.\n\"We are not looking for any different way of treating us,\" said Sen. Kevin Thomas (D-Levittown), who arrived at\nthe United States at the age of 10. He is the state's first Indian American state senator. \"Just treat us the same\nas everyone else.\"\nAs Thomas and others spoke, Young Ono of Stewart Manor waved a \"#STOP ASIAN HATE\" sign along with her\ndaughter, Alexandra, 7, who held a second sign that said: \"HATE IS A VIRUS.\" The mother of two wrote the\nletters and her daughter and son Jayson, 3, decorated the signs with color-filled hearts and stars.\n\"Watching all these people getting hurt reminded me so much of my own parents and grandparents,\" said Ono,\nwhose parents emigrated from South Korea. \"It's important for me to show support and show my children that\nit's important to show support for something like this.\"\nHarry Woodrow, 73, of Westbury, heard about the rally from his temple in Jericho. With friend Sharyn Levine,\nthey decided to come out to show solidarity.\n\"There's so much hate in our country, not only toward Asians ... but toward various ethnic groups,\" Woodrow\nsaid. \"This is a small way to stand against it and just be part of a crowd. And people will look on the news and\nsee a large number of people [to] whom this means somethin"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196286, Requested 7401. Please try again in 1.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196286, Requested 7401. Please try again in 1.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A: The fact that we see an increasing number of Californians whose UI (Unemployment Insurance) payments are being either reduced or denied altogether because they’re working is a good sign for the economy as we slowly re-open. That being said, going back to work also creates a challenging dilemma for some workers: If you’re going back to work on a reduced schedule (with reduced pay), this part-time work can result in your UI benefits being either denied or reduced.\nAnd, of course, there’s still ongoing safety concerns for workers around COVID-19, as well as childcare issues for working parents.\nQ: The brief shows how an increasing number of people are getting denied unemployment because of partial income. Is there a minimum people can earn before they’re cut off from unemployment or is it based on percentage reduction of your usual income?\nWhen you file for unemployment insurance, EDD (The division which handles Unemployment Insurance) takes a look at your earnings history, and determines a weekly benefit amount, which is the maximum you can be paid in benefits each week.\nFor claimants whose amount is below the cap of $450, they can typically earn up to 75% of their prior average weekly earnings. If they earn more than that in a given week, they aren’t eligible for any benefits. But it’s week by week, so if they earn less in the next week, then they could be eligible to receive unemployment again.\nQ: Your findings show one in four women, as opposed to one in five men, are affected by the crisis. Why are women impacted so much more?\nSome of this, though not all, can be explained by the nature of the crisis: We’ve seen that several of the industries hardest hit by the pandemic are often the ones where more women tend to work. In particular, women make up a disproportionate share of the labor force employed in accommodation, food services and sales, industries hit especially hard by the shutdowns.\nQ: I’ve heard from a lot of people that they’re making more in unemployment than working because of the extra $600 federal unemployment assistance. So should people go back to work if they can?\nA: Generally speaking, yes, the Unemployment Insurance system in the U.S. is structured in such a way as to encourage people to go back to work because you’ll make more money.\nThat being said, policymakers created the $600 FPUC payment because they recognized the unique circumstances created by the COVID-19 crisis and all of the social distancing challenges it created, especially for the labor market.\nWhile earning a bit of extra money during the shutdown helps many workers, the $600 payment is set to expire next month, and hence workers may return to work when they can since the risk of being unemployed and uninsured again in July may outweigh the benefits of a few weeks of extra cash now.\nQ: The report suggests a program like work sharing could help workers. How does it work?\nA: Work sharing programs allow firms to avoid layoffs by instead reducing hours for a group of employees. For these employees, a portion of their lost pay is replaced with prorated UI benefits. Importantly, for companies re-opening at reduced hours, work sharing allows some employees who would otherwise earn too much money in a week to remain eligible for partial UI, to instead receive these benefits, and also receive the $600 weekly FPUC payment.\nQ: What else could be done to help workers and/or business owners who have been hit so hard because of this pandemic?\nA: The $600 FPUC payments are a lifeline for many workers. Even with these payments, the median weekly benefit amount for initial claimants in California is just above 50% of median family income in the state. Without these additional payments, many families out of work would be facing even more severe income shocks, which will have repercussions throughout the economy as they’re forced to further reduce spending.\nTying such benefits to a meaningful economic trigger as opposed to an arbitrary calendar date should be a high priority.\nQ: Anything I didn’t ask or you want to add?\nA: The report also finds that close to 80% of workers who filed unemployment insurance benefits during the crisis could have received a payment. The typical UI claimant could have been paid within two weeks of filing benefits. While this may not have been true for every claimant, overall it seems the system was able to provide support for unemployed workers in a timely fashion!\nIn California is a roundup of news from across USA TODAY Network newsrooms. Also contributing: Reason, WNYC, San Francisco Chronicle, Los Angeles Times.\nThis article originally appeared on USA TODAY: In CA: In the O.C., the protesters have it — masks are no longer required"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195582, Requested 8094. Please try again in 1.102s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195582, Requested 8094. Please try again in 1.102s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "from COVID-19 infection is very high. The website of The Atlantic published two articles respectively on March 28 and April 29, titled Ageism Is Making the Pandemic Worse and We're Literally Killing Elders Now. They pointed out that long-existing defects in the old-age care system, such as insufficient capital investment and staffing, caused the United States to fall behind in ensuring the rights and interests of the elderly, and that such a situation was due to many political reasons. The website of the Washington Post observed on May 9 that the U.S. anti-pandemic action had become a state-approved massacre that deliberately sacrificed the lives of the elderly, the working class, African Americans, and Hispanic Americans.\nThe Homeless Having Nowhere to Go during the Pandemic. The website of USA Today reported on April 22, 2020, that more than 550,000 people were homeless every night in the United States. The report also pointed out that according to the statistics released by the Homeless Alliance, about 17 out of every 10,000 Americans have experienced homelessness and about 33 percent of them are families with children. Many of the homeless\nAmericans are elderly people and disabled people. Given their originally poor physical health and bad living and hygienic conditions, they are susceptible to the virus. During the epidemic, the homeless who are living on the streets are deported and forced to live in temporary shelters for isolation. The website of Reuters reported on April 23 that the crowded shelters made it impossible for the homeless who lived there to maintain social distance, which made it easier for the virus to spread. As of April 20, 43 homeless people living in New York City's shelters had died from infection of COVID-19, and 617 tested positive for COVID-19.The website of The New York Times reported on April 13 that the shelters for the homeless had become a delayed-action bomb of a virus outbreak in New York City, where more than 17,000 people lived and slept almost side by side in centralized shelters. The website of Nature journal reported on May 7 that when researchers began conducting virus testing on homeless people in the United States, they found that the situation there had gotten out of control. The website of the Boston Globe reported on May 4 that 596 homeless people in Boston had been diagnosed with COVID-19, accounting for one-third of the confirmed cases in the region.\nThe website of the Los Angeles Times reported on May 14 that research showed that due to the impact of the pandemic, the number of homeless people in the United States might surge by as much as 45 percent within a year, further exacerbating the public health crisis.\nWorrisome Situation of Poor Children and Immigrant Children. The United States has not yet ratified the United Nations Convention on the Rights of the Child, which is one of the core international human rights conventions. In recent years, child poverty and abuse has remained a grave problem in the United States. This problem has been exacerbated by the pandemic. Forbes News reported on May 7, 2020 that a survey showed a large number of American children were facing hunger during the pandemic. As of the end of April, more than one-fifth of American households had been facing food crises, and two-fifths of American households with children under 12 years of age have been facing such crises. Forbes News reported on May 9 that the number of child exploitation reports in the United States surged during the pandemic. The National Center for Missing &Exploited Children received 4.2 million reports in April, an increase of 2 million from March 2020 and an increase of nearly 3 million from April 2019. Apart from that, a more worrisome fact is that a large number of unaccompanied immigrant children are still being held in detention centers in the United States. They are currently in an extremely dangerous situation during the pandemic. The United Nations Special Rapporteur on human rights of migrants Felipe Gonzalez Morales and other UN human rights experts issued a joint statement on April 27, requesting the U.S. government to transfer immigrants from overcrowded and insanitary detention centers. On May 29, 15 experts of the Special Procedures of United Nations Human Rights Council issued a joint statement urging the United States to take more measures to prevent virus outbreaks in the detention centers. The website of the United Nations reported on May 21 that since March, the U.S. government had repatriated at least 1,000 unaccompanied immigrant children to Central and South America regardless of the risk of the pandemic. The United Nations Children's Fund (UNICEF) criticized this move, for it would expose the children to greater danger.\n5. Relevant Behaviors of the U.S. Government Seriously Violating the Spirit of International Human Rights Law\nWhen the citizens' right to life and health is severely threatened by the spreading pandemic, the U.S. government, instead of focusing on controlling the pandemic, wields a hegemonic stick and fans the flames of trouble everywhere, trying to divert attention and shirk responsibility. Its behaviors have seriously undermined the international community's concerted efforts to control the pandemic.\nIneffective Anti-pandemic Efforts Failing the National Duty of Ensuring the Citizens' Right to Life. The International Covenant on Civil and Political Rights (ICCPR) stipulates that every human being has the inherent right to life, and countries are obliged to take proactive measures to guarantee their citizens' right to life. As a party to the convention, the U.S.government, however, has not given priority to its citizens' right to life and health during the pandemic. Instead, it has been prioritizing the political campaign at home and the political drive to suppress China abroad, rather than safeguarding the lives and safety of its citizens. Given this, it has missed the best chance to curb the spread of the virus, and caused a grave human"
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196131, Requested 6505. Please try again in 790ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196131, Requested 6505. Please try again in 790ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "A 75-year-old Asian man has died after succumbing to injuries sustained from a robbery. The suspect is now\nfacing special circumstance murder charges.\nOn Thursday, Alameda County DA Nancy O’Malley filed special circumstances murder and multiple serious felony\ncharges against Teaunte Bailey, 26, in connection with crimes committed on two different dates in Oakland, CA,\naccording to a press release.\nBailey allegedly pushed 75-year-old Pak Ho to the ground and robbed him on the morning of March 9 in the\nAdams Point neighborhood of Oakland. He fled in his car and was eventually apprehended.\nHo’s head hit the pavement after Bailey shoved him, causing him to sustain a traumatic head injury and brain\ndamage. He died of his injuries on Thursday.\nBailey is also being charged in connection with a similar crime that occurred February 19 2021. That day he\nallegedly broke into a senior living apartment and shoved the 72-year-old victim inside. Police say he stole the\nvictim’s phone and several other items.\nBailey is being held at Santa Rita jail without bail, according to ABC 7. He is set to be arraigned on Friday, March 12."
    }
}
{
    "type": "error",
    "data": "Entity Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193283, Requested 7956. Please try again in 371ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193283, Requested 7956. Please try again in 371ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": {
        "doc_index": 0,
        "text": "Ro Nguyen thought it would play out differently.\nThe 30-year-old had just watched a movie at a Streeterville cinema with a friend on March 12 and was strolling down East Grand Avenue around 8 p.m., the two of them\nmarveling at the deserted streets.\nAs they headed toward the Red Line station, Nguyen said a man walking nearby saw them and yelled out, \"F--- China!\"\nThen the man spat on Nguyen, he said. The saliva splattered on his jacket.\nNguyen, who is of Vietnamese and Filipino descent, had imagined this moment. Having read news stories about harassment and attacks against Asians as coronavirus cases\nrose, he wondered if he would be next. If he was, he thought he would take a stand on behalf of himself and other Asians.\nBut as he wiped the saliva off with his sleeve, he and his friend -- of South Asian descent -- hurried away without a word to the assailant.\n\"In that instance, I was just kind of shocked, or dumbfounded, of what occurred,\" Nguyen said.\nNguyen worries that East Asians in the United States will face even more harassment and attacks as coronavirus cases continue to rise. He said he believes President Donald\nTrump stoked such hate-filled reaction during a news briefing last week when he defended his use of \"Chinese virus.\"\n\"It's not racist at all,\" Trump said. \"I think they probably would agree with it 100%. It comes from China.\" He also said he was using the expression to fight back against Beijing\nofficials who were blaming the U.S. military for the introduction of the disease.\nThe World Health Organization has recommended people use coronavirus or COVID-19, stating that other characterizations spread negative stereotypes.\nOn Monday, Trump appeared to walk back his earlier position, using the word \"coronavirus\" and stressing it was important to \"totally protect our Asian American community.\"\n\"Yes, it seems that there could be a little bit of nasty language toward the Asian Americans in our country,\" he admitted. \"I don't like that at all. These are incredible people,\nthey love our country and I'm not going to let it happen.\"\nBut Nguyen fears it's too late. Since the president and other politicians first used the term, Nguyen and other Asians in Chicago said they have felt growing apprehension that\npeople take the president's comments as a license for racism.\nElsewhere in the U.S., reports of hate crimes have cropped up from New York to San Francisco, some caught on video and circulated through social media. In California's San\nFernando Valley, a 16-year-old was sent to the emergency room after being beat up and accused of having the coronavirus at his high school, CBS News reported.\n\"It's outrageous,\" said state Rep. Theresa Mah from the 2nd District, which includes Chicago's Chinatown. \"Chinese Americans and constituents of mine understand this is a\nsituation in which they could potentially be scapegoats for the uncertainty people feel.\"\n'History is repeating itself'\nMabel Menard was sitting alone at her favorite neighborhood tavern in Old Town earlier this month when she noticed an unfamiliar face.\nA man, very intoxicated, made eye contact with her and said, \"Do you have the corona?\"\nMenard, a 58-year-old Chinese American, said she didn't think twice before quipping, \"I'm having a wine.\"\nThe bar's staff kicked the man out.\n\"I kind of thought he was an idiot,\" Menard said. \"That's the kind of mentality that you walk around with, thinking that you're better than anybody else, and the way to make\nyou feel better is to be nasty to other people. I think that's pretty sad.\"\nTuyet Anh, 20, said she was sitting alone on a Red Line train one day late in February, headed south from Lakeview to DePaul University, where she is a junior, when she noticed\ntwo men whispering.\n\"Got to get our masks on,\" one of them said.\nTuyet Anh, who did not want her last name used, looked away. One of the men wondered aloud, \"Oh, do you think she heard us?\"\n\"I felt at first just very shocked,\" Tuyet Anh said. \"Really? Someone would say that? But then processing it more, I just felt a little nervous and that feeling of being under\nsurveillance, almost.\"\nTuyet Anh, who is of Vietnamese descent, has since been saddened to read social media comments from her fellow classmates using the terms \"Chinese virus\" and \"Wu flu.\"\nShe wasn't surprised, given that the first phrase was picked up by Trump.\n\"It makes me and other Asian Americans feel as if we are the virus,\" Tuyet Anh said. \"We are labeled and demonized as this threat to white American safety.\"\nMenard, president of Chicago's chapter of the advocacy group OCA-Asian Pacific American Advocates, said the tense environment reminds her of the 1882 Chinese Exclusion Act\nbarring Chinese immigrants from entering the country, the first immigration law to exclude an ethnic group, as well as the World War II Japanese internment camps forcing\nJapanese Americans into incarceration.\n\"History is repeating itself,\" Menard said. \"If we looked at the history of Asian Americans in the U.S., we've always been either held up as a model minority -- that despite the\ndiscrimination, we're still doing well -- or we're demonized because when something like this happens, it's all our fault.\"\nAsian American organizations last week launched the #WashTheHate campaign on social media, highlighting stories of coronavirus-related racism. The Asian Pacific Policy and\nPlanning Council and Chinese for Affirmative Action groups also started collecting reports of incidents of hostility against Asians.\nIn less than a week, more than 400 reports surfaced"
    }
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195578, Requested 8400. Please try again in 1.193s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195578, Requested 8400. Please try again in 1.193s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195554, Requested 8172. Please try again in 1.117s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195554, Requested 8172. Please try again in 1.117s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197536, Requested 5715. Please try again in 975ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197536, Requested 5715. Please try again in 975ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195679, Requested 8172. Please try again in 1.155s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195679, Requested 8172. Please try again in 1.155s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198504, Requested 8172. Please try again in 2.002s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198504, Requested 8172. Please try again in 2.002s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195314, Requested 6870. Please try again in 655ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195314, Requested 6870. Please try again in 655ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195312, Requested 7084. Please try again in 718ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195312, Requested 7084. Please try again in 718ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193345, Requested 7774. Please try again in 335ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 193345, Requested 7774. Please try again in 335ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197122, Requested 8172. Please try again in 1.588s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197122, Requested 8172. Please try again in 1.588s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197499, Requested 10618. Please try again in 2.435s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197499, Requested 10618. Please try again in 2.435s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196306, Requested 8279. Please try again in 1.375s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196306, Requested 8279. Please try again in 1.375s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194450, Requested 7787. Please try again in 671ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194450, Requested 7787. Please try again in 671ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198240, Requested 6649. Please try again in 1.466s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 198240, Requested 6649. Please try again in 1.466s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197663, Requested 5789. Please try again in 1.035s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197663, Requested 5789. Please try again in 1.035s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196320, Requested 8172. Please try again in 1.347s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 196320, Requested 8172. Please try again in 1.347s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197812, Requested 6202. Please try again in 1.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197812, Requested 6202. Please try again in 1.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195120, Requested 5723. Please try again in 252ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195120, Requested 5723. Please try again in 252ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194965, Requested 8172. Please try again in 941ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194965, Requested 8172. Please try again in 941ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195849, Requested 8172. Please try again in 1.206s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195849, Requested 8172. Please try again in 1.206s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194417, Requested 5601. Please try again in 5ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194417, Requested 5601. Please try again in 5ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195117, Requested 5852. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 195117, Requested 5852. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197892, Requested 4656. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 197892, Requested 4656. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194314, Requested 6582. Please try again in 268ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 194314, Requested 6582. Please try again in 268ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
{
    "type": "error",
    "data": "Community Report Extraction Error",
    "stack": "Traceback (most recent call last):\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke_json(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 80, in _invoke_json\n    result = await generate()\n             ^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 72, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 96, in _native_json\n    result = await self._invoke(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1633, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1838, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1532, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bensonchiu/opt/anaconda3/envs/graph-rag-new/lib/python3.12/site-packages/openai/_base_client.py\", line 1633, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192144, Requested 8172. Please try again in 94ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
    "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-rs61Ho5Oja1wAkjw3BJVwj1r on tokens per min (TPM): Limit 200000, Used 192144, Requested 8172. Please try again in 94ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
    "details": null
}
